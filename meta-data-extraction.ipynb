{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.216639,
     "end_time": "2021-02-04T08:30:11.020599",
     "exception": false,
     "start_time": "2021-02-04T08:30:10.803960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Meta-data Extraction Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.206128,
     "end_time": "2021-02-04T08:30:11.431203",
     "exception": false,
     "start_time": "2021-02-04T08:30:11.225075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    \n",
    "    Format - PEP8\n",
    "    Python - 3.6.3\n",
    "    Created Date - 04 April 2020\n",
    "    Author - Pranjal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211874,
     "end_time": "2021-02-04T08:30:11.847464",
     "exception": false,
     "start_time": "2021-02-04T08:30:11.635590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CONTENTS\n",
    "\n",
    "\n",
    "1. Possible Approaches\n",
    "2. Imports\n",
    "3. Settings: Hyper-parameters\n",
    "4. Load Mapper File \n",
    "5. Abbreviations Module \n",
    "\t- Legal Known abbrvs\n",
    "\t- Dynamic Abbrvs\n",
    "\n",
    "\n",
    "6. Reading from XML Module\n",
    "7. Intial Preprocessing Module\n",
    "8. Address Block (AB) Identification Module\n",
    "9. Labelling Data Module\n",
    "10. Feature Engineering Module\n",
    "    - TYPE I. Linguistic Features\n",
    "\t- TYPE II. Text Features\n",
    "\n",
    "\n",
    "11. Saving & Loading Module \n",
    "12. Filtering Dataset Module \n",
    "13. Training Module\n",
    "    - Phase I: Line Classification TrainingÂ¶\n",
    "    - **FINAL ARCHITECTURE**\n",
    "    - Phase II: Chunk Identification Module\n",
    "\n",
    "\n",
    "14. OLD Methods/Versions for Chunk Identification Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.203607,
     "end_time": "2021-02-04T08:30:12.258998",
     "exception": false,
     "start_time": "2021-02-04T08:30:12.055391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <ins>1. Possible Approaches</ins>\n",
    "    \n",
    "...\n",
    "\n",
    "`Approach 1`\n",
    "    \n",
    "        Phase I: Line Classification\n",
    "\n",
    "        1. Extract text from XML files line-byline\n",
    "        2. filter using NNP, NP, remove VB, ADV, and levenstine match bewteen supplier name, score, without filter score.\n",
    "        2. Create features such as:\n",
    "            :email\n",
    "            :URL\n",
    "            :1Cap\n",
    "            :Keywords list - Presence of list keywords in line\n",
    "            :Legal abbrv - Presence of legal abbrv from list\n",
    "            \n",
    "                a. Create  a list of user defined abbrv - Scraped from wikipedia for all English countries.\n",
    "                b. SN -> split -> last token -> dictword(?) POS tag(?) Synonyms using Word2Vec model\n",
    "                c. Custom NER Spacy \n",
    "                    i.   Filter out LINES from XML containing SN. - Fuzzywuzzy, levensthine, Exact Regex Match\n",
    "                    ii.  Find out index-start, index-end of SN.\n",
    "                    iii. Train custom Spacy NER on 1k sentences atleast./ biLSTM CRF to using IOB tags.\n",
    "                    iV.  Test data + ABC INC 10 May\" -> Spacy.ents (ABC INC, ent)\n",
    "                d. Split SN by space and take last token\n",
    "                e. Check last token has vowels or not\n",
    "                f. Contains punct marks or not.\n",
    "                   \n",
    "                \n",
    "            :city name\n",
    "            :country name\n",
    "            :%Cap1DictWord\n",
    "            :%CapDictWord\n",
    "            :%DictWord\n",
    "            :%Cap1NonDictWord\n",
    "            :%CapNonDictWord\n",
    "            :%NonDictWord\n",
    "            :%ProperN - spacys pos tag\n",
    "            :LinePos - postion of line in quadrant\n",
    "            :l,t,r,b - Regularized l,t,r,b coordinates of line\n",
    "            :FontSize - font size of Regularized (l,t,r,b) coordinates of line\n",
    "            :%Spacy NER+ - Presence of Spacy's NER tags in line\n",
    "            :AddressNeighbour - Context of 'n' neighbouring lines above/below selected line if context is address or not 1/0\n",
    "         3. Target Values (1/0) if its a \"Supplier Name Line\" or \"Not a Supplier Name Line\"\n",
    "         4. Labelling -\n",
    "             a. [HIGH WEIGHT] Regex match of Line with Excel Mapper file Supplier Name. - fuzzywuzzy/diff lib levensthine  \n",
    "             b. [LESS WEIGHT] Score of above values\n",
    "             c. Manual Check by DA\n",
    "         5. Classification using XGBClassifer or SVM.\n",
    "         \n",
    "         Phase II. Chunk Identification\n",
    "         chunk identifi - Sentence NNP extract as chunks.\n",
    "         chunk - % of NER ORG in sent -> thresholding to identify as chunk or no chunks present in sent.\n",
    "         1. Once the LINES are classified as a class 'Supplier'Name Line SNL'. It shall be broken into 'n' chunks.\n",
    "         2. Again repeat above steps to classify the chunk and then the chunk shall be classified: \"Supplier\"/\"Notsupplier\"\n",
    "      \n",
    "...\n",
    "\n",
    "`Approach 2`\n",
    "    \n",
    "        1. Get text in a paragraph from XML. Combine lines side-by-side to get a paragraph.\n",
    "        2. Word tokenize this to extract words from it and POS tag it.\n",
    "        3. IOB annotate it or Use BILOU annotate. - Rsa NPM UI, spacy prodigy \n",
    "        4. Annotated data is used to train custom bi-LSTM CRF or a HMMM Model.\n",
    "        5. Test data is annotated using tagging mechanism and passed to trained model to predict as a tag (I,O,B)\n",
    "        6. CHALLENGE - Sequential Data is reuqired from XML files.\n",
    "...\n",
    "\n",
    "`Approach 3`\n",
    "        \n",
    "        1. Train an LSTM on patterns of Words followed by a legal abbrev, such that the model predicts that if a word willbe \n",
    "           followed by a legal abbrv or not.\n",
    "...\n",
    "\n",
    "`Approach 4 (Common)`\n",
    "        \n",
    "        1. Convert PDF -> Images using VOTT\n",
    "        2. Annotate images - Annotate Address (tag: addr); Supplier Name (tag: SN).\n",
    "        3. Build a custom NER (TF Object detection model) to identify location of address and SN\n",
    "        4. Use the output from TF Object detection model -> Coordinates of Address and SN across 10k Images.\n",
    "        5. Train a classifier on these coordinates as Address/Not Address; SN/Not SN\n",
    "        6. Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.201011,
     "end_time": "2021-02-04T08:30:12.664072",
     "exception": false,
     "start_time": "2021-02-04T08:30:12.463061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>2. Imports</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "import uuid\n",
    "import json\n",
    "import sqlite3\n",
    "import configparser\n",
    "import ast\n",
    "from collections import OrderedDict, defaultdict\n",
    "from operator import itemgetter    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Rectangle area\n",
    "from collections import namedtuple\n",
    "\n",
    "# Pre-processing libs\n",
    "## NLTK\n",
    "import nltk\n",
    "nltk.data.path.append(\"../nltk_data__v1.0/\")\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import words as eng_words\n",
    "lemma = WordNetLemmatizer()\n",
    "ENG_words = eng_words.words()\n",
    "\n",
    "## sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "## Gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases, phrases\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## Java standfordNLP\n",
    "import stanfordnlp \n",
    "stf_nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,pos\")\n",
    "\n",
    "import unicodedata\n",
    "import sacremoses\n",
    "from sacremoses import truecase, MosesTruecaser, MosesPunctNormalizer, normalize, truecase\n",
    "\n",
    "## LEXNLP\n",
    "# from bs4 import BeautifulSoup\n",
    "# import lexnlp.extract.en.money\n",
    "# import lexnlp.extract.en.urls\n",
    "\n",
    "## FuzzyWuzzy: Distances\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "## geotext: Geoparsing\n",
    "import geopandas as gpd\n",
    "from urllib import request\n",
    "from geotext import GeoText\n",
    "\n",
    "## libpostal: address\n",
    "from postal.parser import parse_address as crf_NER\n",
    "\n",
    "## Spacy\n",
    "import spacy\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Text Vectorization libs\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Metrics libs\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, auc, roc_curve, roc_auc_score, accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "# from xgboost import XGBClassifier\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import *\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "# KERAS ELMO IMPLEMENTATION\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import keras.layers as layers\n",
    "# import pydot\n",
    "# from collections import Counter\n",
    "# from keras import backend as K\n",
    "# from keras.callbacks import TensorBoard\n",
    "# from keras.callbacks import LearningRateScheduler\n",
    "# from keras.layers import Input, Embedding, BatchNormalization, LSTM, Dense, Concatenate,Bidirectional\n",
    "# from keras.models import Model\n",
    "# from keras.utils import plot_model\n",
    "\n",
    "# Plotting libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.215161,
     "end_time": "2021-02-04T08:30:17.310960",
     "exception": false,
     "start_time": "2021-02-04T08:30:17.095799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.226036,
     "end_time": "2021-02-04T08:30:17.756091",
     "exception": false,
     "start_time": "2021-02-04T08:30:17.530055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>3. Settings: Hyper-parameters</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.215359,
     "end_time": "2021-02-04T08:30:18.190274",
     "exception": false,
     "start_time": "2021-02-04T08:30:17.974915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Training\" data path\n",
    "data_path = \"Data/OCR_XML/\"\n",
    "\n",
    "# deps\n",
    "mapper_file_path = \"Data/actual_training_labels.xls\"\n",
    "legal_abbrv_path = \"Data/Legal_Abbrvs_20042020.txt\"\n",
    "dynamic_abbrv_path = \"Data/Dynamic_Abbrvs_20042020.txt\"\n",
    "contractions = dict(json.load(open('Data/Contractions.json', 'r')))\n",
    "REGEX_PUNCTS = \"\\\\\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\"\n",
    "REGEX_SYMBOLS = \"\\\\â \\\\Â¤\\\\Â¦\\\\Â§\\\\Â¨\\\\Âª\\\\Â«\\\\Â¬\\\\Â®\\\\Ë\\\\Â°\\\\Â±\\\\Â²\\\\Â³\\\\Â´\\\\Âµ\\\\Â¶\\\\Â¹\\\\Âº\\\\Â»\\\\Â¼\\\\Â½\\\\Â¾\\\\Â¿\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã\\\\Ã \\\\Ã¡\\\\Ã¢\\\\Ã£\\\\Ã¤\\\\Ã¥\\\\Ã¦\\\\Ã§\\\\Ã¨\\\\Ã©\\\\Ãª\\\\Ã«\\\\Ã¬\\\\Ã­\\\\Ã®\\\\Ã¯\\\\Ã°\\\\Ã±\\\\Ã²\\\\Ã³\\\\Ã´\\\\Ãµ\\\\Ã¶\\\\Ã·\\\\Ã¸\\\\Ã¹\\\\Ãº\\\\Ã»\\\\Ã¼\\\\Ã½\\\\Ã¾\\\\Ã¿\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211993,
     "end_time": "2021-02-04T08:30:19.053004",
     "exception": false,
     "start_time": "2021-02-04T08:30:18.841011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### :: Settings ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_TAG = \" <&> \"\n",
    "MINIMUM_CHARS_IN_LINE = 2\n",
    "LENGTH_OF_EMAIL = 3\n",
    "LENGTH_OF_URL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.213384,
     "end_time": "2021-02-04T08:30:19.928975",
     "exception": false,
     "start_time": "2021-02-04T08:30:19.715591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.212259,
     "end_time": "2021-02-04T08:30:20.359064",
     "exception": false,
     "start_time": "2021-02-04T08:30:20.146805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins> 4. Load Mapper File </ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading mapper file (labelling data)\n",
    "mapper_file = pd.read_excel(mapper_file_path)\n",
    "\n",
    "# Supplier Names found in mapper file\n",
    "SN_all = list(mapper_file.SUPPLIER_NAME.values)\n",
    "SN_unique = list(mapper_file.SUPPLIER_NAME.unique())\n",
    "print('total invoices = ', len(SN_all))\n",
    "print('Unique SN = ', len(SN_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.209177,
     "end_time": "2021-02-04T08:30:21.261408",
     "exception": false,
     "start_time": "2021-02-04T08:30:21.052231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.205292,
     "end_time": "2021-02-04T08:30:21.672514",
     "exception": false,
     "start_time": "2021-02-04T08:30:21.467222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>5. Abbreviations Module </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.207401,
     "end_time": "2021-02-04T08:30:22.088282",
     "exception": false,
     "start_time": "2021-02-04T08:30:21.880881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Legal knwon abbrvs\n",
    "2. Dynaimc frequent Abbrvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.205674,
     "end_time": "2021-02-04T08:30:22.498241",
     "exception": false,
     "start_time": "2021-02-04T08:30:22.292567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Legal Known abbrvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.20479,
     "end_time": "2021-02-04T08:30:22.906926",
     "exception": false,
     "start_time": "2021-02-04T08:30:22.702136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Legal known Abbrvs = An extension found in company names that usually follow below rules:\n",
    "\n",
    "\n",
    "- These lack vowels and are not morphologically well-formed words, thus can be detected because of this.\n",
    "- Infringe upon the phonotactics of the language in which they occur.\n",
    "- These predominantly use period \".\" and might also use typical alphanumeric characters such as /, & or ~ within them.\n",
    "- Have the same collocations as their unabbreviated counterparts.\n",
    "- Can exploit the rebus principle (expanding contractions eg. inb4 \"in before\", NRG \"energy\").\n",
    "\n",
    "...\n",
    "\n",
    "<ins>STEPS:</ins>\n",
    "\n",
    "1. Srapping a wikipedia page using beautiful Soup4.\n",
    "    \n",
    "    scraped = https://en.wikipedia.org/wiki/List_of_legal_entity_types_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legal abbrvs\n",
    "list_legal_abbrv = []\n",
    "with open(legal_abbrv_path, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        abbrv, fullform = line.split(\"(\")[0].strip(), line.split(\"(\")[1].rstrip(\")\").strip()\n",
    "        list_legal_abbrv.append(abbrv)\n",
    "list_legal_abbrv = list(set(list_legal_abbrv + list(map(lambda x:x.upper(), list_legal_abbrv)) + list(map(lambda x:x.lower(), list_legal_abbrv))))\n",
    "list_legal_abbrv_regex = list(map(lambda x: x.replace(\".\", \"\\.\").replace(\",\", \"\\,\").replace(\"-\", \"\\-\").replace(\"(\", \"\\(\").replace(\")\", \"\\)\").replace(\"[\", \"\\[\").replace(\"]\", \"\\]\"), list_legal_abbrv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.202849,
     "end_time": "2021-02-04T08:30:23.755313",
     "exception": false,
     "start_time": "2021-02-04T08:30:23.552464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Dynamic Abbrvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.224388,
     "end_time": "2021-02-04T08:30:24.192782",
     "exception": false,
     "start_time": "2021-02-04T08:30:23.968394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Dynamic Abbrvs = Last token in a Supplier Name with a POS tag=Noun/Pronoun and which occurs in atleast 50 or more documents.\n",
    "\n",
    "- These tokens are considered to be dynamically found abbreviation, and stored in a 'dynamic_list'.\n",
    "- These will be added along with the known legal abbrvs.\n",
    "- From time-to-time, when more Supplier Names are discovered, this dynamic list will be updated and new tokens will be allowed to enter the list.\n",
    "\n",
    "....\n",
    "\n",
    "<ins>STEPS</ins>\n",
    "\n",
    "**RULES for considering 'last token' as a Dynamic Abbrv:**\n",
    "\n",
    "1. Should occur in at-least 50 documents.\n",
    "2. Should not be a stop-word.\n",
    "3. Should have a POS tag = NN or NNS or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking last token across all Supplier Names\n",
    "stop_words = set(stopwords.words('english'))\n",
    "frequent_abbrv = []\n",
    "for S in SN_all:\n",
    "    S = S.strip()\n",
    "    known_abbrv = re.findall(r\"(?=(\\b\" + '\\\\b|\\\\b'.join(list_legal_abbrv_regex) + r\"\\b))\", S.lower())\n",
    "    if len(known_abbrv) > 0:\n",
    "        continue\n",
    "    else:\n",
    "        # If not a known legal abbrv...\n",
    "        last_token = S.split(\" \")[-1]\n",
    "        # POS tag should be NN or similar\n",
    "        pos_tag = nltk.pos_tag(word_tokenize(last_token))\n",
    "        if pos_tag in [\"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"LS\", \"IN\", \"PDT\", \"TO\", \"UH\", \"WDT\", \"WP\", \"WRB\"]: continue\n",
    "        # Should not be a stop-word\n",
    "        if last_token.lower() in stop_words: continue\n",
    "        frequent_abbrv.append(last_token.upper())\n",
    "\n",
    "\n",
    "# MINIMUM FREQUENCY: TFIDF's min freq = 50\n",
    "MIN_FREQ_ABBRV = 2\n",
    "\n",
    "corpus = frequent_abbrv\n",
    "vectorizer = TfidfVectorizer(min_df=MIN_FREQ_ABBRV) # select all last_token occuring in atleast 5 PDFs as addition to abbrv list\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "list_frequent_abbrv = vectorizer.get_feature_names() # lowercase\n",
    "list_frequent_abbrv = list_frequent_abbrv + list(map(lambda x: x.upper(), list_frequent_abbrv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.205511,
     "end_time": "2021-02-04T08:30:25.043231",
     "exception": false,
     "start_time": "2021-02-04T08:30:24.837720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Combined: Legal + Dynamic Abbrvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_abbrv = list_legal_abbrv + list_frequent_abbrv\n",
    "list_abbrv_regex = list(map(lambda x: x.replace(\".\", \"\\.\").replace(\",\", \"\\,\").replace(\"-\", \"\\-\").replace(\"(\", \"\\(\").replace(\")\", \"\\)\").replace(\"[\", \"\\[\").replace(\"]\", \"\\]\"), list_abbrv))\n",
    "\n",
    "print(\"Known Abbrvs (Upper + lower) = \", len(list_legal_abbrv))\n",
    "print(\"Dynamic Abbrvs = \", len(list_frequent_abbrv))\n",
    "print(\"TOTAL ABBRVS = \", len(list_abbrv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.204073,
     "end_time": "2021-02-04T08:30:25.929781",
     "exception": false,
     "start_time": "2021-02-04T08:30:25.725708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.203429,
     "end_time": "2021-02-04T08:30:26.339078",
     "exception": false,
     "start_time": "2021-02-04T08:30:26.135649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>6. Reading from XML Module</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.206605,
     "end_time": "2021-02-04T08:30:26.750888",
     "exception": false,
     "start_time": "2021-02-04T08:30:26.544283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. XML data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:27.187523Z",
     "iopub.status.busy": "2021-02-04T08:30:27.181475Z",
     "iopub.status.idle": "2021-02-04T08:30:27.240483Z",
     "shell.execute_reply": "2021-02-04T08:30:27.239914Z"
    },
    "papermill": {
     "duration": 0.278073,
     "end_time": "2021-02-04T08:30:27.240658",
     "exception": false,
     "start_time": "2021-02-04T08:30:26.962585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_word_bounding_box(X, Y):\n",
    "    \"\"\"\n",
    "    Finds word-boundary for a given word.\n",
    "    :param: list X: X coordinates of all chars; list Y: Y coordinates of all chars\n",
    "    :return: The left,top & right,bottom coordinates of the char constituting the word.\n",
    "    \"\"\"\n",
    "    r = max(X)\n",
    "    l = min(X)\n",
    "    b = max(Y)\n",
    "    t = min(Y)\n",
    "    return l, t, r, b\n",
    "\n",
    "def get_average_font_size(list_font_size, total_n, dtype):\n",
    "    \"\"\"\n",
    "    Finds font-size of word by calculating avg font-size of all chars. (e.g. \"AbcD\" -> FS=[25,20,20,25]; avgFS = SUM/4)\n",
    "    :param: \n",
    "    list list_font_size: Font-sizes of all chars in word; int total_n: No of chars in word; type dtype: Data type of required font-size\n",
    "    :return: Average font-size of a word.\n",
    "    \"\"\"\n",
    "    if total_n != 0:\n",
    "        avg_font_size = float(sum(list_font_size)/total_n)\n",
    "    else:\n",
    "        avg_font_size = 0.0\n",
    "    if dtype == int:\n",
    "        return int(avg_font_size)\n",
    "    elif dtype == float:\n",
    "        return float(avg_font_size)\n",
    "    else:\n",
    "        return avg_font_size\n",
    "\n",
    "def get_words_from_char(char_dict):\n",
    "    \"\"\"\n",
    "    Finds out words from set of chars. Creates a dict of {Words: coordinates, font-size, page-dims}\n",
    "    :param: dict char_dict: A dict containing character wise Coordinates, FS\n",
    "    :return: A dict of word and its Coordinates, FS, Page-dims\n",
    "    \"\"\"    \n",
    "    word_dict = []\n",
    "    word = \"\"\n",
    "    X, Y, X_, Y_ = [], [], [], []\n",
    "    font_size, font_size_, total_chars = [], [], 0\n",
    "    page_width, page_height = 0, 0\n",
    "    for ch in char_dict:\n",
    "        # NOT EMPTY\n",
    "        if ch[\"text\"]:\n",
    "            # 1. It's a LINEBREAK - Appends stored variables\n",
    "            if ch[\"text\"] == \"<<LINEBREAK>>\":\n",
    "                l, t, r, b = get_word_bounding_box(X, Y)\n",
    "                l_, t_, r_, b_ = get_word_bounding_box(X_, Y_)\n",
    "                avg_font_size = get_average_font_size(font_size, total_chars, int)\n",
    "                avg_font_size_ = get_average_font_size(font_size_, total_chars, float)\n",
    "                word_dict.append({\"content\": word, \"l\": l, \"t\": t, \"r\": r, \"b\": b, 'font_size': avg_font_size, \n",
    "                                  \"l_\":l_, \"t_\": t_, \"r_\":r_, \"b_\":b_, 'font_size_': avg_font_size_,\n",
    "                                  \"page_width\": page_width, \"page_height\": page_height})\n",
    "                \n",
    "                word_dict.append(\"<<LINEBREAK>>\")\n",
    "                word = \"\"\n",
    "                X, Y, X_, Y_ = [], [], [], []\n",
    "                font_size, font_size_, total_chars = [], [], 0\n",
    "                page_width, page_height = 0, 0\n",
    "            \n",
    "            else:\n",
    "                # 2. It's EMPTY - Appends stored variables\n",
    "                if ch[\"text\"] == \" \":\n",
    "                    # if the first and only char is space itself\n",
    "                    if len(word.strip(\" \")):\n",
    "                        l, t, r, b = get_word_bounding_box(X, Y)\n",
    "                        l_, t_, r_, b_ = get_word_bounding_box(X_, Y_)\n",
    "                        avg_font_size = get_average_font_size(font_size, total_chars, int)\n",
    "                        avg_font_size_ = get_average_font_size(font_size_, total_chars, float)\n",
    "                        word_dict.append({\"content\": word, \"l\": l, \"t\": t, \"r\": r, \"b\": b, 'font_size': avg_font_size, \n",
    "                                          \"l_\":l_, \"t_\": t_, \"r_\":r_, \"b_\":b_, 'font_size_': avg_font_size_,\n",
    "                                          \"page_width\": page_width, \"page_height\": page_height})\n",
    "                        word = \"\"\n",
    "                        X, Y, X_, Y_ = [], [], [], []\n",
    "                        font_size, font_size_, total_chars = [], [], 0\n",
    "                        page_width, page_height = 0, 0\n",
    "                    continue\n",
    "                else:\n",
    "                    # 3. IT IS A WORD\n",
    "                    word += ch[\"text\"]\n",
    "                    # 1. Normal Coordinates\n",
    "                    X.append(int(ch['l']))\n",
    "                    X.append(int(ch['r']))\n",
    "                    Y.append(int(ch['t']))\n",
    "                    Y.append(int(ch['b']))\n",
    "                    # 2. Regularized Coordinates\n",
    "                    X_.append(float(ch['l_']))\n",
    "                    X_.append(float(ch['r_']))\n",
    "                    Y_.append(float(ch['t_']))\n",
    "                    Y_.append(float(ch['b_']))\n",
    "                    # Storing font-sizes of all chars (letters, digits and some marks)\n",
    "                    if re.match(\"^[A-Za-z0-9!@#$%&{}[]()]*$\", ch[\"text\"]):\n",
    "                        font_size.append(np.abs(int(ch['t']) - int(ch['b'])))\n",
    "                        font_size_.append(np.abs(ch['t_'] - ch['b_']))\n",
    "                        total_chars += 1\n",
    "                    page_width, page_height = int(ch['page_width']), int(ch['page_height'])\n",
    "    return word_dict\n",
    "\n",
    "def get_non_table_blocks(page):\n",
    "    \"\"\"\n",
    "    Creates a dict of all words present outside the table area.\n",
    "    :param: obj page: XML object of page\n",
    "    :return: A dict of all words outside the table area with Coordinates, FS, Page-dims\n",
    "    \"\"\"\n",
    "    page_width, page_height, page_res  = page.attrib['width'], page.attrib['height'], page.attrib['resolution']\n",
    "    non_table_char_coor = []\n",
    "    for block in page.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}block'):\n",
    "        if block.attrib[\"blockType\"] == 'Text':\n",
    "            for text in block.findall('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}text'):\n",
    "                for line in text.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}line'):  \n",
    "                    for ch in line.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}charParams'):\n",
    "                        non_table_char_coor.append({\"text\": ch.text, \n",
    "                                                    \"l\": int(ch.attrib['l']), \"t\": int(ch.attrib['t']),\n",
    "                                                    \"r\": int(ch.attrib['r']), \"b\": int(ch.attrib['b']),\n",
    "                                                    \"l_\": float(int(ch.attrib['l'])/int(page_width)), \n",
    "                                                    \"t_\": float(int(ch.attrib['t'])/int(page_height)), \n",
    "                                                    \"r_\": float(int(ch.attrib['r'])/int(page_width)), \n",
    "                                                    \"b_\": float(int(ch.attrib['b'])/int(page_height)),\n",
    "                                                    \"page_width\": int(page_width), \"page_height\": int(page_height)})\n",
    "                    non_table_char_coor.append({\"text\": \"<<LINEBREAK>>\", \"l\": \"\", \"t\": \"\", \"r\": \"\", \"b\": \"\", \n",
    "                                                \"l_\": \"\", \"t_\": \"\", \"r_\": \"\", \"b_\": \"\", \"page_width\": \"\", \"page_height\": \"\"})\n",
    "    non_table_word_dict = get_words_from_char(non_table_char_coor)\n",
    "    return non_table_word_dict\n",
    "\n",
    "def get_table_blocks(page):\n",
    "    \"\"\"\n",
    "    Creates a dict of all words present inside the table area.\n",
    "    :param: obj page: XML object of page\n",
    "    :return: A dict of all words inside the table area with Coordinates, FS, Page-dims.\n",
    "    \"\"\"\n",
    "    page_width, page_height, page_res  = page.attrib['width'], page.attrib['height'], page.attrib['resolution']\n",
    "    table_char_coor = []\n",
    "    for block in page.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}block'):\n",
    "        if block.attrib[\"blockType\"] == 'Table':\n",
    "            for row in block.findall('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}row'):\n",
    "                for cell in row.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}cell'):\n",
    "                    for text in cell.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}text'):\n",
    "                        for line in text.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}line'):\n",
    "                            for ch in line.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}charParams'):\n",
    "                                table_char_coor.append({\"text\": ch.text, \n",
    "                                                        \"l\": int(ch.attrib['l']), \"t\": int(ch.attrib['t']), \n",
    "                                                        \"r\": int(ch.attrib['r']), \"b\": int(ch.attrib['b']),\n",
    "                                                        \"l_\": float(int(ch.attrib['l'])/int(page_width)), \n",
    "                                                        \"t_\": float(int(ch.attrib['t'])/int(page_height)), \n",
    "                                                        \"r_\": float(int(ch.attrib['r'])/int(page_width)), \n",
    "                                                        \"b_\": float(int(ch.attrib['b'])/int(page_height)),\n",
    "                                                        \"page_width\": int(page_width), \"page_height\": int(page_height)})\n",
    "                            table_char_coor.append({\"text\": \"<<LINEBREAK>>\", \"l\": \"\", \"t\": \"\", \"r\": \"\", \"b\": \"\", \n",
    "                                                    \"l_\": \"\", \"t_\": \"\", \"r_\": \"\", \"b_\": \"\", \"page_width\": \"\", \"page_height\": \"\"})\n",
    "    table_word_dict = get_words_from_char(table_char_coor)\n",
    "    return table_word_dict\n",
    "\n",
    "def get_invoice_pages(sub_folder_location, complete_extraction):\n",
    "    \"\"\"\n",
    "    Finds all words(table-block, non-table block) in all pages and their coords, fs, page-dim.\n",
    "    :param: string sub_folder_location: Path of folder containing XML file\n",
    "    :return: List of 1st page with words and attributes.\n",
    "    \"\"\"\n",
    "    # read only those files which has xml and images, few folders contain zips files and xlxs\n",
    "    all_pages = []\n",
    "    print(\"DIR : {}\".format(sub_folder_location))\n",
    "    for file in os.listdir(sub_folder_location):\n",
    "        file_add = os.path.join(sub_folder_location, file)\n",
    "        if file.endswith(\".xml\"):\n",
    "            tree = ET.parse(file_add)\n",
    "            r = tree.getroot()\n",
    "            pages = r.iter('{http://www.abbyy.com/FineReader_xml/FineReader10-schema-v1.xml}page')\n",
    "            for page in pages:\n",
    "                \n",
    "                # Page Dims\n",
    "                page_width, page_height, page_res  = page.attrib['width'], page.attrib['height'], page.attrib['resolution']\n",
    "                \n",
    "                # Non-Table Text\n",
    "                non_table_blocks = get_non_table_blocks(page)\n",
    "                \n",
    "                # Table Text\n",
    "                # - NOTE: Uncoment for table extraction\n",
    "                if complete_extraction == True:\n",
    "                    table_blocks = get_table_blocks(page)\n",
    "                else:\n",
    "                    # Not extarcting data from table region\n",
    "                    table_blocks = []\n",
    "                \n",
    "                # Add: Non Table + Table data\n",
    "                all_blocks = non_table_blocks + table_blocks\n",
    "                \n",
    "                # All Pages in XML\n",
    "                all_pages.extend(all_blocks)\n",
    "                \n",
    "                # *** NOTE *** \n",
    "                # considering only PAGE 1\n",
    "                break\n",
    "                \n",
    "    return all_pages\n",
    "\n",
    "def get_all_words(all_word_dict):\n",
    "    \"\"\"\n",
    "    Extracts meta-data from dict of words/lines\n",
    "    :param: dict all_word_dict: A dict of all words across all pages with coords, fs, page-dim\n",
    "    :return: \n",
    "    string text: The text from the entire page\n",
    "    dict text_dict: A dictionary containing word: meta-data from the entire page\n",
    "    list lines: A list of all lines from the entire page\n",
    "    dict line_dict: A dictionary containing line: meta-data from the entire page\n",
    "    \"\"\"\n",
    "    X, Y, X_, Y_ = [], [], [], []\n",
    "    font_size, font_size_, total_words = [], [], 0\n",
    "    page_width, page_height = 0, 0\n",
    "    text, text_dict = \" \", []\n",
    "    line_text, line_dict = \" \", []\n",
    "    lines = []\n",
    "    for w in all_word_dict:\n",
    "        if w != \"<<LINEBREAK>>\":\n",
    "            # Word: l,t,r,b, font_size, l*, t*, r*, b*, font_size*\n",
    "            text_dict.append({'word': str(w[\"content\"]), 'l': w['l'], 't': w['t'], 'r': w['r'], 'b': w['b'], \n",
    "                              'font_size': w['font_size'], 'l_': w['l_'], 't_': w['t_'], 'r_': w['r_'], 'b_': w['b_'], \n",
    "                              'font_size_': w['font_size_'], 'page_width': w['page_width'], 'page_height': w['page_height']})\n",
    "            # Text\n",
    "            text = text + str(w[\"content\"]) + \" \"\n",
    "            # Line: Bounding Box of all words in line\n",
    "            # 1. Normal Coords\n",
    "            X.append(int(w['l']))\n",
    "            X.append(int(w['r']))\n",
    "            Y.append(int(w['t']))\n",
    "            Y.append(int(w['b']))\n",
    "            # 2. Regularized Coords\n",
    "            X_.append(float(w['l_']))\n",
    "            X_.append(float(w['r_']))\n",
    "            Y_.append(float(w['t_']))\n",
    "            Y_.append(float(w['b_']))\n",
    "            # Font-size of normal & regularized coords\n",
    "            font_size.append(int(w['font_size']))\n",
    "            font_size_.append(float(w['font_size_']))\n",
    "            total_words += 1\n",
    "            # Page Dims\n",
    "            page_width, page_height =  w['page_width'], w['page_height']\n",
    "            line_text = line_text + str(w[\"content\"]) + \" \" \n",
    "\n",
    "        else:\n",
    "            # SAVE LINE DICT\n",
    "            l, t, r, b = get_word_bounding_box(X, Y)\n",
    "            l_, t_, r_, b_ = get_word_bounding_box(X_, Y_)\n",
    "            avg_font_size = get_average_font_size(font_size, total_words, int)\n",
    "            avg_font_size_ = get_average_font_size(font_size_, total_words, float)\n",
    "            line_dict.append({'line': line_text.strip(), 'l':l, 't':t, 'r':r, 'b':b, 'font_size':avg_font_size, \n",
    "                              'l_':l_, 't_':t_, 'r_':r_, 'b_':b_, 'font_size_':avg_font_size_,\n",
    "                              'page_width': page_width, 'page_height': page_height})\n",
    "            # SAVE LINE TEXT\n",
    "            lines.append(line_text.strip())\n",
    "            # Initilaize\n",
    "            X, Y, X_, Y_ = [], [], [], []\n",
    "            font_size, font_size_, total_words = [], [], 0\n",
    "            page_width, page_height = 0, 0\n",
    "            line_text = \" \"\n",
    "    text = text.strip()\n",
    "    return text, text_dict, lines, line_dict\n",
    "\n",
    "def extract_data(directory, complete_extraction):\n",
    "    \"\"\"\n",
    "    Extracts linguistic meta-data for each XML file.\n",
    "    :param: string directory: Path where all XML files are located\n",
    "    :return: List of text, text-with-attributes, line, line-with-attributes for all XML files.\n",
    "    \"\"\"\n",
    "    all_text, all_text_dict, all_lines, all_line_dict = [], [], [], []\n",
    "    try:\n",
    "        # Extract XML Text data\n",
    "        all_words = get_invoice_pages(directory, complete_extraction)\n",
    "        text, text_dict, lines, line_dict = get_all_words(all_words)\n",
    "        print(\"Words = \", len(text))\n",
    "        # Check for non xml sub-folders...\n",
    "        if len(text) == 0:\n",
    "            print(\"No XML file found in {}\".format(sub_folder))\n",
    "            text = np.nan\n",
    "        # Stores all text\n",
    "        all_text.append(text)\n",
    "        all_text_dict.append(text_dict)\n",
    "        all_lines.append(lines)\n",
    "        all_line_dict.append(line_dict)\n",
    "        print(\"Extracted !\")\n",
    "        print(\"******************\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in extraction! Error = {}\".format(str(e)))\n",
    "        print(\"******************\")\n",
    "        return [\"error\"]*4\n",
    "    return all_text, all_text_dict, all_lines, all_line_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:27.671349Z",
     "iopub.status.busy": "2021-02-04T08:30:27.670471Z",
     "iopub.status.idle": "2021-02-04T08:30:27.674647Z",
     "shell.execute_reply": "2021-02-04T08:30:27.673982Z"
    },
    "papermill": {
     "duration": 0.226154,
     "end_time": "2021-02-04T08:30:27.674809",
     "exception": false,
     "start_time": "2021-02-04T08:30:27.448655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Create_Data(data_path, complete_extraction):\n",
    "    \"\"\"\n",
    "    Extracts linguistic meta-data for all XML files ilocated in given path.\n",
    "    :param: string directory: Path where all XML files are located\n",
    "    :return: List of text, text-with-attributes, line, line-with-attributes for all XML files.\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    mapping_notfound = []\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(data_path):\n",
    "        print(file)\n",
    "        # Reading a XML file\n",
    "        print(\"#>\", counter)\n",
    "        AID = re.findall(r\"(.*)\", str(file))[0]\n",
    "        text_data, text_dict, lines_data, lines_dict = extract_data(data_path + file + \"/\", complete_extraction)\n",
    "        if type(text_data) == str:\n",
    "            if set([text_data, text_dict, lines_data, lines_dict]) == {\"error\"}:\n",
    "                counter+=1\n",
    "                continue\n",
    "        \n",
    "        # Storing in a df\n",
    "        df_data = pd.DataFrame()\n",
    "        df_data['LINES'] = pd.Series(lines_data[0])\n",
    "        df_data['LINES_DICT'] = pd.Series(lines_dict[0])\n",
    "        df_data['TEXT'] = text_data[0]\n",
    "        df_data['WORDS'] = pd.Series(text_dict*df_data.shape[0])\n",
    "        df_data['FILE_INDEX'] = [counter] * df_data.shape[0]\n",
    "        df_data['FILENAME'] = str(file)\n",
    "\n",
    "        # Finding 'Target variable Y_SN' using mapper file\n",
    "        if AID in mapper_file.FILENAME.values:\n",
    "            mapper_df = mapper_file[mapper_file.FILENAME == AID].copy()\n",
    "            df_data['SUPPLIER_NAME'] = mapper_df['SUPPLIER_NAME'].values[0]\n",
    "        else:\n",
    "            print(\"Supplier Name Mapping DOESN'T EXIST for - \", file)\n",
    "            mapping_notfound.append(file)\n",
    "\n",
    "        # Appending in a common df\n",
    "        df = df.append(df_data)\n",
    "        counter+=1\n",
    "    \n",
    "    # Final df\n",
    "    df = df.reindex(columns=['FILE_INDEX', 'FILENAME', 'TEXT', 'WORDS', 'LINES', 'LINES_DICT', \n",
    "                             'SUPPLIER_NAME']).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_complete --> For Address Block extraction\n",
    "df_complete = Create_Data(data_path, complete_extraction=True)\n",
    "\n",
    "# df --> For Supplier Name extraction\n",
    "df = Create_Data(data_path, complete_extraction=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.210442,
     "end_time": "2021-02-04T08:30:29.223540",
     "exception": false,
     "start_time": "2021-02-04T08:30:29.013098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.207427,
     "end_time": "2021-02-04T08:30:29.639508",
     "exception": false,
     "start_time": "2021-02-04T08:30:29.432081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>7. Intial Preprocessing Module </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211461,
     "end_time": "2021-02-04T08:30:30.060969",
     "exception": false,
     "start_time": "2021-02-04T08:30:29.849508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Helper Functions: Finding NER entities in `df.Lines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:30.495457Z",
     "iopub.status.busy": "2021-02-04T08:30:30.489806Z",
     "iopub.status.idle": "2021-02-04T08:30:30.508607Z",
     "shell.execute_reply": "2021-02-04T08:30:30.508051Z"
    },
    "papermill": {
     "duration": 0.240249,
     "end_time": "2021-02-04T08:30:30.508768",
     "exception": false,
     "start_time": "2021-02-04T08:30:30.268519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_email(s):\n",
    "    \"\"\"\n",
    "    Extracts NER email in text.\n",
    "    :param: string s: Paragraph or line\n",
    "    :return: Returns email-address and email-domain.\n",
    "    \"\"\"\n",
    "    LENGTH_OF_EMAIL = 3\n",
    "    email_tags = re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    email_tags = [email.strip() for email in email_tags if len(email) >= LENGTH_OF_EMAIL]\n",
    "    email_domains = re.findall(r\"[\\w\\.-]+@([\\w\\.-]+)\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    email_domains = list(map(lambda x: x.split('.')[0].lower(), email_domains))\n",
    "    return email_tags, email_domains\n",
    "\n",
    "def find_url(s):   \n",
    "    \"\"\"\n",
    "    Extracts NER url in text.\n",
    "    :param: string s: Paragraph or line\n",
    "    :return: Returns all url-address, non-email url-addres, non-email url-domains.\n",
    "    \"\"\"\n",
    "    # removed emails and decimal values to reduce noise\n",
    "    s = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \" \", s, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE)\n",
    "    s = re.sub(r\"\\d+\\.\\d+\", \" \", s, flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "    \n",
    "    # TYPE 1:  On entire data that may/maynot have email attributes like (From: <URL> To: <URL>)\n",
    "    url_tags = re.findall(r'(https://www.|http://www.|ftp://www.|http://|ftp://|https://|www.)+([A-z_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', s, flags=re.IGNORECASE | re.MULTILINE)    \n",
    "    url_tags = list(map(lambda x: \"\".join(x), url_tags))\n",
    "    \n",
    "    # TYPE 2:  On data that DO-NOT have email attributes - ** !! IMP FOR CHECKING DOMAIN NAME MATCHING !!**\n",
    "    s_ = re.sub('[from|to|cc|bcc|subject]+\\s*\\:+\\s+(http://|ftp://|https://|www.)+([A-z_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', \" \", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    url_tags_ = re.findall(r'(https://www.|http://www.|ftp://www.|http://|ftp://|https://|www.)+([A-z_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', s_, flags=re.IGNORECASE | re.MULTILINE)    \n",
    "    url_domains = list(map(lambda x: x[1].split('.')[0].lower(), url_tags_))\n",
    "    url_tags_ = list(map(lambda x: \"\".join(x), url_tags_))\n",
    "    \n",
    "    return url_tags, url_tags_, url_domains\n",
    "\n",
    "def find_phone(s):\n",
    "    \"\"\"\n",
    "    Extracts NER telephone in text.\n",
    "    :param: string s: Paragraph or line\n",
    "    :return: Returns telephone tags.\n",
    "    \"\"\"\n",
    "    phone_tags = re.findall(r\"\\s*(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})(?: *x(\\d+))?\\s*\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    return phone_tags\n",
    "\n",
    "def find_money(s):\n",
    "    \"\"\"\n",
    "    Extracts NER currency in text.\n",
    "    :param: tring s: Paragraph or line\n",
    "    :return: Returns currency tags in regex pattern.\n",
    "    \n",
    "    MONEY TAGS\n",
    "    ----------\n",
    "    '$': 'USD',\n",
    "    'â¬': 'EUR',\n",
    "    'Â¥': 'JPY',\n",
    "    'Â£': 'GBP',\n",
    "    'â ': 'EUR',\n",
    "    'â¨': 'INR',\n",
    "    'â¹': 'INR',\n",
    "    'âº': 'TRY',\n",
    "    'å': 'CNY',\n",
    "    'â½': 'RUB',\n",
    "    'â©': 'KRW'\n",
    "    \"\"\"         \n",
    "    # \"lexNLP\" sometimes catches \"Cost\" or \"Amount\" keywords as money-phrases! Thus removing these.\n",
    "    s = re.sub(r\"\\bcost\\b\\s+|\\bamount\\b\\s+|\\bcostamount\\b\\s+|\\bcosts\\b\\s+|\\bamounts\\b\\s+|\\bprice\\b\\s+|\\bpriced\\b\\s+\", \" \", s.strip().lower())\n",
    "    # lexNLP\n",
    "    money_tags = list(lexnlp.extract.en.money.get_money(s, return_sources=True))\n",
    "    money_tags = [str(tag[2]) for tag in money_tags]\n",
    "    money_tags = [tags for tags in money_tags if len(re.sub(r'\\s+', ' ', str(tags)).split(' ')) < 4]\n",
    "    # to regex format\n",
    "    money_tags_RE = [tag.replace('$','\\$').replace('Â£','\\Â£').replace('â¬','\\â¬').replace('Â¥','\\Â¥').replace('â ','\\â ').replace('â¨','\\â¨').replace('â¹','\\â¹').replace('âº','\\âº').replace('å','\\å').replace('â½','\\â½').replace('â©','\\â©').strip() for tag in money_tags]\n",
    "    return money_tags_RE\n",
    "\n",
    "def find_date(s):\n",
    "    \"\"\"\n",
    "    Extracts NER date in text.\n",
    "    :param: string s: Paragraph or line\n",
    "    :return: Returns date tags in regex pattern.\n",
    "    \"\"\"\n",
    "    date_tags = []\n",
    "    # 10/10/2015\n",
    "    date_tags += re.findall(r\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    # 1-11-10\n",
    "    date_tags += re.findall(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{2}\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    # 1 NOV 2010\n",
    "    date_tags += re.findall(r\"([\\d]{1,2}\\s(?:JAN|NOV|OCT|DEC)\\s[\\d]{4})\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    # 1 November 2010\n",
    "    date_tags += re.findall(r\"(\\d{1,2} (?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{4})\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    # November 1, 2010\n",
    "    date_tags += re.findall(r\"(\\s*(?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}\\,* \\d{4})\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    # to regex format\n",
    "    date_tags_RE = [tag.replace('/','\\/').replace(\"\\\\\",\"\\\\\").replace('-','\\-').replace(',','\\,').replace('.','\\.').replace('|','\\|').strip() for tag in date_tags]\n",
    "    return date_tags_RE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.209964,
     "end_time": "2021-02-04T08:30:30.924648",
     "exception": false,
     "start_time": "2021-02-04T08:30:30.714684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Pre-Processing in `df.LINES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:31.358879Z",
     "iopub.status.busy": "2021-02-04T08:30:31.357808Z",
     "iopub.status.idle": "2021-02-04T08:30:31.361059Z",
     "shell.execute_reply": "2021-02-04T08:30:31.360539Z"
    },
    "papermill": {
     "duration": 0.22366,
     "end_time": "2021-02-04T08:30:31.361210",
     "exception": false,
     "start_time": "2021-02-04T08:30:31.137550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lines_cleaned(line):\n",
    "    \"\"\"\n",
    "    Performs basic-level cleansing on text.\n",
    "    :param: string s: line\n",
    "    :return: Returns cleaned line.\n",
    "    \"\"\"\n",
    "\n",
    "    # Removes a line if it contains: only 1 digit, only 1 letter, only symbols, or it is empty\n",
    "    def find_outliers(line):\n",
    "        original_line = line\n",
    "        line = str(line).strip().lower()\n",
    "        line_stripped = re.sub(\"\\s+\", \"\", re.sub(\"[\"+REGEX_PUNCTS+\"]+\", \"\", re.sub(r\"[^A-z0-9]\", \"\", line))).strip()\n",
    "        if len(line) < MINIMUM_CHARS_IN_LINE or len(line_stripped) < MINIMUM_CHARS_IN_LINE \\\n",
    "           or re.match(r'^\\s*[total|amount|sum|whole|total amount|totals|amounts|final|final amount|final total]+\\s*\\d*$', \n",
    "                       line_stripped) != None:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return original_line\n",
    "\n",
    "    # Normalizes accented characters in a line.\n",
    "    def find_accentedChars(line):\n",
    "        accented_text = unicodedata.normalize('NFKD', line).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return accented_text\n",
    "    \n",
    "    # Expands contarcted words in a line.\n",
    "    def find_contractions(line):\n",
    "        expanded_text = list(pd.Series(re.sub(r\"\\â\", \"\\'\", line).split('\\n')).apply(lambda x: re.sub(r\"(\\w+\\'\\w+)\", lambda x: contractions.get(x.group().lower(), x.group().lower()), x)))\n",
    "        return \"\\n\".join(expanded_text)\n",
    "    \n",
    "    # Removes unwanted symbols from lines\n",
    "    def find_symbols(line):\n",
    "        return re.sub(r\"[\"+REGEX_SYMBOLS+\"]+\", \" \", line, flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "    \n",
    "    # Removes line tag \"<&>\" present in a line. Removing it is important for postal_tagging step\n",
    "    def find_lineTag(line):\n",
    "        LINE_TAG_RE = LINE_TAG.replace('<','\\<').replace('&','\\&').replace('>','\\>').strip()\n",
    "        line = re.sub(LINE_TAG_RE, \" \", line, flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "        return line\n",
    "    \n",
    "    # CLEANING\n",
    "    line = find_outliers(line)\n",
    "    if str(line) != 'nan':\n",
    "        line = find_accentedChars(line)\n",
    "        line = find_contractions(line)\n",
    "        line = find_symbols(line)\n",
    "        line = find_lineTag(line)\n",
    "        line = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\n\\t\\r]+\", \"  \", line, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_complete\n",
    "original_shape = df_complete.shape[0]\n",
    "df_complete['LINES'] = df_complete.LINES.apply(get_lines_cleaned)\n",
    "df_complete = df_complete.dropna(subset=['LINES']).reset_index(drop=True)\n",
    "print(\"Original Shape (before cleaning) = \", original_shape, \"\\nCleaned Shape = \", df_complete.shape[0], \"\\n\")\n",
    "\n",
    "\n",
    "# df\n",
    "original_shape = df.shape[0]\n",
    "df['LINES'] = df.LINES.apply(get_lines_cleaned)\n",
    "df = df.dropna(subset=['LINES']).reset_index(drop=True)\n",
    "print(\"Original Shape (before cleaning) = \", original_shape, \"\\nCleaned Shape = \", df.shape[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.208722,
     "end_time": "2021-02-04T08:30:32.475320",
     "exception": false,
     "start_time": "2021-02-04T08:30:32.266598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### NER Entities Replacement with whitesapce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.207981,
     "end_time": "2021-02-04T08:30:32.892238",
     "exception": false,
     "start_time": "2021-02-04T08:30:32.684257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:33.320088Z",
     "iopub.status.busy": "2021-02-04T08:30:33.319346Z",
     "iopub.status.idle": "2021-02-04T08:30:33.322541Z",
     "shell.execute_reply": "2021-02-04T08:30:33.321925Z"
    },
    "papermill": {
     "duration": 0.219352,
     "end_time": "2021-02-04T08:30:33.322685",
     "exception": false,
     "start_time": "2021-02-04T08:30:33.103333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Accepts cleaned/pre-processed 'df.LINES' and performs entity recognition and its replacement with tags.\n",
    "# def get_lines_preprocessed(line):\n",
    "    \n",
    "#     # REPLACE entities like: | EMAIL | URL | MONEY | DATE |\n",
    "#     def replace_email(s):\n",
    "#         try:\n",
    "#             email_tags, _ = find_email(s)\n",
    "#             for tag in email_tags:\n",
    "#                 s = re.sub(tag, EMAIL_TAG, s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#         return s\n",
    "#     def replace_url(s):\n",
    "#         try:\n",
    "#             url_tags, _, _ = find_url(s)\n",
    "#             for tag in url_tags:\n",
    "#                 s = re.sub(tag, URL_TAG, s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#         return s\n",
    "#     def replace_money(s):\n",
    "#         try:\n",
    "#             money_tags = find_money(s)\n",
    "#             for tag in money_tags:\n",
    "#                 s = re.sub(tag, MONEY_TAG, s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#         return s\n",
    "#     def replace_date(s):\n",
    "#         try:\n",
    "#             date_tags = find_date(s)\n",
    "#             for tag in date_tags:\n",
    "#                 s = re.sub(tag, DATE_TAG, s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#         return s\n",
    "    \n",
    "#     # finds NER entity and replaces them with assigned TAGS - for feature extraction\n",
    "#     line = replace_email(line)\n",
    "#     line = replace_url(line)\n",
    "#     line = replace_date(line)\n",
    "#     line = replace_money(line)\n",
    "#     return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.206801,
     "end_time": "2021-02-04T08:30:33.741670",
     "exception": false,
     "start_time": "2021-02-04T08:30:33.534869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.208515,
     "end_time": "2021-02-04T08:30:34.161130",
     "exception": false,
     "start_time": "2021-02-04T08:30:33.952615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>8. Address Block (AB) Identification Module </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.208168,
     "end_time": "2021-02-04T08:30:34.622079",
     "exception": false,
     "start_time": "2021-02-04T08:30:34.413911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Approach 1: Using Custom NER (biLSTM-CRF): BILUO Tagging and tag prediciton\n",
    "- Approach 2: Using Word-Level features, n-grams with a Decision Tree model\n",
    "- **Approach 3: Using lexNLP, libpostal and a scoring decision matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.231668,
     "end_time": "2021-02-04T08:30:35.062318",
     "exception": false,
     "start_time": "2021-02-04T08:30:34.830650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "....................................................................................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.210158,
     "end_time": "2021-02-04T08:30:35.481090",
     "exception": false,
     "start_time": "2021-02-04T08:30:35.270932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Approach 1\n",
    "Custom NER (biLSTM-CRF): BILUO Tagging and tag prediciton\n",
    "\n",
    "\n",
    "- All lines are taken and are subjected to spacy NER tagging. Then these lines are subjected to Spacy GoldParser and BILUO tags are created. A BiLSTM-CRF model is trained on these BILUO tags to correctly predict a tag when bunch of new lines are passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:35.906067Z",
     "iopub.status.busy": "2021-02-04T08:30:35.905370Z",
     "iopub.status.idle": "2021-02-04T08:30:35.908647Z",
     "shell.execute_reply": "2021-02-04T08:30:35.907990Z"
    },
    "papermill": {
     "duration": 0.218219,
     "end_time": "2021-02-04T08:30:35.908801",
     "exception": false,
     "start_time": "2021-02-04T08:30:35.690582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_BILOU_tagging(df):\n",
    "    \n",
    "#     def get_BILUO_tags(file_num, line_num, text, sn):\n",
    "        \n",
    "#         file_num = \"FILE#\" + str(file_num)\n",
    "#         line_num = \"LINE#\" + str(line_num)\n",
    "\n",
    "#         text = re.sub(r\"[\\n\\t\\r]+\", \" \", text)\n",
    "#         text = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9!!@#$%&?]+\", \" \", text))\n",
    "#         text = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\`\\~\\^\\*\\(\\)\\[\\]\\{\\}\\:\\;\\'\\\"\\,\\<\\.\\>\\/\\\\\\|\\-\\_\\=\\+]+\", \" \", text))\n",
    "#         sn = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9!@#$%&?]+\", \" \", sn))\n",
    "#         # indexes of regex match\n",
    "#         match_indexs = [(a.start(), a.end()) for a in list(re.finditer(sn.lower(), text.lower()))]\n",
    "\n",
    "#         # Spacy GOLD parser\n",
    "#         doc = nlp(text)\n",
    "#         pos_tags = [pos[1] for pos in nltk.pos_tag(word_tokenize(text))]\n",
    "#         entities = [tuple(list(i) + [\"SN\"]) for i in match_indexs]\n",
    "#         tags = biluo_tags_from_offsets(doc, entities)\n",
    "#         BILUO = [(file_num, line_num, str(token), str(pos), tag) for token,pos,tag in zip(doc, pos_tags, tags)]\n",
    "#         return BILUO\n",
    "    \n",
    "#     # Creating tag line-by-line...\n",
    "#     list_line_word_tags = []\n",
    "#     line_num = 0\n",
    "#     for file_num, line, sn in zip(df.index.values, df.TEXT.values, df.SUPPLIER_NAME.values):\n",
    "#         list_line_word_tags.append(get_BILUO_tags(file_num, line_num, line, sn))\n",
    "#         line_num+=1\n",
    "    \n",
    "#     # Extract attributes\n",
    "#     FILE_NO, LINE_NO, WORD, POS, TAG = zip(*sum(list_line_word_tags, []))\n",
    "#     tag_df = pd.DataFrame({\"File#\": FILE_NO, \"Line#\":LINE_NO, \"Word\": WORD, \"POS\": POS, \"Tag\": TAG})\n",
    "#     return tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:36.329291Z",
     "iopub.status.busy": "2021-02-04T08:30:36.328539Z",
     "iopub.status.idle": "2021-02-04T08:30:36.331240Z",
     "shell.execute_reply": "2021-02-04T08:30:36.331970Z"
    },
    "papermill": {
     "duration": 0.214885,
     "end_time": "2021-02-04T08:30:36.332149",
     "exception": false,
     "start_time": "2021-02-04T08:30:36.117264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_BILOU_SN = create_BILOU_tagging(df_text[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:36.758417Z",
     "iopub.status.busy": "2021-02-04T08:30:36.757623Z",
     "iopub.status.idle": "2021-02-04T08:30:36.761933Z",
     "shell.execute_reply": "2021-02-04T08:30:36.761199Z"
    },
    "papermill": {
     "duration": 0.219403,
     "end_time": "2021-02-04T08:30:36.762093",
     "exception": false,
     "start_time": "2021-02-04T08:30:36.542690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def preprocess_bilou(x):\n",
    "#     w = x.Word\n",
    "#     t = x.Tag\n",
    "#     original_w = w\n",
    "#     w = str(w).strip().lower()\n",
    "#     if t not in ['B-SN', 'I-SN', 'L-SN', 'U-SN']:\n",
    "#         if len(w) < 4 or w.isdigit() or re.sub(\"\\s+\", \"\", re.sub(r\"[^A-z0-9]\", \" \", w)).strip().isdigit() or len(re.sub(\"\\s+\", \"\", re.sub(r\"[^A-z0-9]\", \" \", w)).strip()) == 0:\n",
    "#             return np.nan\n",
    "#     return original_w\n",
    "    \n",
    "# data = df_BILOU_SN.copy()\n",
    "# data[\"Word\"] = data.apply(preprocess_bilou, axis=1)\n",
    "# data = data.dropna(subset=['Word']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:37.190696Z",
     "iopub.status.busy": "2021-02-04T08:30:37.189623Z",
     "iopub.status.idle": "2021-02-04T08:30:37.192710Z",
     "shell.execute_reply": "2021-02-04T08:30:37.192088Z"
    },
    "papermill": {
     "duration": 0.219556,
     "end_time": "2021-02-04T08:30:37.192895",
     "exception": false,
     "start_time": "2021-02-04T08:30:36.973339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# words = list(set(data[\"Word\"].values))\n",
    "# words.append(\"ENDPAD\")\n",
    "# n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:37.626911Z",
     "iopub.status.busy": "2021-02-04T08:30:37.625714Z",
     "iopub.status.idle": "2021-02-04T08:30:37.629934Z",
     "shell.execute_reply": "2021-02-04T08:30:37.629351Z"
    },
    "papermill": {
     "duration": 0.223775,
     "end_time": "2021-02-04T08:30:37.630085",
     "exception": false,
     "start_time": "2021-02-04T08:30:37.406310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tags = list(set(data[\"Tag\"].values))\n",
    "# n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:38.070549Z",
     "iopub.status.busy": "2021-02-04T08:30:38.069863Z",
     "iopub.status.idle": "2021-02-04T08:30:38.074171Z",
     "shell.execute_reply": "2021-02-04T08:30:38.074948Z"
    },
    "papermill": {
     "duration": 0.232642,
     "end_time": "2021-02-04T08:30:38.075161",
     "exception": false,
     "start_time": "2021-02-04T08:30:37.842519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_len = 100\n",
    "# word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "# tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:38.513276Z",
     "iopub.status.busy": "2021-02-04T08:30:38.512554Z",
     "iopub.status.idle": "2021-02-04T08:30:38.516233Z",
     "shell.execute_reply": "2021-02-04T08:30:38.515500Z"
    },
    "papermill": {
     "duration": 0.225322,
     "end_time": "2021-02-04T08:30:38.516426",
     "exception": false,
     "start_time": "2021-02-04T08:30:38.291104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SentenceGetter(object):\n",
    "    \n",
    "#     def __init__(self, data):\n",
    "#         self.n_sent = 1\n",
    "#         self.data = data\n",
    "#         self.empty = False\n",
    "#         agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "#                                                            s[\"POS\"].values.tolist(),\n",
    "#                                                            s[\"Tag\"].values.tolist())]\n",
    "#         self.grouped = self.data.groupby(\"Line#\").apply(agg_func)\n",
    "#         self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "#     def get_next(self):\n",
    "#         try:\n",
    "#             s = self.grouped[\"LINE#{}\".format(self.n_sent)]\n",
    "#             self.n_sent += 1\n",
    "#             return s\n",
    "#         except:\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:38.957162Z",
     "iopub.status.busy": "2021-02-04T08:30:38.956418Z",
     "iopub.status.idle": "2021-02-04T08:30:38.959550Z",
     "shell.execute_reply": "2021-02-04T08:30:38.960354Z"
    },
    "papermill": {
     "duration": 0.226291,
     "end_time": "2021-02-04T08:30:38.960542",
     "exception": false,
     "start_time": "2021-02-04T08:30:38.734251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getter = SentenceGetter(data)\n",
    "# sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:39.400143Z",
     "iopub.status.busy": "2021-02-04T08:30:39.399388Z",
     "iopub.status.idle": "2021-02-04T08:30:39.403674Z",
     "shell.execute_reply": "2021-02-04T08:30:39.402944Z"
    },
    "papermill": {
     "duration": 0.225699,
     "end_time": "2021-02-04T08:30:39.403869",
     "exception": false,
     "start_time": "2021-02-04T08:30:39.178170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pad the sequence\n",
    "# X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "# X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words-1)\n",
    "\n",
    "# # pad the target\n",
    "# y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "# y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "# y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:39.832897Z",
     "iopub.status.busy": "2021-02-04T08:30:39.832060Z",
     "iopub.status.idle": "2021-02-04T08:30:39.835935Z",
     "shell.execute_reply": "2021-02-04T08:30:39.835205Z"
    },
    "papermill": {
     "duration": 0.217686,
     "end_time": "2021-02-04T08:30:39.836089",
     "exception": false,
     "start_time": "2021-02-04T08:30:39.618403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=n_words+1, output_dim=200, input_length=max_len))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "# model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "# crf_layer = CRF(n_tags)\n",
    "# model.add(crf_layer)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:40.264972Z",
     "iopub.status.busy": "2021-02-04T08:30:40.264254Z",
     "iopub.status.idle": "2021-02-04T08:30:40.268104Z",
     "shell.execute_reply": "2021-02-04T08:30:40.267444Z"
    },
    "papermill": {
     "duration": 0.220246,
     "end_time": "2021-02-04T08:30:40.268264",
     "exception": false,
     "start_time": "2021-02-04T08:30:40.048018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "# model.compile(optimizer='adam', loss=crf_layer.loss_function, metrics=[crf_layer.accuracy])\n",
    "# history = model.fit(X_tr, np.array(y_tr), batch_size=256, epochs=10, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:40.700200Z",
     "iopub.status.busy": "2021-02-04T08:30:40.699357Z",
     "iopub.status.idle": "2021-02-04T08:30:40.703049Z",
     "shell.execute_reply": "2021-02-04T08:30:40.702432Z"
    },
    "papermill": {
     "duration": 0.223838,
     "end_time": "2021-02-04T08:30:40.703209",
     "exception": false,
     "start_time": "2021-02-04T08:30:40.479371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hist = pd.DataFrame(history.history)\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(hist[\"crf_viterbi_accuracy\"])\n",
    "# plt.plot(hist[\"val_crf_viterbi_accuracy\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:41.138426Z",
     "iopub.status.busy": "2021-02-04T08:30:41.137572Z",
     "iopub.status.idle": "2021-02-04T08:30:41.142191Z",
     "shell.execute_reply": "2021-02-04T08:30:41.141574Z"
    },
    "papermill": {
     "duration": 0.227193,
     "end_time": "2021-02-04T08:30:41.142344",
     "exception": false,
     "start_time": "2021-02-04T08:30:40.915151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_pred = model.predict(X_te, verbose=1)\n",
    "\n",
    "# idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "# def pred2label(pred):\n",
    "#     out = []\n",
    "#     for pred_i in pred:\n",
    "#         out_i = []\n",
    "#         for p in pred_i:\n",
    "#             p_i = np.argmax(p)\n",
    "#             out_i.append(idx2tag[p_i].replace(\"PAD\", \"O\"))\n",
    "#         out.append(out_i)\n",
    "#     return out\n",
    "    \n",
    "# pred_labels = pred2label(test_pred)\n",
    "# test_labels = pred2label(y_te)\n",
    "# print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))\n",
    "# print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "# model.evaluate(X_te, np.array(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:41.574225Z",
     "iopub.status.busy": "2021-02-04T08:30:41.573073Z",
     "iopub.status.idle": "2021-02-04T08:30:41.576934Z",
     "shell.execute_reply": "2021-02-04T08:30:41.576363Z"
    },
    "papermill": {
     "duration": 0.220295,
     "end_time": "2021-02-04T08:30:41.577103",
     "exception": false,
     "start_time": "2021-02-04T08:30:41.356808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 10\n",
    "# p = model.predict(np.array([X_te[i]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "# true = np.argmax(y_te[i], -1)\n",
    "# print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(30 * \"=\")\n",
    "# for w, t, pred in zip(X_te[i], true, p[0]):\n",
    "#     if w != 0:\n",
    "#         print(\"{:15}: {:5} {}\".format(words[w-1], tags[t], tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:42.010331Z",
     "iopub.status.busy": "2021-02-04T08:30:42.009115Z",
     "iopub.status.idle": "2021-02-04T08:30:42.013412Z",
     "shell.execute_reply": "2021-02-04T08:30:42.012739Z"
    },
    "papermill": {
     "duration": 0.227648,
     "end_time": "2021-02-04T08:30:42.014407",
     "exception": false,
     "start_time": "2021-02-04T08:30:41.786759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Custom Tokenizer\n",
    "# re_tok = re.compile(f'([{string.punctuation}ââÂ¨Â«Â»Â®Â´Â·ÂºÂ½Â¾Â¿Â¡Â§Â£â¤ââ])')\n",
    "\n",
    "# def tokenize(s): \n",
    "#     return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:42.467779Z",
     "iopub.status.busy": "2021-02-04T08:30:42.467099Z",
     "iopub.status.idle": "2021-02-04T08:30:42.469245Z",
     "shell.execute_reply": "2021-02-04T08:30:42.469756Z"
    },
    "papermill": {
     "duration": 0.233478,
     "end_time": "2021-02-04T08:30:42.469954",
     "exception": false,
     "start_time": "2021-02-04T08:30:42.236476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in tokenize(test_sentence)]], \n",
    "#                             padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "# p = model.predict(np.array([x_test_sent[0]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "\n",
    "# print(\"{:15}||{}\".format(\"Word\", \"Prediction\"))\n",
    "# print(30 * \"=\")\n",
    "\n",
    "# for w, pred in zip(tokenize(test_sentence), p[0]):\n",
    "#     print(\"{:15}: {:5}\".format(w, tags[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211612,
     "end_time": "2021-02-04T08:30:42.895137",
     "exception": false,
     "start_time": "2021-02-04T08:30:42.683525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "....................................................................................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.210262,
     "end_time": "2021-02-04T08:30:43.370337",
     "exception": false,
     "start_time": "2021-02-04T08:30:43.160075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Approach 2\n",
    "Using Word-Level features, n-grams with a Decision Tree model\n",
    "\n",
    "- Creating n-grams for each word in a line. Constructing Word-Level features for each n-grma (middle word). Labelling the data using auto-label BILUO tags (using Spacy's GoldParser). Passing the constructed features to a classifier model Decision Tree for classifiying the middle term as S(Start), M(middle) and E(end) of an Address Block field. Finally passing the S,M,E values to a Scoring Decision Matrix(SME) for final chunk prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211695,
     "end_time": "2021-02-04T08:30:43.792866",
     "exception": false,
     "start_time": "2021-02-04T08:30:43.581171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<ins>STEPS</ins>\n",
    "\n",
    "\n",
    "`Step 1: Construct n-grams`\n",
    "\n",
    "1. The entire XML file is stored as a *'assumed sequential text'* in a paragraph P. For e.g. P = (w1, w2, w3, ..., wn)\n",
    "\n",
    "\n",
    "2. The paragraph is then broken in trigrams. For e.g. n-grams = ([_, w1, w2], [w1, w2, w3], [w2, w3, w4], ..., [wn-1, wn, _ ])\n",
    "\n",
    "...\n",
    "\n",
    "`Step 2: Construct Word-Level features`\n",
    "\n",
    "1. Word-Level features are exploited for each middle word. Word-Level features are: \n",
    "            \n",
    "            - is Digit\n",
    "            - is Text\n",
    "            - is AlphaNumeric\n",
    "            - is NER(email)\n",
    "            - is NER(url)\n",
    "            - is NRE(phone)\n",
    "            - is Punts marks\n",
    "            - is All Punct marks\n",
    "            - is Dictionary Word\n",
    "            - is 1st letter Capital Dictionary Word\n",
    "            - is All Capital Dictionary Word\n",
    "            - is Non Dictionary Word\n",
    "            - is 1st letter Capital Non Dictionary Word\n",
    "            - is All Capital Non Dictionary Word\n",
    "            \n",
    "            \n",
    "2. All features give BOOL values(1 or 0). For each middle word in n-gram, these BOOL values are taken and concatanated to create a feature set.\n",
    "\n",
    "\n",
    "3. For e.g. for each n-gram these features are calculated:\n",
    "\n",
    "        - (_,  w1, w2)  = CalWordLevelFeatures(w1) =  [1,0,1,0, ...., 1]\n",
    "        - (w1, w2, w3)  = CalWordLevelFeatures(w2) =  [0,0,0,0, ...., 0]\n",
    "        - (w2, w3, w4)  = CalWordLevelFeatures(w3) =  [1,1,1,1, ...., 1]\n",
    "          ...\n",
    "        - (wn-1, wn, _)  = CalWordLevelFeatures(wn) =  [0,1,1,1, ...., 0]\n",
    "\n",
    "...\n",
    "\n",
    "`Step 3: Labelling data`\n",
    "\n",
    "1. Each n-gram is considered and using some static rules, these n-grams are labelled: Start(S), Middle(M), End(E) or Other(O)\n",
    "\n",
    "2. For each n-gram (e.g. W1, W2, W3) are checked and based on below rules:\n",
    "\n",
    "            - RULE1: If W1 is a digit, and W2 or W3 contains a text mark the n-gram as S\n",
    "            - RULE2: If W3 contains a zipcode, mark the n-gram as E\n",
    "            - RULE3: If W1 & W3 donot contian a zipcode and W2 contains a city-name, state-name, mark the n-gram as M\n",
    "            - RULE4: If W1, W3 both contains a digit, mark the n-gram as O\n",
    "            - RULE5: If W3 contains a NER(phone) and W1 conatins a zipcode, mark the n-gram as E\n",
    "            - RULE6: If W3 contains a NER(email) and W1 conatins a zipcode, mark the n-gram as E\n",
    "            ... likewise\n",
    "            \n",
    "            \n",
    "3. This labelled data is passed to SpaCy's GoldParse to create BILUO tags: Beginning of a chunk(B-tag), Inside of a chunk(I-tag), Last of a chunk(L-tag), Unit chunk(U-tag), Outside of a chunk(O-tag). Finally all n-grams are labelled as BILUO tags.\n",
    "\n",
    "...\n",
    "\n",
    "`Step 4: Constuct a Decision Tree`\n",
    "\n",
    "1. Each labelled n-gram along with its feature set is passed to a DT model.\n",
    "\n",
    "\n",
    "2. Intensive(heavy) pruning of the created DT model is carried out.\n",
    "\n",
    "\n",
    "3. Overfitting is allowed (relaxed state).\n",
    "\n",
    "...\n",
    "\n",
    "`Step 5: Pass the predicted values to a SME`\n",
    "\n",
    "1. For new data, n-grams are generated. For each new n-gram feature set of each n-gram is passed to a fitted pruned DT model and end-labels(B-tag, I-tag, .., O-tag) are generated. These values are converted back to intial labels(S,M,E and O). These labels are then passed to a final SME. \n",
    "\n",
    "\n",
    "2. Final set of n-grams for new data looks like:\n",
    "\n",
    "        - Ngram1 = S\n",
    "        - Ngram2 = M\n",
    "        - Ngram3 = O\n",
    "        - Ngram4 = M\n",
    "        - Ngram5 = O\n",
    "        - Ngram6 = E\n",
    "        ...\n",
    "        - NgramN = S\n",
    "        \n",
    "\n",
    "3. A *potential AB* is created from above labelled n-grams as when a 'S' is encountered - which marks beginning of a *potential AB* and all the following labels are considered/stored until a 'E' is encountered - which marks the end of this *potenital AB*.\n",
    "\n",
    "\n",
    "\n",
    "4. Likewise all *potenital AB* are created. AB may contian a minimum of only 3 n-grams(S, M, E) or may contain multiple n-grams such as (S, O, O, O, O, E) or (S, M, O, O, E). The newly created AB bunches look like:\n",
    "\n",
    "        - Ngram1  = S  :>\n",
    "        - Ngram2  = M  :>  Potential AB 1  (Pattern = SME)\n",
    "        - Ngram3  = E  :> \n",
    "        - Ngram4  = O\n",
    "        - Ngram5  = O\n",
    "        - Ngram6  = M\n",
    "        - Ngram7  = M\n",
    "        - Ngrma8  = O\n",
    "        - Ngram9  = S  :>\n",
    "        - Ngram10 = O  :>\n",
    "        - Ngram11 = M  :>  Potential AB 2  (Pattern = SOMOE)\n",
    "        - Ngram12 = O  :>\n",
    "        - Ngram13 = E  :>\n",
    "        - Ngram14 = O\n",
    "        - Ngram15 = S  :>\n",
    "        - Ngram16 = O  :>  Potential AB 3  (Pattern = SOE)\n",
    "        - Ngram17 = E  :>\n",
    "\n",
    "\n",
    "5. All patterns which have atleast 1 \"M\" label are subjected to a pattern-check following below rules:\n",
    "        \n",
    "        - \"S\" is followed by either \"O\" or \"M\"  AND  not followed by a \"E\"\n",
    "        - \"M\" is followed by either \"O\" or \"E\"  AND  not followed by a \"S\"\n",
    "        - \"E\" is followed by a \"O\"              AND  not followed by either \"M\" or \"S\"\n",
    "        ... likewise\n",
    "        \n",
    "        \n",
    "6. All potenital AB blocks which follow a correct pattern are considered as ***final Address Block (AB)***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.213653,
     "end_time": "2021-02-04T08:30:44.215922",
     "exception": false,
     "start_time": "2021-02-04T08:30:44.002269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "....................................................................................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.212581,
     "end_time": "2021-02-04T08:30:44.640895",
     "exception": false,
     "start_time": "2021-02-04T08:30:44.428314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Approach 3\n",
    "Using lexNLP, libpostal and a scoring decision matrix\n",
    "\n",
    "- Taking all lines in an XML file and passing it to 3rd party library. Using combination of rules and a scoring decision matrix all AddressBlocks(AB) are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.210755,
     "end_time": "2021-02-04T08:30:45.065088",
     "exception": false,
     "start_time": "2021-02-04T08:30:44.854333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<ins>STEPS:</ins>\n",
    "\n",
    "\n",
    "1. INPUT = All `LINES` in the 1st page of the XML file are considered and stored in a separate dataframe i.e. df_complete. (As opposed to another dataframe which reads only non-table blocks from the 1st page of XML file and was used for SN extraction process).\n",
    "\n",
    "\n",
    "2. All `LINES` are subjected to a *libpostal* and `POSTAL-TAG` are found.\n",
    "\n",
    "\n",
    "3. Using some *Rules*, POSTAL-TAG are converted to `LABEL-TAG`.\n",
    "\n",
    "\n",
    "4. Using some *Rules*, LABEL-TAG are converted to `LABEL`.\n",
    "\n",
    "\n",
    "5. Using some *Rules*, a `potenital-AB` is discovered.\n",
    "\n",
    "\n",
    "6. For all potential-AB, LABEL are collected together to create a Pattern.\n",
    "\n",
    "\n",
    "7. This pattern is passed to a pattern-checker module to idnetify all actual `AB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:45.485170Z",
     "iopub.status.busy": "2021-02-04T08:30:45.484424Z",
     "iopub.status.idle": "2021-02-04T08:30:45.490412Z",
     "shell.execute_reply": "2021-02-04T08:30:45.490973Z"
    },
    "papermill": {
     "duration": 0.21727,
     "end_time": "2021-02-04T08:30:45.491161",
     "exception": false,
     "start_time": "2021-02-04T08:30:45.273891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Rules for AB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "----------------------------------------\n",
    "ADDRESS BLOCK LABELLING RULES\n",
    "----------------------------------------\n",
    "Start = HAS a NUMBER (DIGIT), AND following one word from (street number, house number, road number, block number)\n",
    "\n",
    "MIDDLE = between fields like highway, floor, unit, level, building, compound, etc\n",
    "\n",
    "END = HAS a ZIPCODE AND preecidng one pair from (city AND state | city AND country| state AND country)\n",
    "\n",
    "OTHERS = house\n",
    "\"\"\"\n",
    "\n",
    "print(\"...Rules for AB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.216804,
     "end_time": "2021-02-04T08:30:45.920559",
     "exception": false,
     "start_time": "2021-02-04T08:30:45.703755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Labelling Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211582,
     "end_time": "2021-02-04T08:30:46.379708",
     "exception": false,
     "start_time": "2021-02-04T08:30:46.168126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **POSTAL-TAG**\n",
    "\n",
    "\n",
    "    - generated using libpostal\n",
    "    - Examples: house_number, unit, level, po_box, suburb, city_district, city, island, state_district, state, \n",
    "                country_region, country,  world_region, postcode\n",
    ". \n",
    "- **LABEL-TAG**\n",
    "\n",
    "\n",
    "    - generated using rules\n",
    "    - Examples:\n",
    "      S_LABELS: Marks the beginning of an AB\n",
    "      M_LABELS: Marks the middle of an AB\n",
    "      E_LABELS: Marks the ending of an AB\n",
    ".\n",
    "\n",
    "- **LABEL**\n",
    "\n",
    "\n",
    "    - generated using rules\n",
    "    - Examples:\n",
    "      S house_number: usually refers to the external (street-facing) building number.\n",
    "      S unit: an apartment, unit, office, lot, or other secondary unit designator\n",
    "      S level: expressions indicating a floor number e.g. \"3rd Floor\", \"Ground Floor\", etc.\n",
    "      M po_box: post office box number\n",
    "      M suburb: an unofficial neighborhood name like \"Harlem\", \"South Bronx\", or \"Crown Heights\"\n",
    "      M city_district: districts within a city e.g. \"Brooklyn\" or \"Hackney\" or \"Bratislava IV\"\n",
    "      M city: settlement including cities, towns, villages, hamlets, localities, etc.\n",
    "      M island: named islands e.g. \"Maui\"\n",
    "      M state_district: usually a second-level administrative division or county.\n",
    "      M state: a first-level administrative division.\n",
    "      M country_region: informal subdivision of a country without any political status\n",
    "      M country: sovereign nations and their dependent territories, anything with an ISO-3166 code.\n",
    "      M world_region: a pattern frequently used in the English-speaking Caribbean e.g. âJamaica, West Indiesâ\n",
    "      E postcode: postal codes used for mail sorting    \n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:46.809512Z",
     "iopub.status.busy": "2021-02-04T08:30:46.808809Z",
     "iopub.status.idle": "2021-02-04T08:30:46.811095Z",
     "shell.execute_reply": "2021-02-04T08:30:46.811580Z"
    },
    "papermill": {
     "duration": 0.220482,
     "end_time": "2021-02-04T08:30:46.811778",
     "exception": false,
     "start_time": "2021-02-04T08:30:46.591296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Module 1: Extracts POSTAL-TAG for each line in a file.\n",
    "##########################################################################\n",
    "# INPUT  ::  df.LINES\n",
    "# OUTPUT ::  returns a list of \"tagged_lines\" and \"tags\"\n",
    "\n",
    "def get_postal_lines(df_lines):\n",
    "    list_tagged_lines, list_tags = [], []\n",
    "    for line in df_lines.values:\n",
    "        tagged_line = crf_NER(line)\n",
    "        tags = list(map(lambda x: x[1], tagged_line))\n",
    "        list_tagged_lines.append(tagged_line)\n",
    "        list_tags.append(tags)\n",
    "    return list_tagged_lines, list_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:47.248085Z",
     "iopub.status.busy": "2021-02-04T08:30:47.247337Z",
     "iopub.status.idle": "2021-02-04T08:30:47.269367Z",
     "shell.execute_reply": "2021-02-04T08:30:47.269949Z"
    },
    "papermill": {
     "duration": 0.243283,
     "end_time": "2021-02-04T08:30:47.270137",
     "exception": false,
     "start_time": "2021-02-04T08:30:47.026854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# Module 2: Decides which bunch of lines form an AddressBlock. Creates a final-tag \"AB\" or \"O\" for a bunch of lines\n",
    "#########################################################################################################################\n",
    "# INPUT  ::  Accepts a column having POSTAL-TAG(house, road,..,city) for all lines in file\n",
    "# OUTPUT ::  Returns O or AB predicitions for each line in file\n",
    "\n",
    "def get_postal_labels(df_TAGS):\n",
    "    \n",
    "    # DEFINE LABELS\n",
    "    S_LABELS = ['house_number', 'road', 'unit', 'level', 'po_box']\n",
    "    M_LABELS = ['suburb', 'city_district', 'city', 'island', 'state_district', 'state', 'country_region', 'country', 'world_region']\n",
    "    E_LABELS = ['postcode']\n",
    "    E1_LABELS = ['phonetag', 'urltag', 'emailtag']\n",
    "    \n",
    "    # 1. Replacing POSTAL-TAG with corresponding LABEL using the above MAPPER.\n",
    "    # Returns corresponding LABEL per line (e.g. L1 = S, L2 = M, L3 = E, etc)\n",
    "    def get_label_perLine(line):\n",
    "        label = []\n",
    "        for i in line:\n",
    "            if i in S_LABELS:\n",
    "                label.append('S')\n",
    "            elif i in M_LABELS:\n",
    "                label.append('M')\n",
    "            elif i in E_LABELS:\n",
    "                label.append('E')\n",
    "            elif i in E1_LABELS:\n",
    "                label.append('E1')\n",
    "            else:\n",
    "                label.append('O')\n",
    "        return label\n",
    "    df = pd.DataFrame({'TAGS': df_TAGS})\n",
    "    df['LABELS'] = df.TAGS.apply(get_label_perLine)\n",
    "    \n",
    " \n",
    "    # This module runs after each TAGGED_LINE has been mapped with corresponding \"S\", \"M\", \"E\" labels.\n",
    "    # Executes some rules to find address-boundaries!\n",
    "    # Pattern - S+O*M+O*E\n",
    "\n",
    "    \n",
    "    # 2. Finding LABEL(S and E) for bunch of tagged lines.\n",
    "    # Returns a 2D list containing S label(Row_num, Col_num) & E label(Row_num, Col_num)\n",
    "    found_index, store_found_index = [], []\n",
    "    finding_value = 'S'\n",
    "    for row_index, row in enumerate(df.LABELS.tolist()): \n",
    "        for column_index, col in enumerate(row):\n",
    "            # if a row contains only 'O' labels only OR consecutive two lines contain only 'O'  ---> break\n",
    "            if list(set(row)) == ['O'] or list(set(sum(df.LABELS.iloc[row_index:row_index+2].tolist(), []))) == ['O']:\n",
    "                finding_value = 'S'\n",
    "                found_index = []\n",
    "                break\n",
    "            if col == finding_value and finding_value == 'S':\n",
    "                found_index.append((row_index, column_index))\n",
    "                finding_value = 'E'\n",
    "                continue\n",
    "            if col == finding_value and len(found_index) == 0:\n",
    "                continue\n",
    "            if col == finding_value and len(found_index) != 0:\n",
    "                found_index.append((row_index, column_index))\n",
    "                finding_value = 'S'\n",
    "                store_found_index.append(found_index)\n",
    "                found_index = []\n",
    "    \n",
    "    # 3. Finding LABEL-PATTERN(\"S+O*M+O*E\") for bunch of tagged lines.\n",
    "    # Returns a series of LABEL-PATTERN per line (e.g. L1 = [], L2 = [SOOMOOE], L3 = [SMESS], etc)\n",
    "    list_final_labels = ['O']*df.LABELS.shape[0]\n",
    "    for index_tuple in store_found_index:\n",
    "        start_row, end_row = index_tuple[0][0], index_tuple[1][0]\n",
    "        start_col, end_col = index_tuple[0][1], index_tuple[1][1]\n",
    "        i = 0\n",
    "        values = []\n",
    "        label_list = df.LABELS.tolist()[start_row: end_row+1]\n",
    "        for lst in label_list:\n",
    "            if i == 0:\n",
    "                slicer_start, slicer_end = start_col, len(lst)\n",
    "            elif i == len(label_list)-1:\n",
    "                slicer_start, slicer_end = 0, end_col \n",
    "            else:\n",
    "                slicer_start, slicer_end = 0, len(lst)\n",
    "            values.append(lst[slicer_start: slicer_end+1])\n",
    "            i+=1\n",
    "        for i in range(start_row, end_row+1):\n",
    "            list_final_labels[i] = \"\".join(sum(values, []))\n",
    "    df['POSTAL_LABEL'] = list_final_labels\n",
    "    \n",
    "    # 4. Checks if the found LABEL-PATTERN qualifies to be \"AB\" or \"O\".\n",
    "    # Rreturns a series of final tag \"AB\" or \"O\" per line (e.g. L1 = O, L2 = AB, L3 = AB, etc)\n",
    "    def checkPattern(string, pattern): \n",
    "        if 'M' not in string:\n",
    "            return False\n",
    "        l = len(pattern) \n",
    "        if len(string) < l: \n",
    "            return False\n",
    "        for i in range(l - 1): \n",
    "            x = pattern[i] \n",
    "            y = pattern[i + 1] \n",
    "            last = string.rindex(x) \n",
    "            first = string.index(y) \n",
    "            if last == -1 or first == -1 or last > first: \n",
    "                return False\n",
    "        return True\n",
    "    # Check for the pattern, if M is not followed by 'S' likewise...\n",
    "    final_label = []\n",
    "    for tags in df.POSTAL_LABEL.values:\n",
    "        label = checkPattern(tags, pattern=\"SME\")\n",
    "        if label == True:\n",
    "            final_label.append('AB')\n",
    "        else:\n",
    "            final_label.append('O')\n",
    "    df['FINAL_LABEL'] = final_label\n",
    "    return df.LABELS, df.POSTAL_LABEL, df.FINAL_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:47.697851Z",
     "iopub.status.busy": "2021-02-04T08:30:47.697165Z",
     "iopub.status.idle": "2021-02-04T08:30:47.702788Z",
     "shell.execute_reply": "2021-02-04T08:30:47.703377Z"
    },
    "papermill": {
     "duration": 0.219897,
     "end_time": "2021-02-04T08:30:47.703559",
     "exception": false,
     "start_time": "2021-02-04T08:30:47.483662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "# Module 3: Accepts a df and returns various postal columns. Creates final tag \"AB\" or \"O\" for each line in a df\n",
    "#####################################################################################################################\n",
    "# INPUT  ::  Accepts a whole dataframe\n",
    "# OUTPUT ::  Returns 5 pd.series with postal information\n",
    "\n",
    "def get_postal(df):\n",
    "    \n",
    "    # Step 1: Create POSTAL-TAG for each line\n",
    "    df['POSTAL_taggedLines'], df['POSTAL_tags'] = get_postal_lines(df['LINES'])\n",
    "    \n",
    "    # Step 2: Create final tag(\"AB\" or \"O\") for each line\n",
    "    df['POSTAL_labels'], df['POSTAL_labels_AB'], df['POSTAL_AB'] = get_postal_labels(df['POSTAL_tags'])\n",
    "    \n",
    "    return df.POSTAL_taggedLines, df.POSTAL_tags, df.POSTAL_labels, df.POSTAL_labels_AB, df.POSTAL_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:48.129893Z",
     "iopub.status.busy": "2021-02-04T08:30:48.129034Z",
     "iopub.status.idle": "2021-02-04T08:30:48.140344Z",
     "shell.execute_reply": "2021-02-04T08:30:48.140892Z"
    },
    "papermill": {
     "duration": 0.225808,
     "end_time": "2021-02-04T08:30:48.141074",
     "exception": false,
     "start_time": "2021-02-04T08:30:47.915266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# Module 4: Runs above 3 modules and combines them to create final \"AB CHUNK\" for each file\n",
    "##################################################################################################################\n",
    "# INPUT  ::  Accepts the whole dataframe(viz. \"without\" postal columns)\n",
    "# OUTPUT ::  List of AB chunks for each file\n",
    "\n",
    "def get_AB_file(df):\n",
    "    \n",
    "    AB_file = {}\n",
    "    for index, f in enumerate(df.FILENAME.unique()):\n",
    "        \n",
    "        # Create df\n",
    "        df_AB = df[df.FILENAME == f].copy()\n",
    "        print(\"Extracting AB for File: #{} - {}\".format(index, f))\n",
    "        \n",
    "        # Create feature set\n",
    "        df_AB['POSTAL_taggedLines'], df_AB['POSTAL_tags'], df_AB['POSTAL_labels'], \\\n",
    "        df_AB['POSTAL_labels_AB'], df_AB['POSTAL_AB'] = get_postal(df_AB)\n",
    "        \n",
    "        # Extract AB chunks\n",
    "        AB_blocks, AB_lines = [], []\n",
    "        start, end = 0, 0\n",
    "        for line, postal in zip(df_AB.LINES, df_AB.POSTAL_AB):\n",
    "            if postal == 'AB':\n",
    "                start = 1\n",
    "                AB_lines.append(line)\n",
    "            if postal != 'AB':\n",
    "                end = 1\n",
    "                if start == 1 and end == 1 and len(AB_lines) > 0:\n",
    "                    AB_blocks.append(\" \".join(AB_lines))\n",
    "                    AB_lines, start, end = [], 0, 0\n",
    "        \n",
    "        # Store extracted AB Chunks\n",
    "        AB_file[str(f)] = list(map(lambda x: [x], AB_blocks))\n",
    "        \n",
    "    # Storing AB Chunks in a df\n",
    "    k,v = zip(*AB_file.items())\n",
    "    df_AB_chunks = pd.DataFrame({'FILENAME': k, 'AddressBlocks': v})\n",
    "    df_AB_chunks = df_AB_chunks[df_AB_chunks.AddressBlocks.apply(lambda x: len(x)) > 0].reset_index(drop=True)\n",
    "    return df_AB_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211256,
     "end_time": "2021-02-04T08:30:48.567283",
     "exception": false,
     "start_time": "2021-02-04T08:30:48.356027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AddressBlock Extraction Module\n",
    "\n",
    "\n",
    "`df`          - Shall be used for model prediciton (viz. contains only non-table blocks on 1st page)\n",
    "\n",
    "`df_complete` - Shall be used for AB extraction (viz. contains all blocks table + non-table on 1st page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.213926,
     "end_time": "2021-02-04T08:30:48.995639",
     "exception": false,
     "start_time": "2021-02-04T08:30:48.781713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. AB Extraction for `df` \n",
    "\n",
    "(i.e. for creating feature set used later in *SupplierName Extraction* module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for whole df\n",
    "df['POSTAL_taggedLines'], df['POSTAL_tags'], df['POSTAL_labels'], df['POSTAL_labels_AB'], df['POSTAL_AB'] = get_postal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['LINES', 'POSTAL_tags', 'POSTAL_labels', 'POSTAL_labels_AB', 'POSTAL_AB']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.215093,
     "end_time": "2021-02-04T08:30:50.579657",
     "exception": false,
     "start_time": "2021-02-04T08:30:50.364564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. AB Extraction for `df_complete` \n",
    "\n",
    "(i.e. creating feature set for each file and then extracting final AB CHUNKS for output report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Address Block Chunks for each file\n",
    "df_AB_chunks = get_AB_file(df_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:52.180441Z",
     "iopub.status.busy": "2021-02-04T08:30:52.179502Z",
     "iopub.status.idle": "2021-02-04T08:30:52.183345Z",
     "shell.execute_reply": "2021-02-04T08:30:52.182713Z"
    },
    "papermill": {
     "duration": 0.222274,
     "end_time": "2021-02-04T08:30:52.183492",
     "exception": false,
     "start_time": "2021-02-04T08:30:51.961218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the df in a report...\n",
    "# df_AB_chunks.to_excel('AmeriHealth_AB_8000Files.xlsx')\n",
    "# df_AB_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.211969,
     "end_time": "2021-02-04T08:30:52.607433",
     "exception": false,
     "start_time": "2021-02-04T08:30:52.395464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.215884,
     "end_time": "2021-02-04T08:30:53.036733",
     "exception": false,
     "start_time": "2021-02-04T08:30:52.820849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>9. Labelling Data Module </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.213791,
     "end_time": "2021-02-04T08:30:53.464938",
     "exception": false,
     "start_time": "2021-02-04T08:30:53.251147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- A line is labelled as 1 if the line contains a Supplier Name (as found in Mapper File) else a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:53.911311Z",
     "iopub.status.busy": "2021-02-04T08:30:53.900679Z",
     "iopub.status.idle": "2021-02-04T08:30:53.915969Z",
     "shell.execute_reply": "2021-02-04T08:30:53.915155Z"
    },
    "papermill": {
     "duration": 0.235061,
     "end_time": "2021-02-04T08:30:53.916128",
     "exception": false,
     "start_time": "2021-02-04T08:30:53.681067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "s, ts = 0, 0\n",
    "orgsn = []\n",
    "for sn in list(df.SUPPLIER_NAME.unique()):\n",
    "    # English Dictionary words\n",
    "    ts+=len(word_tokenize(sn))\n",
    "    s+=len([w for w in word_tokenize(sn) if lemma.lemmatize(w.lower()).strip() in ENG_words])\n",
    "    \n",
    "    # NER Org tags\n",
    "    x = 0\n",
    "    if [1 if ent.label_ == 'ORG' else 0 for ent in nlp(sn).ents] != []: x=1\n",
    "    orgsn.append(x)\n",
    "    \n",
    "print(\"% of English Dictionary words in Supplier Names = \", s*100.0/ts)\n",
    "print(\"% of Spacy NER TAG \\'ORG\\' in Supplier Names = \", sum(orgsn)*100.0/len(list(df.SUPPLIER_NAME.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:54.358506Z",
     "iopub.status.busy": "2021-02-04T08:30:54.348724Z",
     "iopub.status.idle": "2021-02-04T08:30:54.361625Z",
     "shell.execute_reply": "2021-02-04T08:30:54.360981Z"
    },
    "papermill": {
     "duration": 0.233685,
     "end_time": "2021-02-04T08:30:54.361784",
     "exception": false,
     "start_time": "2021-02-04T08:30:54.128099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Labels df w.r.t computed similarity (b/w line & SN) from mapper file\n",
    "##########################################################################\n",
    "\n",
    "def label_data(df):\n",
    "    \n",
    "    def clean(sn, line):\n",
    "        \"\"\"\n",
    "        Regex cleaning of SN and line.\n",
    "        :param: string sn: Supplier Name; string line: A text line\n",
    "        :return: Returns cleaned SN, cleaned line.\n",
    "        \"\"\"\n",
    "        sn = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9!!@#$%&?]+\", \" \", sn, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE)\n",
    "        line = re.sub(r\"[\\n\\t\\r]+\", \" \", line, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        line = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9!!@#$%&?]+\", \" \", line, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE)\n",
    "        line = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\`\\~\\^\\*\\(\\)\\[\\]\\{\\}\\:\\;\\'\\\"\\,\\<\\.\\>\\/\\\\\\|\\-\\_\\=\\+]+\", \" \", line, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE)\n",
    "        return sn.lower(), line.lower()\n",
    "    \n",
    "    def score(sn, line):\n",
    "        \"\"\"\n",
    "        Labels a line based on (1)Regex match (2)Exact regex match (3)FuzzyWuzzy Distance > 95 between (SN, line).\n",
    "        :param: string sn: Supplier Name; string line: A text line\n",
    "        :return: Returns 1 or 0\n",
    "        \"\"\"\n",
    "        # 1. Regex match\n",
    "        match_index = [(a.start(), a.end()) for a in list(re.finditer(sn, line))]\n",
    "        if len(match_index) > 0:\n",
    "            iter_score = 1\n",
    "        else:\n",
    "            iter_score = 0\n",
    "        \n",
    "        # 2. Exact regex match\n",
    "        finds = re.findall(r\"(?=(\\b\" + '\\\\b|\\\\b'.join([sn]) + r\"\\b))\", line, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        if len(finds) > 0:\n",
    "            find_score = 1\n",
    "        else:\n",
    "            find_score = 0\n",
    "        \n",
    "        # 3. FuzzyWuzzy distance\n",
    "        excat_score, partial_score = fuzz.ratio(sn, line), fuzz.partial_ratio(sn, line)\n",
    "        if excat_score >=95:\n",
    "            fuzzy_score = 1\n",
    "        else:\n",
    "            fuzzy_score = 0\n",
    "        \n",
    "        # Returns \"1\" if any of the above is 1 else a \"0\"\n",
    "        return max(iter_score, find_score, fuzzy_score)\n",
    "\n",
    "    def exact_score(sn, line):\n",
    "        \"\"\"\n",
    "        Finds 'exact' label for a line.\n",
    "        :param: string sn: Supplier Name; string line: A text line\n",
    "        :return: Returns 1 or 0\n",
    "        \"\"\"\n",
    "        exact_sn = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z\\s\\&]+\", \" \", sn.strip(), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).strip().lower()\n",
    "        exact_line = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z\\s\\&]+\", \" \", line.strip(), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).strip().lower()\n",
    "        if exact_line == exact_sn:\n",
    "            exact_Label = 1\n",
    "        else:\n",
    "            exact_Label = 0\n",
    "        return exact_Label\n",
    "    \n",
    "    \n",
    "    # LABELLING\n",
    "    def regex_match(x):\n",
    "        # SN, Line\n",
    "        sn = x.SUPPLIER_NAME\n",
    "        line = x.LINES\n",
    "        # 1. Regex cleaning\n",
    "        clean_sn, clean_line = clean(sn, line)\n",
    "        # 2. Normal labelling\n",
    "        label = score(clean_sn, clean_line)\n",
    "        # 3. Exact labelling\n",
    "        exact_Label = exact_score(sn, line)\n",
    "        return pd.Series([exact_Label, label])\n",
    "    \n",
    "    # Using only 'Y_SN' value in training...\n",
    "    df[['Y_SN_EXACT', 'Y_SN']] = df.apply(regex_match, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:54.912117Z",
     "iopub.status.busy": "2021-02-04T08:30:54.911405Z",
     "iopub.status.idle": "2021-02-04T08:30:55.070401Z",
     "shell.execute_reply": "2021-02-04T08:30:55.069838Z"
    },
    "papermill": {
     "duration": 0.494023,
     "end_time": "2021-02-04T08:30:55.070560",
     "exception": false,
     "start_time": "2021-02-04T08:30:54.576537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-120>\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                                \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                                \u001b[0mmax_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                                include_children=include_children)\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mmem_usage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36m_func_exec\u001b[0;34m(stmt, ns)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# helper for magic_memit, just a function proxy for the exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;31m# statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "df = label_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:55.558561Z",
     "iopub.status.busy": "2021-02-04T08:30:55.547048Z",
     "iopub.status.idle": "2021-02-04T08:30:55.563743Z",
     "shell.execute_reply": "2021-02-04T08:30:55.564428Z"
    },
    "papermill": {
     "duration": 0.236488,
     "end_time": "2021-02-04T08:30:55.564603",
     "exception": false,
     "start_time": "2021-02-04T08:30:55.328115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution ::\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fa9fdf4eceea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLabel Distribution ::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nLabel Distribution ::\")\n",
    "df.Y_SN.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.217405,
     "end_time": "2021-02-04T08:30:55.995971",
     "exception": false,
     "start_time": "2021-02-04T08:30:55.778566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.215136,
     "end_time": "2021-02-04T08:30:56.427314",
     "exception": false,
     "start_time": "2021-02-04T08:30:56.212178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>10. Feature Engineering Module </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.213621,
     "end_time": "2021-02-04T08:30:56.860052",
     "exception": false,
     "start_time": "2021-02-04T08:30:56.646431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Linguistic Features = Features exploited based on meta-information about text such as Font-Size, coordinates, presence of dictionary words in a line, context neighbouring to a line, etc.\n",
    "\n",
    "\n",
    "- Text Features = Text converted into numbers used as a feature. Using a count vectorizer to convert text into count of tokens per line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.214353,
     "end_time": "2021-02-04T08:30:57.331622",
     "exception": false,
     "start_time": "2021-02-04T08:30:57.117269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "....................................................................................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.216031,
     "end_time": "2021-02-04T08:30:57.765982",
     "exception": false,
     "start_time": "2021-02-04T08:30:57.549951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `TYPE I. Linguistic Features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.214688,
     "end_time": "2021-02-04T08:30:58.196320",
     "exception": false,
     "start_time": "2021-02-04T08:30:57.981632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Features List ::**\n",
    "        \n",
    "1. XML feature: Find | Coordinate | Font-size | of a line.\n",
    "\n",
    "2. Line-Level feature: Contains Digit | All Digit | Punct | Email | URL | Date | Phone in a line.\n",
    "\n",
    "3. Contextual feature: Presence of a AB in neghbouring lines.\n",
    "\n",
    "4. Position feature: Quadrant of a line in which it is found.\n",
    "\n",
    "5. Geographical Parser feature: Presence of NER(GPE) tags in neghbouring lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.21333,
     "end_time": "2021-02-04T08:30:58.623980",
     "exception": false,
     "start_time": "2021-02-04T08:30:58.410650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### `1. XML Feature`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": 0.216411,
     "end_time": "2021-02-04T08:30:59.056273",
     "exception": false,
     "start_time": "2021-02-04T08:30:58.839862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Normal COORDINATES      :: Left(l) Top(t) Right(r) Bottom(b) FontSize(FS)\n",
    "\n",
    "Normal Font-Size        :: fs\n",
    "\n",
    "Regularized COORDINATES :: l_ t_ r_ b_ FS_\n",
    "\n",
    "Regularized Font-Size   :: fs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.214581,
     "end_time": "2021-02-04T08:30:59.487365",
     "exception": false,
     "start_time": "2021-02-04T08:30:59.272784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### `2. Line-Level Feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:30:59.932724Z",
     "iopub.status.busy": "2021-02-04T08:30:59.931871Z",
     "iopub.status.idle": "2021-02-04T08:30:59.935789Z",
     "shell.execute_reply": "2021-02-04T08:30:59.935240Z"
    },
    "papermill": {
     "duration": 0.231942,
     "end_time": "2021-02-04T08:30:59.935975",
     "exception": false,
     "start_time": "2021-02-04T08:30:59.704033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# BOOL FEATURE :: Finds various line-level features in each line\n",
    "##########################################################################\n",
    "\n",
    "def f1_lineLevel(line):\n",
    "    \"\"\"\n",
    "    Finds line-level features for each line.\n",
    "    :param: df.LINES\n",
    "    :return: Returns various features at line-level with BOOl values (1 or 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def is_CONTAINSDIGIT(w):\n",
    "        if len(re.findall(r\"\\d+\", w)) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def is_CONTAINSALLDIGITS(w):\n",
    "        if w.isdigit() or re.sub(\"\\s+\", \"\", re.sub(\"[\"+REGEX_PUNCTS+\"]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", w))).strip().isdigit():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def is_CONTAINSPUNCT(w):\n",
    "        if len(re.findall(\"[\"+REGEX_PUNCTS+\"]+\", w)) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def is_CONTAINSEMAIL(w):\n",
    "        email_tags, _ = find_email(w)\n",
    "        if len(email_tags) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def is_CONTAINSURL(w):\n",
    "        url_tags, _, _ = find_url(w)\n",
    "        if len(url_tags) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def is_CONTAINSMONEY(w):\n",
    "        money_tags = find_money(w)\n",
    "        if len(money_tags) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def is_CONTAINSDATE(w):\n",
    "        date_tags = find_date(w)\n",
    "        if len(date_tags) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def is_CONTAINSPHONE(w):\n",
    "        phone_tags = find_phone(w)\n",
    "        if len(phone_tags) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # LINE-LEVEL FEATURES\n",
    "    line_CONTAINSDIGIT = is_CONTAINSDIGIT(line)\n",
    "    line_CONTAINSALLDIGITS = is_CONTAINSALLDIGITS(line)\n",
    "    line_CONTAINSEMAIL = is_CONTAINSEMAIL(line)\n",
    "    line_CONTAINSURL = is_CONTAINSURL(line)\n",
    "    # OLD: line_CONTAINSMONEY = is_CONTAINSMONEY(line)\n",
    "    line_CONTAINSDATE = is_CONTAINSDATE(line)\n",
    "    line_CONTAINSPHONE = is_CONTAINSPHONE(line)\n",
    "    return line_CONTAINSDIGIT, line_CONTAINSALLDIGITS, line_CONTAINSEMAIL, line_CONTAINSURL, line_CONTAINSDATE, \\\n",
    "            line_CONTAINSPHONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:00.367781Z",
     "iopub.status.busy": "2021-02-04T08:31:00.366972Z",
     "iopub.status.idle": "2021-02-04T08:31:00.370171Z",
     "shell.execute_reply": "2021-02-04T08:31:00.369539Z"
    },
    "papermill": {
     "duration": 0.221232,
     "end_time": "2021-02-04T08:31:00.370314",
     "exception": false,
     "start_time": "2021-02-04T08:31:00.149082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# BOOL FEATURE :: Finds if the line contains a legal/dynamic extension ABBRV\n",
    "##########################################################################\n",
    "\n",
    "def f3_checkAbbrv(s):\n",
    "    \"\"\"\n",
    "    Finds if a line contains a Legal or Dynamic abbreviation\n",
    "    :param: df.LINES\n",
    "    :return: Returns 1 if it contains else a 0\n",
    "    \"\"\"\n",
    "    finds = re.findall(r\"(?=(\\b\" + '\\\\b|\\\\b'.join(list_abbrv_regex) + r\"\\b))\", s.lower(), flags=re.IGNORECASE | re.MULTILINE)\n",
    "    if len(finds) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:00.809762Z",
     "iopub.status.busy": "2021-02-04T08:31:00.809064Z",
     "iopub.status.idle": "2021-02-04T08:31:00.813944Z",
     "shell.execute_reply": "2021-02-04T08:31:00.813269Z"
    },
    "papermill": {
     "duration": 0.228707,
     "end_time": "2021-02-04T08:31:00.814118",
     "exception": false,
     "start_time": "2021-02-04T08:31:00.585411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# PERCENTAGE FEATURE :: Finds all email address present in line\n",
    "##########################################################################\n",
    "\n",
    "def f1_checkEmail(s):\n",
    "    \"\"\"\n",
    "    Finds % of email in a line\n",
    "    :param: df.LINES\n",
    "    :return: Returns percentage value number of emails out of total words\n",
    "    \"\"\"\n",
    "    if len(s) < LENGTH_OF_EMAIL or re.sub(\"\\s+\", \"\", re.sub(r\"[^A-z0-9]\", \" \", s, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).isdigit():\n",
    "        emails = []\n",
    "    else:\n",
    "        s = s.strip()\n",
    "        s = re.sub(r\"\\s+\", \" \", re.sub(\"[^A-z\\&\\@\\.\\\\\\/\\:\\;]\", \" \", re.sub(r\"\\s+[\\~\\`\\!\\#\\$\\^\\*\\(\\)\\-\\_\\+\\=\\[\\]\\{\\}\\:\\;\\'\\\"\\,\\<\\.\\>\\?\\/\\\\\\|]+\\s+\", \" \", re.sub(r\"[\\n\\r\\t]+\", \" \", s, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "        emails = re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        emails = [eml for eml in emails if len(eml) >= LENGTH_OF_EMAIL]\n",
    "    total_words = s.split(\" \")\n",
    "    per_emails_line = len(emails)*100.0/len(total_words)\n",
    "    return per_emails_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:01.259548Z",
     "iopub.status.busy": "2021-02-04T08:31:01.258700Z",
     "iopub.status.idle": "2021-02-04T08:31:01.261493Z",
     "shell.execute_reply": "2021-02-04T08:31:01.261994Z"
    },
    "papermill": {
     "duration": 0.226842,
     "end_time": "2021-02-04T08:31:01.262184",
     "exception": false,
     "start_time": "2021-02-04T08:31:01.035342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# PERCENTAGE FEATURE :: Finds all URLs/Websites present in line\n",
    "##########################################################################\n",
    "\n",
    "def f1_checkURL(s):\n",
    "    \"\"\"\n",
    "    Finds % of url in a line\n",
    "    :param: df.LINES\n",
    "    :return: Returns percentage value number of urls out of total words\n",
    "    \"\"\"\n",
    "    if len(s) < LENGTH_OF_URL or re.sub(\"\\s+\", \"\", re.sub(r\"[^A-z0-9]\", \" \", s, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).isdigit():\n",
    "        URL = []\n",
    "    else:\n",
    "        s = s.strip()\n",
    "        s = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \" \", s, flags=re.IGNORECASE | re.MULTILINE).strip(), flags=re.IGNORECASE | re.MULTILINE)\n",
    "        s = re.sub(r\"\\d+\\.\\d+\", \" \", s.strip(), flags=re.IGNORECASE | re.MULTILINE)\n",
    "        URL = re.findall(r'(https://www.|http://www.|ftp://www.|http://|ftp://|https://|www.)+([A-z_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    total_words = s.split(\" \")\n",
    "    per_URL_line = len(URL)*100.0/len(total_words)\n",
    "    return per_URL_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:01.707362Z",
     "iopub.status.busy": "2021-02-04T08:31:01.706619Z",
     "iopub.status.idle": "2021-02-04T08:31:01.710589Z",
     "shell.execute_reply": "2021-02-04T08:31:01.709753Z"
    },
    "papermill": {
     "duration": 0.232266,
     "end_time": "2021-02-04T08:31:01.710747",
     "exception": false,
     "start_time": "2021-02-04T08:31:01.478481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# PERCENTAGE FEATURE :: checks % of DictW, C1DictW, CDictW, NDictW, C1NDictW, CNDictW, Digits\n",
    "##########################################################################\n",
    "\n",
    "def f4_checkDictWord(s):\n",
    "    \"\"\"\n",
    "    Finds % of origin words.\n",
    "    :param: df.LINES\n",
    "    :return: Returns percentage values of various dict-related or non-dict related words\n",
    "    \"\"\"\n",
    "    def percentage(no):\n",
    "        return no*100.0/len(words)\n",
    "    s = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\^\\,\\.\\?\\!\\;\\:\\'\\\"\\`\\-\\+\\_\\[\\]\\(\\)\\{\\}\\=\\*]+\", \" \", re.sub(r\"[^A-z0-9]+\", \" \", s, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE)\n",
    "    words = word_tokenize(s)\n",
    "    DictWords = [w for w in words if lemma.lemmatize(w).lower().strip() in ENG_words]\n",
    "    Cap1DictWords = [w for w in DictWords if w[0].isupper()]\n",
    "    CapDictWords = [w for w in DictWords if w.isupper()]\n",
    "    NonDictWords = [w for w in words if w not in set(DictWords)]\n",
    "    Cap1NonDictWords = [w for w in NonDictWords if w[0].isupper()]\n",
    "    CapNonDictWords = [w for w in NonDictWords if w.isupper()]\n",
    "    return percentage(len(DictWords)), percentage(len(Cap1DictWords)), percentage(len(CapDictWords)), percentage(len(NonDictWords)), percentage(len(Cap1NonDictWords)), percentage(len(CapNonDictWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.213718,
     "end_time": "2021-02-04T08:31:02.138042",
     "exception": false,
     "start_time": "2021-02-04T08:31:01.924324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### `3. Contextual Feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:02.597126Z",
     "iopub.status.busy": "2021-02-04T08:31:02.590568Z",
     "iopub.status.idle": "2021-02-04T08:31:02.600455Z",
     "shell.execute_reply": "2021-02-04T08:31:02.599782Z"
    },
    "papermill": {
     "duration": 0.248465,
     "end_time": "2021-02-04T08:31:02.600600",
     "exception": false,
     "start_time": "2021-02-04T08:31:02.352135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# PERCENTAGE & BOOL FEATURE :: Finds features related to neighbouring lines\n",
    "##########################################################################\n",
    "\n",
    "def f5_checkNeighbourAddress(df, enable_truecasing=True):\n",
    "    \"\"\"\n",
    "    Extracts various features using neighbouring lines(context) for a particular line.\n",
    "    :param: dataframe df\n",
    "    :return: \n",
    "    list list_is_spacy_NER_line: Bool value if line contains a NER(GPE) tag.\n",
    "    list list_per_spacy_NER_address: % value of NER(GPE) tag in neighbouring lines.\n",
    "    list list_per_crf_NER_address: % value of libpostal(Location related) tags in neighbouring lines.\n",
    "    list list_postal_ab_score: A %score if the line could be a Supplier Name line based on Scoring Decision Matrix. \n",
    "    \"\"\"\n",
    "    \n",
    "    def stanfordNLP_truecasing(text):\n",
    "        if len(text.strip()) > 0:\n",
    "            doc = stf_nlp(text)\n",
    "            text = \" \".join([w.text.capitalize() if w.upos in [\"PROPN\",\"NNS\"] else w.text for sent in doc.sentences for w in sent.words])\n",
    "            return text\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    LINE_CONTEXT_LOOKUP_1 = 7\n",
    "    LINE_CONTEXT_LOOKUP_2 = 3\n",
    "    \n",
    "    list_is_spacy_NER_line, list_per_spacy_NER_address, list_per_crf_NER_address, list_postal_ab_score = [], [], [], []\n",
    "    for line_num in range(len(df.LINES)):\n",
    "        \n",
    "        '''Line is the present line L'''\n",
    "        # LINE FEATURE\n",
    "        line = str(df.LINES.iloc[line_num:line_num+1].values[0])\n",
    "        original_line = line\n",
    "        line_tag = str(df.POSTAL_AB.iloc[line_num:line_num+1].values[0])\n",
    "        doc_line = nlp(line)\n",
    "        \n",
    "        # bool spacy NER \"ORG/FAC\" tag in line\n",
    "        spacy_NER_line_tag = ['ORG', 'FAC']\n",
    "        spacy_NER_line = set([ent.label_ for ent in doc_line.ents if ent.label_ in spacy_NER_line_tag])\n",
    "        is_spacy_NER_line = len(spacy_NER_line)\n",
    "        list_is_spacy_NER_line.append(is_spacy_NER_line)\n",
    "        \n",
    "        '''Address is generally found below SN, context could be of varying size'''\n",
    "        # CONTEXT FEATURE: \n",
    "        \n",
    "        # CONTEXT #1 - context is 6 lines below L \n",
    "        address_1 = \" \".join(df.LINES.iloc[line_num+1: line_num+LINE_CONTEXT_LOOKUP_1].tolist())\n",
    "        original_address_1 = \" .  \".join(df.LINES.iloc[line_num+1: line_num+LINE_CONTEXT_LOOKUP_1].tolist())\n",
    "        address_1 = re.sub(\"\\s+\", \" \", re.sub(r\"\\^+\", \" \", re.sub(r\"[^A-z\\!\\@\\&\\(\\)\\,\\.\\?]+\", \" \", address_1.strip(), flags=re.IGNORECASE | re.MULTILINE))).strip()\n",
    "        address_1 = re.sub(r\"po.box|po.box.|po. box|po. box.|p.o.box|p.o.box.|p.o box|p.o box.|p.o. box|p.o. box.\", \"Chicago\", address_1.strip(), flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "        \n",
    "        if enable_truecasing == True:\n",
    "            # No preprocessing only truecasing needed!\n",
    "            address_1 = stanfordNLP_truecasing(address_1)\n",
    "        doc_address = nlp(address_1)\n",
    "            \n",
    "        # Total words\n",
    "        if len(doc_address) > 0:\n",
    "            # if on normal line\n",
    "            total_words = len(doc_address)\n",
    "        else:\n",
    "            # if on last line\n",
    "            total_words = len(doc_line)\n",
    "        \n",
    "        # F1: % of spacy location tags in context\n",
    "        spacy_NER_address_tag =  ['FAC', 'GPE', 'LOC', 'ORDINAL']\n",
    "        spacy_NER_address = [ent.label_ for ent in doc_address.ents if ent.label_ in spacy_NER_address_tag]\n",
    "        per_spacy_NER_address = len(spacy_NER_address)*100.0/total_words\n",
    "        list_per_spacy_NER_address.append(per_spacy_NER_address)\n",
    "\n",
    "        # F2: % of CRF_NER location tags in context\n",
    "        crf_NER_address_tag = ['house_number', 'road', 'level', 'po_box', 'postcode', 'city_district', 'city', 'state_district', 'state', 'country_region', 'country']\n",
    "        crf_NER_address = [ent[1] for ent in crf_NER(doc_address) if ent[1] in crf_NER_address_tag]\n",
    "        per_crf_NER_address = len(crf_NER_address)*100.0/total_words\n",
    "        list_per_crf_NER_address.append(per_crf_NER_address)\n",
    "        \n",
    "        # CONTEXT #2 - context is 3 lines below L \n",
    "        address_2 = df.LINES.iloc[line_num+1: line_num+LINE_CONTEXT_LOOKUP_2]\n",
    "        address_tags = df.POSTAL_AB.iloc[line_num+1: line_num+LINE_CONTEXT_LOOKUP_2].tolist()\n",
    "                \n",
    "        def check_phoneInAddress():\n",
    "            bool_phone_address = 0\n",
    "            found = find_phone(original_address_1)\n",
    "            if len(found) > 0:\n",
    "                bool_phone_address = 1\n",
    "            return bool_phone_address\n",
    "            \n",
    "        def check_emailInAddress():\n",
    "            bool_domain_address = 0\n",
    "            _, found_email_domains = find_email(original_address_1)\n",
    "            _, _, found_url_domains = find_url(original_address_1)\n",
    "            domain = found_email_domains + found_url_domains\n",
    "            if len(domain) > 0:\n",
    "                # Fuzz match (Line L, domains)\n",
    "                domain_match_fuzz = any(np.where(np.array([fuzz.ratio(x, original_line.lower()) for x in domain]) >= 50, 1, 0))\n",
    "                domain_match_re = any([True for x in domain \n",
    "                                       if x == re.sub(\"\\s+\", \"\", re.sub(\"\\^\", \"\", re.sub(r\"[^A-z]+\", \" \", original_line, flags=re.IGNORECASE | re.MULTILINE))).lower() or \n",
    "                                       len(re.findall(x, re.sub(\"\\s+\", \"\", re.sub(r\"[^A-z]+\", \" \", original_line, flags=re.IGNORECASE | re.MULTILINE)).lower())) > 0])\n",
    "                print(original_line.lower(), domain, \"matched = \", domain_match_fuzz, domain_match_re)\n",
    "                if any([domain_match_fuzz + domain_match_re]):\n",
    "                    bool_domain_address = 1\n",
    "            return bool_domain_address\n",
    "             \n",
    "        # Scoring Mechanism for Address Blocks\n",
    "        '''\n",
    "                   1      2       3       4\n",
    "        L          O      AB     AB       O\n",
    "        context    O      O      AB       AB\n",
    "        Phone      -      -       -       -\n",
    "        Email      -      -       -       -\n",
    "        -----------------------------------------\n",
    "        Score  =   0     25%     25-50%    75-100%\n",
    "            \n",
    "        For Case 3: (1). 50   - if L is the 1st/2nd line in 'AB' and has 3 more 'AB' in context\n",
    "                    (2). 41.6 - if L is the 3rd/4th line in 'AB' and has 2 more 'AB' in context\n",
    "                    (2). 33.3 - if L is the 5th line in 'AB' and has 1 more 'AB' in context\n",
    "                    (3). 25   - if L is the last line in 'AB' and has no 'AB' left in context [is same as Case 2]\n",
    "                    \n",
    "        For Case 4: (1). 100  - if an email domain matches with L\n",
    "                    (2). 100  - if L is the line just above 'AB' and has a phone number in context.\n",
    "                    (3). 85   - if L is the line just above 'AB  \n",
    "                    (4). 75   - if L is two lines above 'AB'\n",
    "        '''\n",
    "    \n",
    "        score = 0\n",
    "        # 1. Score = 0%\n",
    "        if 'AB' not in address_tags and line_tag != 'AB':\n",
    "            score = 0\n",
    "        # 2. Score = 25%\n",
    "        elif 'AB' not in address_tags and line_tag == 'AB':\n",
    "            score = 25\n",
    "        # 3. Score = 25% - 50%\n",
    "        elif 'AB' in address_tags and line_tag == 'AB':\n",
    "            # cases 3.1, 3.2, 3.3, 3.4\n",
    "            score = 25 + 25*(int(address_tags.count('AB'))/3.0)\n",
    "        # 4. Score = 75% - 100%\n",
    "        elif 'AB' in address_tags and line_tag != 'AB':\n",
    "            value_phoneInAddress = check_phoneInAddress()\n",
    "            value_emailInAddress = check_emailInAddress()\n",
    "            if value_emailInAddress != 0:\n",
    "                # case 4.1\n",
    "                score = 100\n",
    "            elif address_tags[0] == 'AB':\n",
    "                # case 4.2\n",
    "                if value_phoneInAddress != 0:\n",
    "                    score = 100\n",
    "                else:\n",
    "                    # case 4.3\n",
    "                    score = 85\n",
    "            else:\n",
    "                # case 4.4\n",
    "                score = 75\n",
    "        # 5. Unknown\n",
    "        else:\n",
    "            score = 0\n",
    "        \n",
    "        # append score to global list\n",
    "        list_postal_ab_score.append(score)\n",
    "        \n",
    "    return list_is_spacy_NER_line, list_per_spacy_NER_address, list_per_crf_NER_address, list_postal_ab_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.216913,
     "end_time": "2021-02-04T08:31:03.036496",
     "exception": false,
     "start_time": "2021-02-04T08:31:02.819583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### `4. Position Feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:03.490145Z",
     "iopub.status.busy": "2021-02-04T08:31:03.489078Z",
     "iopub.status.idle": "2021-02-04T08:31:03.492892Z",
     "shell.execute_reply": "2021-02-04T08:31:03.492299Z"
    },
    "papermill": {
     "duration": 0.24043,
     "end_time": "2021-02-04T08:31:03.493052",
     "exception": false,
     "start_time": "2021-02-04T08:31:03.252622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# NUMERICAL FEATURE :: Finds quadrant (1 to 6) in which line is present\n",
    "##########################################################################\n",
    "\n",
    "def f6_checkLineQuadrant(x):\n",
    "    \"\"\"\n",
    "    Finds the quadrant for every line in df.\n",
    "    :param: Meta-data information (Coordinates, Page-Dimensions) for df.LINES\n",
    "    :return: Quadrant for each line in df.\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOT USED\n",
    "    def Overlap(l1,t1,r1,b1, l2,t2,r2,b2):\n",
    "        def Rectangel_overlapp(l1,t1,r1,b1, l2,t2,r2,b2): \n",
    "            # If one rectangle is on left side of other \n",
    "            if(l1 >= r2 or l2 >= r1): \n",
    "                return False\n",
    "            # If one rectangle is above other \n",
    "            if(t1 <= b2 or t2 <= b1): \n",
    "                return False\n",
    "            return True\n",
    "        if(Rectangel_overlapp(l1,t1,r1,b1, l2,t2,r2,b2)): \n",
    "            # Rectangles overlap\n",
    "            return 1\n",
    "        else: \n",
    "            # Rectangles Don't Overlap\n",
    "            return 0\n",
    "    \n",
    "    def Overlap_area(l1,t1,r1,b1, page_width, page_height):\n",
    "        # Calculates intersection area between two rectangles(LINE's bounding box & Quadrant Bounding box)\n",
    "        def area_overlap(l1,t1,r1,b1, l2,t2,r2,b2):\n",
    "            a = Rectangle(min(l1,r1), min(t1,b1), max(l1,r1), max(t1,b1))\n",
    "            b = Rectangle(min(l2,r2), min(t2,b2), max(l2,r2), max(t2,b2))\n",
    "            dx = min(a.xmax, b.xmax) - max(a.xmin, b.xmin)\n",
    "            dy = min(a.ymax, b.ymax) - max(a.ymin, b.ymin)\n",
    "            if (dx>=0) and (dy>=0):\n",
    "                return dx*dy\n",
    "            else: # returns 0 is no intersection (outside quad)\n",
    "                return 0\n",
    "        # Create a named tuple of 2 diagonal points of a rect\n",
    "        Rectangle = namedtuple('Rectangle', 'xmin ymin xmax ymax')\n",
    "        w, h = page_width, page_height\n",
    "        # Area of intersection(LINE Bounding Box, Quadrant Bounding Box)\n",
    "        # Q1\n",
    "        l2, t2, r2, b2 = 0, 0, w/2, h/3\n",
    "        areaL_Q1 = area_overlap(l1,t1,r1,b1, l2,t2,r2,b2)\n",
    "        # Q2\n",
    "        l2, t2, r2, b2 = w/2, 0, w, h/3\n",
    "        areaL_Q2 = area_overlap(l1,t1,r1,b1, l2,t2,r2,b2)\n",
    "        # Q3\n",
    "        l2, t2, r2, b2 = 0, h/3, w/2, 2*h/3\n",
    "        areaL_Q3 = area_overlap(l1,t1,r1,b1, l2,t2,r2,b2)\n",
    "        # Q4\n",
    "        l2, t2, r2, b2 = w/2, h/3, w, 2*h/3\n",
    "        areaL_Q4 = area_overlap(l1,t1,r1,b1, l2,t2,r2,b2)\n",
    "        # Q5\n",
    "        l2, t2, r2, b2 = 0, 2*h/3, w/2, h\n",
    "        areaL_Q5 = area_overlap(l1,t1,r1,b1, l2,t2,r2,b2)\n",
    "        # Q6\n",
    "        l2, t2, r2, b2 = w/2, 2*h/3, w, h\n",
    "        areaL_Q6 = area_overlap(l1,t1,r1,b1, l2,t2,r2,b2)\n",
    "        # Find out maximum area overlap = final position quadrant index(1,2,.., 6)\n",
    "        max_area = {'1': areaL_Q1, '2': areaL_Q2, '3': areaL_Q3, '4': areaL_Q4, '5': areaL_Q5, '6': areaL_Q6}\n",
    "        return max(max_area, key=max_area.get)\n",
    "    \n",
    "    l1,t1,r1,b1 = x['l'], x['t'], x['r'], x['b']\n",
    "    page_width, page_height = x['page_width'], x['page_height']\n",
    "    Quad = int(Overlap_area(l1,t1,r1,b1, page_width, page_height))\n",
    "    return Quad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.217942,
     "end_time": "2021-02-04T08:31:03.929565",
     "exception": false,
     "start_time": "2021-02-04T08:31:03.711623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### `5. Geographical Parser Feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:04.418731Z",
     "iopub.status.busy": "2021-02-04T08:31:04.417946Z",
     "iopub.status.idle": "2021-02-04T08:31:04.421712Z",
     "shell.execute_reply": "2021-02-04T08:31:04.421051Z"
    },
    "papermill": {
     "duration": 0.263487,
     "end_time": "2021-02-04T08:31:04.421870",
     "exception": false,
     "start_time": "2021-02-04T08:31:04.158383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# PERCENTAGE FEATURE :: Finds % of spacy + geopandas NER (GPE) in line\n",
    "##########################################################################\n",
    "\n",
    "def f7_checkGPE(s):\n",
    "    \"\"\"\n",
    "    Finds the % of Spacy NER(GPE) + GeoText Parser(GPE) tags in line.\n",
    "    :param: df.LINES\n",
    "    :return: % of Spacy NER(GPE) + GeoText Parser(GPE) tags in line\n",
    "    \"\"\"\n",
    "    # General\n",
    "    s = re.sub(\"\\s+\", \" \", re.sub(r\"\\\\\", \" \", re.sub(r\"\\^+\", \" \", re.sub(r\"[^A-z\\!\\@\\&\\(\\)\\,\\.\\?]+\", \" \", s.strip(), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).strip()   \n",
    "    # Replace PO BOX with a GPE Location (arbitary, \"Chicago\" is chosen as Spacy NER 100% identifies this city!)\n",
    "    s = re.sub(r\"po.box|po.box.|po. box|po. box.|p.o.box|p.o.box.|p.o box|p.o box.|p.o. box|p.o. box.\", \"Chicago\", s.strip(), flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "    doc = nlp(s)\n",
    "    total_words = len(doc)\n",
    "    if total_words > 0:\n",
    "        # Spacy NER Tagging\n",
    "        spacy_NER_GPE_tag = ['GPE']\n",
    "        NER_tags = [ent.orth_ for ent in doc.ents if ent.label_ in spacy_NER_GPE_tag]\n",
    "        NER_tags = list(map(lambda x: x.replace(\".\", \"\\.\").replace(\"(\", \"\\(\").replace(\")\", \"\\)\").replace(\"*\", \"\\*\").replace(\"+\", \"\\+\").replace(\"?\",\"\\?\").replace(\"^\",\"\\^\"), NER_tags))\n",
    "        s = re.sub(r\"\\s+\", \" \", re.sub(\"\\\\b\" + \"\\\\b|\\\\b\".join(NER_tags) + \"\\\\b\",  \" \", s, flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE) # Removing already found words from line\n",
    "        # Geotext NER Tagging\n",
    "        GEO_tags = GeoText(s).cities\n",
    "        per_GPE = len(NER_tags + GEO_tags)*100.0/total_words\n",
    "    else:\n",
    "        per_GPE = 0\n",
    "    return per_GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.218335,
     "end_time": "2021-02-04T08:31:04.859842",
     "exception": false,
     "start_time": "2021-02-04T08:31:04.641507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Final Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.215339,
     "end_time": "2021-02-04T08:31:05.293769",
     "exception": false,
     "start_time": "2021-02-04T08:31:05.078430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All features are calculated on `df.LINES` not on processed lines. Make sure features are calculated before any line preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:05.849419Z",
     "iopub.status.busy": "2021-02-04T08:31:05.848714Z",
     "iopub.status.idle": "2021-02-04T08:31:06.011487Z",
     "shell.execute_reply": "2021-02-04T08:31:06.010863Z"
    },
    "papermill": {
     "duration": 0.501274,
     "end_time": "2021-02-04T08:31:06.011634",
     "exception": false,
     "start_time": "2021-02-04T08:31:05.510360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-120>\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                                \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                                \u001b[0mmax_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                                include_children=include_children)\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mmem_usage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36m_func_exec\u001b[0;34m(stmt, ns)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# helper for magic_memit, just a function proxy for the exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;31m# statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# 0. XML features\n",
    "df['l'], df['t'], df['r'], df['b'], df['FS'], \\\n",
    "df['l_'], df['t_'], df['r_'], df['b_'], df['FS_'], \\\n",
    "df['page_width'], df['page_height'] = zip(*[(C['l'], C['t'], C['r'], C['b'], C['font_size'],\n",
    "                                             C['l_'], C['t_'], C['r_'], C['b_'], C['font_size_'],\n",
    "                                             C['page_width'], C['page_height']) for C in df.LINES_DICT])\n",
    "\n",
    "# 1. Line-Level features\n",
    "df['F1_CONTAINSDIGIT'], df['F1_CONTAINSALLDIGIT'], df['F1_CONTAINSEMAIL'], df['F1_CONTAINSURL'], \\\n",
    "df['F1_CONTAINSDATE'], df['F1_CONTAINSPHONE'] = zip(*df.LINES.apply(f1_lineLevel))\n",
    "df['F1_emails'] = df.LINES.apply(f1_checkEmail)\n",
    "df['F1_urls'] = df.LINES.apply(f1_checkURL)\n",
    "df['F3_abbrv'] = df.LINES.apply(f3_checkAbbrv)\n",
    "df['F4_DictWords'], df['F4_Cap1DictWords'], df['F4_CapDictWords'], df['F4_NonDictWords'], df['F4_Cap1NonDictWords'], df['F4_CapNonDictWords'] = zip(*df.LINES.apply(f4_checkDictWord))\n",
    "\n",
    "# 2. Contextual and AB features\n",
    "df['F5_isSpacyNERLine'], df['F5_perSpacyNERAddress'], df['F5_perCrfNERAddress'], df['F5_postalAB'] = f5_checkNeighbourAddress(df, enable_truecasing=True)\n",
    "\n",
    "# 3. Position features\n",
    "df['F6_lineQuadrant'] = df.apply(f6_checkLineQuadrant, axis=1)\n",
    "\n",
    "# 4. Geographical features\n",
    "df['F7_gpe'] = df.LINES.apply(f7_checkGPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.21487,
     "end_time": "2021-02-04T08:31:06.443059",
     "exception": false,
     "start_time": "2021-02-04T08:31:06.228189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Total Numerical Features = 26**\n",
    "\n",
    "- l_ > Regularized left coordinate of bounding box (rectangle) around char/word/line. \n",
    "- t_  > Regularized top coordinate of bounding box (rectangle) around char/word/line. Point(x,y) = Point(l,t) of principal diagonal of box.\n",
    "- r_  > Regularized right coordinate of bounding box (rectangle) around char/word/line. \n",
    "- b_  > Regularized bottom coordinate of bounding box (rectangle) around char/word/line. Point(x,y) = Point(r,b) of principal diagonal of box.\n",
    "- FS_  > Regularized font size of line (taken as mean of all font-sizes of all chars present in line)\n",
    "\n",
    "- F1_CONTAINSDIGIT > If line contains a digit or not\n",
    "- F1_CONTAINSALLDIGIT > If line contains only digits\n",
    "- F1_CONTAINSEMAIL > If line contains email or not\n",
    "- F1_CONTAINSURL > If line contains URL or not\n",
    "- F1_CONTAINSDATE > If line contains Date NER or not\n",
    "- F1_CONTAINSPHONE > If line contains Phone NER or not\n",
    "- F1_emails  > % of emails/gmails present in line. % = n(emails)/n(total words)\n",
    "- F1_urls  > % of urls/weblinks with http/https present in line. % = n(urls)/n(total words)\n",
    "- F3_abbrv  > BOOL whether a known abbrv (legal + dynamic) is present in line or not. BOOL = 1 or 0\n",
    "\n",
    "- F4_DictWords  > % of lang='en' dict words present in line. % = n(dict words)/n(total words)\n",
    "- F4_Cap1DictWords  > % of lang='en' dict words with 1st letter capital in line. % = n(Cap1dict words)/n(total words)\n",
    "- F4_CapDictWords  > % of lang='en' dict words in uppercase/capital in line. % = n(Capdict words)/n(total words)\n",
    "- F4_NonDictWords  > % of lang='en' NON dict words present in line. % = n(NONdict words)/n(total words)\n",
    "- F4_Cap1NonDictWords  > % of lang='en' NON dict words with 1st letter capital in line. % = n(Cap1NONdict words)/n(total words)\n",
    "- F4_CapNonDictWords  > % of lang='en' NON dict words in uppercase/capital in line. % = n(CapNONdict words)/n(total words)\n",
    "\n",
    "- F5_isSpacyNERLine  > BOOL if the line contains spacyNER(ORG, FAC) tag or not. BOOL = 1 or 0\n",
    "- F5_perSpacyNERAddress  > % of spacyNER(GPE, LOC, FAC, ORDINAL) tags in '6' Lines below (excluding present line). % = n(tags)/total words\n",
    "- F5_perCrfNERAddress  > % of crfNER(list of 8 tags) in '6' Lines below (excluding present line). % = n(tags)/total words\n",
    "- F5_postalAB > Calcuates the %score of each line inside a AddressBlock based on its position and other factors.\n",
    "\n",
    "- F6_lineQuadrant  > cals intersection area between line-box and all 6 quardant-boxes. Finds the Q in which Line lies.\n",
    "- F7_gpe  > % of Spacy+GeoText (GPE) tags in line. % n(tags)/n(total words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.216103,
     "end_time": "2021-02-04T08:31:06.874205",
     "exception": false,
     "start_time": "2021-02-04T08:31:06.658102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `TYPE II. Text Features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.216043,
     "end_time": "2021-02-04T08:31:07.312548",
     "exception": false,
     "start_time": "2021-02-04T08:31:07.096505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For each line(L), 4 lines below it (including the L) are taken and all Organizational/Geographical NER tags are identified. Then each line is represented in terms of these NER tags.\n",
    "\n",
    "    L = \"hello what's up?\"               -> []\n",
    "    L+1 = \"Going to Mumbai\"              -> [Mumbai, GPE]\n",
    "    L+2 = \"I work at Google\"             -> [Google, ORG]\n",
    "    L+3 = \"Nice, noteworthy greenland!\"  -> [Greenland, LOC]\n",
    "\n",
    "    L_NER = [GPE, ORG, LOC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:07.768680Z",
     "iopub.status.busy": "2021-02-04T08:31:07.764099Z",
     "iopub.status.idle": "2021-02-04T08:31:07.772101Z",
     "shell.execute_reply": "2021-02-04T08:31:07.771361Z"
    },
    "papermill": {
     "duration": 0.241525,
     "end_time": "2021-02-04T08:31:07.772262",
     "exception": false,
     "start_time": "2021-02-04T08:31:07.530737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# VECTORIZER: Converts a line into NER tags corresponding to its context\n",
    "##########################################################################\n",
    "\n",
    "def line_context(df, enable_regex=True, enable_truecasing=True):\n",
    "    \"\"\"\n",
    "    Finds number of NER(GPE-related) tags in neighbouring lines(context) for each line.\n",
    "    :param: dataframe df\n",
    "    :return: num of tags in context for each line.\n",
    "    \"\"\"\n",
    "\n",
    "    def regex(text):\n",
    "        # removes digits symbols, etc\n",
    "        text = re.sub(\"\\s+\", \" \", re.sub(r\"\\^+\", \" \", re.sub(r\"[^A-z\\!\\@\\&\\(\\)\\,\\.\\?]+\", \" \", text.strip(), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE), flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "        text = re.sub(r\"po.box|po.box.|po. box|po. box.|p.o.box|p.o.box.|p.o box|p.o box.|p.o. box|p.o. box.\", \"Chicago\", text.strip(), flags=re.IGNORECASE | re.MULTILINE).strip()\n",
    "        return text\n",
    "    \n",
    "    def stanfordNLP_truecasing(text):\n",
    "        # TrueCasing - converts text into original casing  \n",
    "        if len(text.strip()) > 0:\n",
    "            doc = stf_nlp(text)\n",
    "            text = \" \".join([w.text.capitalize() if w.upos in [\"PROPN\",\"NNS\"] else w.text for sent in doc.sentences for w in sent.words])\n",
    "            return text\n",
    "        else:\n",
    "            return text\n",
    "        \n",
    "    LINE_CONTEXT_LOOKUP = 4\n",
    "    line_context_NERtags =[]\n",
    "    for i in range(len(df.LINES)):\n",
    "        # text = Present LINE + 3 lines below as context\n",
    "        text = \" \".join(df.LINES.iloc[i:i+LINE_CONTEXT_LOOKUP].tolist()).strip()\n",
    "        # preprocessing\n",
    "        if enable_regex == True:\n",
    "            text = regex(text)\n",
    "        if enable_truecasing == True:\n",
    "            text = stanfordNLP_truecasing(text)\n",
    "        # Spacy NER Tagging\n",
    "        doc = nlp(text)\n",
    "        spacy_NER_line_context_tag = ['ORG', 'FAC', 'GPE', 'LOC', 'ORDINAL']\n",
    "        NER_tags = [ent.label_ for ent in doc.ents if ent.label_ in spacy_NER_line_context_tag]\n",
    "        line_context_NERtags.append(NER_tags)\n",
    "    return line_context_NERtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:08.376759Z",
     "iopub.status.busy": "2021-02-04T08:31:08.376053Z",
     "iopub.status.idle": "2021-02-04T08:31:08.540508Z",
     "shell.execute_reply": "2021-02-04T08:31:08.539974Z"
    },
    "papermill": {
     "duration": 0.525139,
     "end_time": "2021-02-04T08:31:08.540665",
     "exception": false,
     "start_time": "2021-02-04T08:31:08.015526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-120>\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                                \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                                \u001b[0mmax_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                                include_children=include_children)\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mmem_usage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36m_func_exec\u001b[0;34m(stmt, ns)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# helper for magic_memit, just a function proxy for the exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;31m# statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# Vectorization - Representation of each line (L) in terms of context and its spacy NER tags. \n",
    "#                 Each line is concatanated with 3 lines below it using \" \".join() and projected to \n",
    "#                 Regex(removal of digits), TrueCasing, and then Spacy NER tagging.\n",
    "#\n",
    "#                 vector(L) == L + 3lines -> remove digits -> truecasing -> SpacyNER -> [list of ner tags]\n",
    "\n",
    "df['LINE_NER'] = line_context(df, enable_regex=True, enable_truecasing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.219795,
     "end_time": "2021-02-04T08:31:08.987346",
     "exception": false,
     "start_time": "2021-02-04T08:31:08.767551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.220492,
     "end_time": "2021-02-04T08:31:09.430951",
     "exception": false,
     "start_time": "2021-02-04T08:31:09.210459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>11. Saving & Loading Module </ins>\n",
    "\n",
    "- All data is saved in excel chunks.\n",
    "- DB is not used as data volumse is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.22282,
     "end_time": "2021-02-04T08:31:09.874348",
     "exception": false,
     "start_time": "2021-02-04T08:31:09.651528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "BOT\n",
    "\n",
    "--> SAVE_DF\n",
    "    \n",
    "- ------->  DF_30042020\n",
    "- ------->  DF_04052020\n",
    "- ------->  **DF_14052020_LATEST** : Contains the latest df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.224908,
     "end_time": "2021-02-04T08:31:10.332040",
     "exception": false,
     "start_time": "2021-02-04T08:31:10.107132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:10.785175Z",
     "iopub.status.busy": "2021-02-04T08:31:10.784233Z",
     "iopub.status.idle": "2021-02-04T08:31:10.786885Z",
     "shell.execute_reply": "2021-02-04T08:31:10.787506Z"
    },
    "papermill": {
     "duration": 0.232363,
     "end_time": "2021-02-04T08:31:10.787713",
     "exception": false,
     "start_time": "2021-02-04T08:31:10.555350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_data = \"SAVE_DF/DF_LATEST (01_07_2020)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:11.253806Z",
     "iopub.status.busy": "2021-02-04T08:31:11.247475Z",
     "iopub.status.idle": "2021-02-04T08:31:11.260166Z",
     "shell.execute_reply": "2021-02-04T08:31:11.260711Z"
    },
    "papermill": {
     "duration": 0.245672,
     "end_time": "2021-02-04T08:31:11.260914",
     "exception": false,
     "start_time": "2021-02-04T08:31:11.015242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Original DF Shape = \", df.shape)\n",
    "\n",
    "# Rows of 50,000\n",
    "size = 50000\n",
    "list_of_dfs = [df.loc[i:i+size-1,:] for i in range(0, len(df),size)]\n",
    "i=0\n",
    "for d in list_of_dfs:\n",
    "    d['KEY'] = \"KEY_\" + str(i)\n",
    "    fn = save_data + \"DF_\" + str(i) + \".xlsx\"\n",
    "    print(\"Saving  ::  File = \", fn)\n",
    "    d.to_excel(fn)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.22382,
     "end_time": "2021-02-04T08:31:11.704563",
     "exception": false,
     "start_time": "2021-02-04T08:31:11.480743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:12.165138Z",
     "iopub.status.busy": "2021-02-04T08:31:12.164398Z",
     "iopub.status.idle": "2021-02-04T08:31:12.168469Z",
     "shell.execute_reply": "2021-02-04T08:31:12.167852Z"
    },
    "papermill": {
     "duration": 0.232341,
     "end_time": "2021-02-04T08:31:12.168658",
     "exception": false,
     "start_time": "2021-02-04T08:31:11.936317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_data = \"SAVE_DF/DF_LATEST (01_07_2020)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:12.643512Z",
     "iopub.status.busy": "2021-02-04T08:31:12.642723Z",
     "iopub.status.idle": "2021-02-04T08:31:12.651114Z",
     "shell.execute_reply": "2021-02-04T08:31:12.650484Z"
    },
    "papermill": {
     "duration": 0.256623,
     "end_time": "2021-02-04T08:31:12.651277",
     "exception": false,
     "start_time": "2021-02-04T08:31:12.394654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SAVE_DF/DF_LATEST (01_07_2020)/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SAVE_DF/DF_LATEST (01_07_2020)/'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.DataFrame()\n",
    "total_saved_df = len(os.listdir(load_data))\n",
    "for i in range(total_saved_df):\n",
    "    fn = load_data + \"DF_\" + str(i) + \".xlsx\"\n",
    "    print(\"Loading  ::  File = \", fn)\n",
    "    temp_df = pd.read_excel(fn)\n",
    "    df = df.append(temp_df)\n",
    "\n",
    "# Re-shaping loaded df...\n",
    "df = df.drop(columns=['Unnamed: 0', 'KEY']).reset_index(drop=True)\n",
    "df.LINE_NER = df.LINE_NER.apply(lambda x: ast.literal_eval(x))\n",
    "print(\"\\nLoaded df Shape = \", df.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.225921,
     "end_time": "2021-02-04T08:31:13.103827",
     "exception": false,
     "start_time": "2021-02-04T08:31:12.877906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.221279,
     "end_time": "2021-02-04T08:31:13.547002",
     "exception": false,
     "start_time": "2021-02-04T08:31:13.325723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins> 12. Filtering Dataset Module </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.221882,
     "end_time": "2021-02-04T08:31:13.989797",
     "exception": false,
     "start_time": "2021-02-04T08:31:13.767915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Filtering those XML files for which mapping doesn't exit in mapper file, i.e. labelling doesn't exist and manual labelling effort is required for these files.\n",
    "\n",
    "\n",
    "- Manual labelling shall be done following the existing mapper file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:14.453273Z",
     "iopub.status.busy": "2021-02-04T08:31:14.445444Z",
     "iopub.status.idle": "2021-02-04T08:31:14.485915Z",
     "shell.execute_reply": "2021-02-04T08:31:14.486710Z"
    },
    "papermill": {
     "duration": 0.27608,
     "end_time": "2021-02-04T08:31:14.486942",
     "exception": false,
     "start_time": "2021-02-04T08:31:14.210862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'FILENAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-294d7aed9624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Label count per file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mFilename_Label_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FILENAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mFilename_Label_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y_SN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFilename_Label_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Files with atleast 1 label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   6722\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6723\u001b[0m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6724\u001b[0;31m             \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6725\u001b[0m         )\n\u001b[1;32m   6726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             )\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'FILENAME'"
     ]
    }
   ],
   "source": [
    "# Label count per file\n",
    "Filename_Label_data = pd.DataFrame(df.groupby('FILENAME').Y_SN.sum()).reset_index(level=0)\n",
    "Filename_Label_data['Y_SN'] = np.where(Filename_Label_data.Y_SN > 0, 1, 0)\n",
    "\n",
    "# Files with atleast 1 label\n",
    "label_files = list(set(Filename_Label_data[Filename_Label_data.Y_SN == 1].FILENAME.unique()))\n",
    "\n",
    "# Files with no labels (multiline split label possibilty - ignoring this for now!)\n",
    "no_label_files = list(set(Filename_Label_data[Filename_Label_data.Y_SN == 0].FILENAME.unique()))\n",
    "\n",
    "# FILTERING DATASET...\n",
    "# - Taking files with atleast one label(maybe - having one-liner SN)\n",
    "df_labelled = df[df['FILENAME'].isin(label_files)]\n",
    "df_labelled = df_labelled.dropna(subset=['LINES']).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:14.939182Z",
     "iopub.status.busy": "2021-02-04T08:31:14.938426Z",
     "iopub.status.idle": "2021-02-04T08:31:14.972876Z",
     "shell.execute_reply": "2021-02-04T08:31:14.972165Z"
    },
    "papermill": {
     "duration": 0.266037,
     "end_time": "2021-02-04T08:31:14.973032",
     "exception": false,
     "start_time": "2021-02-04T08:31:14.706995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<............... STATS .................>\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'FILENAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-15a225a2aabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print(\"{}\\nShape = {}\\nFiles = {} || 0: {} | 1: {} ||\\nLines = {} || 0: {} | 1: {} ||\\n{}\\n\"\n\u001b[0;32m----> 4\u001b[0;31m       .format(\"------------- ORIGINAL DF -------------\", df.shape, len(df.FILENAME.unique()), Filename_Label_data.Y_SN.value_counts()[0], Filename_Label_data.Y_SN.value_counts()[1], df.Y_SN.shape[0], df.Y_SN.value_counts()[0], df.Y_SN.value_counts()[1], \"-------------------------------------\"))\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m print(\"{}\\nShape = {}\\nFiles = {} || 0: {} | 1: {} ||\\nLines = {} || 0: {} | 1: {} ||\\n{}\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5459\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5460\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'FILENAME'"
     ]
    }
   ],
   "source": [
    "print(\"<............... STATS .................>\\n\")\n",
    "\n",
    "print(\"{}\\nShape = {}\\nFiles = {} || 0: {} | 1: {} ||\\nLines = {} || 0: {} | 1: {} ||\\n{}\\n\"\n",
    "      .format(\"------------- ORIGINAL DF -------------\", df.shape, len(df.FILENAME.unique()), Filename_Label_data.Y_SN.value_counts()[0], Filename_Label_data.Y_SN.value_counts()[1], df.Y_SN.shape[0], df.Y_SN.value_counts()[0], df.Y_SN.value_counts()[1], \"-------------------------------------\"))\n",
    "              \n",
    "print(\"{}\\nShape = {}\\nFiles = {} || 0: {} | 1: {} ||\\nLines = {} || 0: {} | 1: {} ||\\n{}\"\n",
    "      .format(\"------------- FILTERED DF -------------\", df_labelled.shape, len(df_labelled.FILENAME.unique()), 0, pd.Series(np.where(df_labelled.groupby(\"FILENAME\").Y_SN.sum() > 0, 1, 0)).value_counts()[1], df_labelled.Y_SN.shape[0], df_labelled.Y_SN.value_counts()[0], df_labelled.Y_SN.value_counts()[1], \"-------------------------------------\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.220194,
     "end_time": "2021-02-04T08:31:15.416407",
     "exception": false,
     "start_time": "2021-02-04T08:31:15.196213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.22409,
     "end_time": "2021-02-04T08:31:15.864260",
     "exception": false,
     "start_time": "2021-02-04T08:31:15.640170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  <ins>13. Training Module</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.220345,
     "end_time": "2021-02-04T08:31:16.315725",
     "exception": false,
     "start_time": "2021-02-04T08:31:16.095380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Phase I: Line Classification Training\n",
    "        - Trains a classifier model to learn which line is a SN Line and which is not.\n",
    "  \n",
    "...\n",
    "\n",
    "- Phase II: Chunk Identification Training\n",
    "        - Trains a Scoring Decision Matrix to decide what part of chunk correpsonds to a SN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.222601,
     "end_time": "2021-02-04T08:31:16.761232",
     "exception": false,
     "start_time": "2021-02-04T08:31:16.538631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`TEXT`: Textual features \n",
    "\n",
    "`LINGUISTIC`: Numerical features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.223141,
     "end_time": "2021-02-04T08:31:17.206406",
     "exception": false,
     "start_time": "2021-02-04T08:31:16.983265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `Phase I: Line Classification Training`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.224353,
     "end_time": "2021-02-04T08:31:17.653371",
     "exception": false,
     "start_time": "2021-02-04T08:31:17.429018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<ins> `Normal Methods: ` </ins>\n",
    "\n",
    "1. **Method 1: LINGUSITIC [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC) --------------BEST----------**\n",
    "\n",
    "\n",
    "2. Method 2: TEXT + LINGUSITIC via direct concat [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "3. Method 3: TEXT(DR) + LINGUSITIC via direct concat [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "4. Method 4: TEXT [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "5. Method 5: TEXT + LINGUSITIC via hstack [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "6. Method 6: LINGUSITIC using Ensemble_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "7. Method 7: TEXT + LINGUSITIC using Ensemble_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "...\n",
    "\n",
    "<ins> `NN Methods: ` </ins>\n",
    "8. Method 8: TEXT using LSTM_CLF\n",
    "\n",
    "\n",
    "9. Method 9: TEXT using LSTM_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "10. Method 10: TEXT using LSTM_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC + TEXT)\n",
    "\n",
    "\n",
    "11. Method 11: TEXT(*ELMO*) using LSTM_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "12. Method 12: LINGUSITIC using NN_CLF\n",
    "\n",
    "\n",
    "13. Method 13: LINGUSITIC using NN_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "14. Method 14: LINGUSITIC using NN_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC + TEXT)\n",
    "\n",
    "\n",
    "15. Method 15: TEXT + LINGUSITIC via h-stack using NN_CLF\n",
    "\n",
    "\n",
    "16. Method 16: TEXT + LINGUSITIC via h-stack using NN_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + LINGUSITIC)\n",
    "\n",
    "\n",
    "17. **Method 17: TEXT + LINGUSITIC via 2-input Channel using LSTM_CLF [BASE]   --------------BEST----------**\n",
    "\n",
    "\n",
    "18. Method 18: TEXT(NER -> Clustering -> OHE) + LINGUSITIC via h-stack *<ins>EXPERIMENTAL</ins>*\n",
    "\n",
    "\n",
    "19. Method 19: Ensembling via Stacking - LINGUSITIC Features using VotingClassifier (XGB, RF, AB)\n",
    "\n",
    ".\n",
    "\n",
    "20. **FINAL ARCHITECTURE**\n",
    "\n",
    "...\n",
    "\n",
    "#### Normalization\n",
    "    \n",
    "Used RandomForest feature importance to check values with and without normailization of meta-data attributes. Figured out, without normalization of meta-data attributes gave better results.\n",
    "    \n",
    "1. Normalize: Meta-data attributes(Coordinates, FS) + Engineered Numerical Features\n",
    "2. Normalize: Engineered Numerical Features   **[SELECTED]**\n",
    "    \n",
    "...\n",
    "    \n",
    "#### Feature Transformation\n",
    "Various feature transformation techniques were performed. None of them seemsed to improve results, and thus **no feature transformation was used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.22399,
     "end_time": "2021-02-04T08:31:18.106729",
     "exception": false,
     "start_time": "2021-02-04T08:31:17.882739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### FEATURE SELECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:18.555767Z",
     "iopub.status.busy": "2021-02-04T08:31:18.554686Z",
     "iopub.status.idle": "2021-02-04T08:31:18.557150Z",
     "shell.execute_reply": "2021-02-04T08:31:18.557739Z"
    },
    "papermill": {
     "duration": 0.228889,
     "end_time": "2021-02-04T08:31:18.557933",
     "exception": false,
     "start_time": "2021-02-04T08:31:18.329044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ALL FEATURES\n",
    "\n",
    "# Features_NUM = ['l_', 't_', 'r_', 'b_', 'FS_', 'F1_CONTAINSDIGIT', 'F1_CONTAINSALLDIGIT', 'F1_CONTAINSEMAIL',\n",
    "#                 'F1_CONTAINSURL', 'F1_CONTAINSDATE', 'F1_CONTAINSPHONE', 'F1_emails', 'F1_urls', 'F3_abbrv',\n",
    "#                 'F4_DictWords', 'F4_Cap1DictWords', 'F4_CapDictWords', 'F4_NonDictWords', 'F4_Cap1NonDictWords',\n",
    "#                 'F4_CapNonDictWords', 'F5_isSpacyNERLine', 'F5_perSpacyNERAddress', 'F5_perCrfNERAddress', 'F5_postalAB', \n",
    "#                 'F6_lineQuadrant', 'F7_gpe']\n",
    "\n",
    "# normalize_cols = ['F4_DictWords', 'F4_Cap1DictWords', 'F4_CapDictWords', 'F4_NonDictWords', 'F4_Cap1NonDictWords', \n",
    "#                   'F4_CapNonDictWords', 'F5_perSpacyNERAddress', 'F5_perCrfNERAddress', 'F5_postalAB', 'F7_gpe']\n",
    "\n",
    "# Features_L2 = Features_NUM + ['P0', 'P1']\n",
    "\n",
    "# Target = 'Y_SN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:19.024351Z",
     "iopub.status.busy": "2021-02-04T08:31:19.023334Z",
     "iopub.status.idle": "2021-02-04T08:31:19.030539Z",
     "shell.execute_reply": "2021-02-04T08:31:19.031147Z"
    },
    "papermill": {
     "duration": 0.250542,
     "end_time": "2021-02-04T08:31:19.031349",
     "exception": false,
     "start_time": "2021-02-04T08:31:18.780807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SELECTED FEATURES\n",
    "# - All features were taken except 3 (SpacyNERLine, SpacyNERAddress, CrfNERAddress).\n",
    "\n",
    "Features_NUM = ['l_', 't_', 'r_', 'b_', 'FS_', 'F1_CONTAINSDIGIT', 'F1_CONTAINSALLDIGIT', 'F1_CONTAINSEMAIL',\n",
    "                'F1_CONTAINSURL', 'F1_CONTAINSDATE', 'F1_CONTAINSPHONE', 'F1_emails', 'F1_urls', 'F3_abbrv',\n",
    "                'F4_DictWords', 'F4_Cap1DictWords', 'F4_CapDictWords', 'F4_NonDictWords', 'F4_Cap1NonDictWords',\n",
    "                'F4_CapNonDictWords', 'F5_postalAB', 'F6_lineQuadrant', 'F7_gpe']\n",
    "\n",
    "normalize_cols = ['F4_DictWords', 'F4_Cap1DictWords', 'F4_CapDictWords', 'F4_NonDictWords', 'F4_Cap1NonDictWords', \n",
    "                  'F4_CapNonDictWords', 'F5_postalAB', 'F7_gpe']\n",
    "\n",
    "Features_L2 = Features_NUM + ['P0', 'P1']\n",
    "\n",
    "Target = 'Y_SN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.223408,
     "end_time": "2021-02-04T08:31:19.498776",
     "exception": false,
     "start_time": "2021-02-04T08:31:19.275368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:19.947952Z",
     "iopub.status.busy": "2021-02-04T08:31:19.947247Z",
     "iopub.status.idle": "2021-02-04T08:31:19.963946Z",
     "shell.execute_reply": "2021-02-04T08:31:19.963186Z"
    },
    "papermill": {
     "duration": 0.244453,
     "end_time": "2021-02-04T08:31:19.964103",
     "exception": false,
     "start_time": "2021-02-04T08:31:19.719650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Helper 1: Accuracy Metrics for Generic and NN models\n",
    "##########################################################################\n",
    "# INPUT  -> df, X, Y_true, model(keras/sklearn)\n",
    "# OUTPUT -> Df with P0(Prob of 0) and P1(Prob of 1)\n",
    "\n",
    "def accuracy(df, X, Y, model):\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def accuracy_metrics(Y_true, Y_pred, pos_probs):\n",
    "        def percent(value):\n",
    "            return round(float(value)*100.0, 3)\n",
    "        accuracy = percent(accuracy_score(Y_true, Y_pred))\n",
    "        precision = percent(precision_score(Y_true, Y_pred))\n",
    "        recall = percent(recall_score(Y_true, Y_pred))\n",
    "        f1 = percent(f1_score(Y_true, Y_pred))\n",
    "        roc_auc = roc_auc_score(Y_true, pos_probs)\n",
    "        P, R, _ = precision_recall_curve(Y_true, pos_probs)\n",
    "        pr_auc = auc(R, P)\n",
    "        confusion = confusion_matrix(Y_true, Y_pred)\n",
    "        TP, TN, FP, FN = confusion[1, 1], confusion[0, 0], confusion[0, 1], confusion[1, 0]\n",
    "        print(\"Accuracy: {};  P: {};  R: {};  F1: {};  ROC-AUC: {};  PR-AUC: {}\\nValueCounts() :: \\n{}\\nTP = {}\\nTN = {}\\\n",
    "              \\nFP = {}\\nFN = {}\".format(accuracy, precision, recall, f1, roc_auc, pr_auc, \n",
    "                                         pd.Series(Y_true).value_counts(), TP, TN, FP, FN))\n",
    "    \n",
    "    # Decides probability function based on model type\n",
    "    def accuracy_normal():\n",
    "        Y_pred_probs = model.predict_proba(X)\n",
    "        Y_pred = np.where(Y_pred_probs>0.50, 1, 0).argmax(axis=1)\n",
    "        pos_probs = Y_pred_probs[:, 1]\n",
    "        return Y_pred_probs, Y_pred, pos_probs\n",
    "    def accuracy_NN():\n",
    "        Y_pred_probs = model.predict(X, verbose=0)\n",
    "        Y_pred = np.where(Y_pred_probs>0.50, 1, 0).argmax(axis=1)\n",
    "        pos_probs = Y_pred_probs[:, 1]\n",
    "        return Y_pred_probs, Y_pred, pos_probs\n",
    "    \n",
    "    # Check model type\n",
    "    if 'keras' in str(model):\n",
    "        # NN model\n",
    "        Y_pred_probs, Y_pred, pos_probs = accuracy_NN()\n",
    "    else:\n",
    "        # Normal model\n",
    "        Y_pred_probs, Y_pred, pos_probs = accuracy_normal()\n",
    "    \n",
    "    # Calculate Accuracy Metrics\n",
    "    ACCURACY = accuracy_metrics(Y, Y_pred, pos_probs)\n",
    "    \n",
    "    # Store Model Probability Predictions P0, P1\n",
    "    df['Y_PRED'] = Y_pred\n",
    "    df['P0'], df['P1'] = zip(*Y_pred_probs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:20.418402Z",
     "iopub.status.busy": "2021-02-04T08:31:20.417639Z",
     "iopub.status.idle": "2021-02-04T08:31:20.426193Z",
     "shell.execute_reply": "2021-02-04T08:31:20.425575Z"
    },
    "papermill": {
     "duration": 0.238928,
     "end_time": "2021-02-04T08:31:20.426357",
     "exception": false,
     "start_time": "2021-02-04T08:31:20.187429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Helper 2: 2-layer Contextual Classifier Model\n",
    "##########################################################################\n",
    "# INPUT  -> level (1 or 2), model, Normalized(X_train, X_test, X_unseen), Y_train, Y_test, Y_unseen\n",
    "# OUTPUT -> returns trained models\n",
    "\n",
    "def execute_model(level, model, X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    ##########################\n",
    "    # LEVEL 1 - CLassification\n",
    "    ##########################\n",
    "    print(\"## ---- Training: Level 1 ---- ##\")\n",
    "    \n",
    "    # 1.Normalization\n",
    "    # Input Data should be NORMALIZED already!\n",
    "    \n",
    "    # 2. Sampling\n",
    "    Sampler = SMOTE('minority')\n",
    "    X_train_final, Y_train_final = Sampler.fit_sample(X_train, Y_train)\n",
    "    \n",
    "    # 3. Fitting model\n",
    "    model.fit(X_train_final, Y_train_final)\n",
    "    \n",
    "    # 4. Predictions(Probs: P0, P1)\n",
    "    print(\"Test Data Results :: \")\n",
    "    TEST_DF_1 = accuracy(test_df, X_test, Y_test, model)\n",
    "    \n",
    "    ##########################\n",
    "    # LEVEL 2 - CLassification\n",
    "    ##########################\n",
    "    if level == 2:\n",
    "        print(\"## ---- Training: Level 2 ---- ##\")\n",
    "        \n",
    "        # 1. Adding probs P0, P1 columns to df\n",
    "        train_df['P0'], train_df['P1'] = zip(*model.predict_proba(X_train))\n",
    "        test_df['P0'], test_df['P1'] = zip(*model.predict_proba(X_test))\n",
    "        \n",
    "        # 2. Considering L2 Features (i.e. Linguistic(Numerical) features + P0, P1)\n",
    "        X_train_L2, X_test_L2 = train_df[Features_L2], test_df[Features_L2]\n",
    "        \n",
    "        # 3. Normalize the final features\n",
    "        Normalize = MinMaxScaler()\n",
    "        X_train_L2[normalize_cols] = Normalize.fit_transform(X_train_L2[normalize_cols])\n",
    "        X_test_L2[normalize_cols] = Normalize.transform(X_test_L2[normalize_cols])\n",
    "        \n",
    "        # 4. Sampling\n",
    "        Sampler = SMOTE('minority')\n",
    "        X_train_final, Y_train_final = Sampler.fit_sample(X_train_L2, Y_train)\n",
    "\n",
    "        # 3. Fitting model\n",
    "        model2 = RandomForestClassifier(n_jobs=-1)\n",
    "        model2.fit(X_train_final, Y_train_final)\n",
    "        \n",
    "        # 5. Final Predictions\n",
    "        print(\"\\nTest Data Results :: \")\n",
    "        TEST_DF = accuracy(test_df, X_test_L2, Y_test, model2)\n",
    "        return Normalize, model, model2, TEST_DF\n",
    "    \n",
    "    else:\n",
    "        # level 1 return\n",
    "        return model, TEST_DF_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:20.888634Z",
     "iopub.status.busy": "2021-02-04T08:31:20.887554Z",
     "iopub.status.idle": "2021-02-04T08:31:20.891782Z",
     "shell.execute_reply": "2021-02-04T08:31:20.891259Z"
    },
    "papermill": {
     "duration": 0.243397,
     "end_time": "2021-02-04T08:31:20.891954",
     "exception": false,
     "start_time": "2021-02-04T08:31:20.648557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Helper 3: Loading Testing data from customer(Porsche), called as \"UNSEEN_DF\" henceforth\n",
    "################################################################################################\n",
    "# INPUT  -> None\n",
    "# OUTPUT -> returns loaded dataframe \"UNSEEN_DF\"\n",
    "\n",
    "def unseendata_load():\n",
    "    # PATH\n",
    "    UNSEEN_DATA_PATH = SAVED_test_data_path\n",
    "    # Loading unseen df\n",
    "    total_saved_df = len(os.listdir(UNSEEN_DATA_PATH))\n",
    "    unseen_df = pd.DataFrame()\n",
    "    for i in range(total_saved_df):\n",
    "        fn = UNSEEN_DATA_PATH + \"DF_\" + str(i) + \".xlsx\"\n",
    "        print(\"Loading      ::\", fn)\n",
    "        temp_df = pd.read_excel(fn)\n",
    "        unseen_df = unseen_df.append(temp_df)\n",
    "    unseen_df = unseen_df.drop(columns=['Unnamed: 0', 'KEY']).reset_index(drop=True)    \n",
    "    # Label count per file\n",
    "    Filename_Label_data = pd.DataFrame(unseen_df.groupby('FILENAME').Y_SN.sum()).reset_index(level=0)\n",
    "    Filename_Label_data['Y_SN'] = np.where(Filename_Label_data.Y_SN > 0, 1, 0)\n",
    "    # Files with alteast 1 label\n",
    "    label_files = list(set(Filename_Label_data[Filename_Label_data.Y_SN == 1].FILENAME.unique()))\n",
    "    # Files with no labels\n",
    "    no_label_files = list(set(Filename_Label_data[Filename_Label_data.Y_SN == 0].FILENAME.unique()))\n",
    "    # FILTERING DF\n",
    "    # - Taking files with atleast 1 label found!\n",
    "    unseen_df_labelled = unseen_df[unseen_df['FILENAME'].isin(label_files)]\n",
    "    unseen_df_labelled = unseen_df_labelled.dropna(subset=['LINES']).reset_index(drop=True).copy()\n",
    "    unseen_df_labelled.LINE_NER = unseen_df_labelled.LINE_NER.apply(lambda x: ast.literal_eval(x))\n",
    "    # Display\n",
    "    print(\"Original Df  :: Shape = {};   Files = {}  >> 0 = {};  1 = {}\".format(unseen_df.shape, Filename_Label_data.shape[0], Filename_Label_data.Y_SN.value_counts()[0], Filename_Label_data.Y_SN.value_counts()[1]))\n",
    "    print(\"Filtered Df  :: Shape = {};   Files = {}  >> 0 = {};   1 = {}\".format(unseen_df_labelled.shape, pd.Series(unseen_df_labelled.groupby('FILENAME').Y_SN.sum()).shape[0], 0, pd.Series(np.where(unseen_df_labelled.groupby(\"FILENAME\").Y_SN.sum() > 0, 1, 0)).value_counts()[1]))\n",
    "    return unseen_df_labelled\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Helper 4: Loading a 'sample' of testing data from same customer(AH), called as \"UNSEEN_AHDF\" henceforth\n",
    "################################################################################################\n",
    "# INPUT  -> df\n",
    "# OUTPUT -> returns loaded dataframe \"UNSEEN_AHDF\"\n",
    "\n",
    "def create_AH_testdf(df):\n",
    "    counter=0\n",
    "    create_ahdf = pd.DataFrame()\n",
    "    for f in df.FILENAME.unique():\n",
    "        tempdf = df[df.FILENAME == f]\n",
    "        if 1 in tempdf.Y_SN.value_counts() and tempdf.Y_SN.value_counts()[1] >=2 :\n",
    "            create_ahdf = create_ahdf.append(tempdf)\n",
    "            counter+=1\n",
    "        if counter == 100:\n",
    "            break\n",
    "    create_ahdf = create_ahdf.reset_index(drop=True)\n",
    "    return create_ahdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:21.345581Z",
     "iopub.status.busy": "2021-02-04T08:31:21.344844Z",
     "iopub.status.idle": "2021-02-04T08:31:21.347876Z",
     "shell.execute_reply": "2021-02-04T08:31:21.347233Z"
    },
    "papermill": {
     "duration": 0.231573,
     "end_time": "2021-02-04T08:31:21.348020",
     "exception": false,
     "start_time": "2021-02-04T08:31:21.116447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##########################################################################\n",
    "# # Helper 5: Loading different Vectorizer functions for TEXTUAL FEATURES\n",
    "# ##########################################################################\n",
    "# # INPUT  -> None\n",
    "# # OUTPUT -> returns loaded vectorizer class\n",
    "\n",
    "# # VECTORIZERS...\n",
    "# class Word2vec_Vectorizer(object):\n",
    "#     def __init__(self, pretrained_model='local'):\n",
    "#         self.word2vec_modelname = pretrained_model \n",
    "#         if self.word2vec_modelname == 'google':\n",
    "#             self.word2vec = KeyedVectors.load_word2vec_format('Models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#         elif self.word2vec_modelname == 'glove':\n",
    "#             self.word2vec = KeyedVectors.load_word2vec_format('Models/glove.6B.300d.txt.word2vec', binary=False)\n",
    "#         else:\n",
    "#             self.word2vec = None\n",
    "#         print(\"word2vec loaded \", self.word2vec_modelname)\n",
    "#         return\n",
    "#     def fit(self, X):\n",
    "#         if self.word2vec_modelname != 'local':\n",
    "#             return\n",
    "#         sentences = X.tolist()\n",
    "#         self.word2vec = Word2Vec(sentences, size=300, min_count=1, seed=7)\n",
    "#         return\n",
    "#     def transform(self, X):\n",
    "#         doc_vector = []\n",
    "#         for doc in X:  \n",
    "#             word_vector_doc = []  \n",
    "#             if doc != []:\n",
    "#                 for word in doc:    \n",
    "#                     if word in self.word2vec.wv:\n",
    "#                         word_vector_doc.append(self.word2vec.wv[word])\n",
    "#                     else:\n",
    "#                         word_vector_doc.append(np.ones(300))\n",
    "#                 doc_vector.append(list(np.mean(word_vector_doc, axis=0)))\n",
    "#             else:\n",
    "#                 doc_vector.append(list(np.zeros(300)))     \n",
    "#         doc_vector = np.array(doc_vector)\n",
    "#         return doc_vector\n",
    "#     def fit_transform(self, X):\n",
    "#         self.fit(X)\n",
    "#         return self.transform(X)\n",
    "#     def model(self):\n",
    "#         return self.word2vec\n",
    "    \n",
    "# class Spacy_Vectorizer(object):\n",
    "#     def __init__(self, nlp):\n",
    "#         self.nlp = nlp\n",
    "#         return\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self    \n",
    "#     def transform(self, X):\n",
    "#         doc_vector = []\n",
    "#         print(len(X))\n",
    "#         c=0\n",
    "#         for doc in X:\n",
    "#             print(c)\n",
    "#             doc_vector.append(self.nlp(doc).vector)\n",
    "#             c+=1\n",
    "#         doc_vector = np.array(doc_vector)\n",
    "#         return doc_vector\n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.transform(X)\n",
    "#     def model(self):\n",
    "#         return self.nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.221528,
     "end_time": "2021-02-04T08:31:21.789730",
     "exception": false,
     "start_time": "2021-02-04T08:31:21.568202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Vectorizers are not being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:22.256872Z",
     "iopub.status.busy": "2021-02-04T08:31:22.250563Z",
     "iopub.status.idle": "2021-02-04T08:31:22.260764Z",
     "shell.execute_reply": "2021-02-04T08:31:22.260036Z"
    },
    "papermill": {
     "duration": 0.244445,
     "end_time": "2021-02-04T08:31:22.260948",
     "exception": false,
     "start_time": "2021-02-04T08:31:22.016503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# Helper 6: Uses only 1 model to predict 'chunk' using Phase II module (which actually requires 2 models)\n",
    "##################################################################################################\n",
    "# INPUT   --> df with only 1 model(for all code before phase II)\n",
    "# OUTPUT  --> Comparison df having SN by prob, SN by order(line number), CN by prob\n",
    "\n",
    "def predict_chunk_lines_PHASE1(df):\n",
    "\n",
    "    # Execute for each filename...\n",
    "    final_df = []\n",
    "    for f in df.FILENAME.unique():\n",
    "        # every df\n",
    "        tempdf = df[df.FILENAME == f].copy().reset_index(drop=True)\n",
    "\n",
    "        #########################################################################\n",
    "        # 1. Line Classification\n",
    "        # TRUE\n",
    "        actual = tempdf[tempdf.Y_SN == 1]\n",
    "        actual_SN = str(actual.SUPPLIER_NAME.tolist()[0])\n",
    "        # Accuracy\n",
    "        correct_df = tempdf[(tempdf.Y_SN == 1) & (tempdf.Y_PRED == 1)]\n",
    "        correct_LINES = correct_df['LINES'].tolist()\n",
    "        if len(correct_LINES) > 0:\n",
    "            correct_LINE_found = 1\n",
    "        else:\n",
    "            correct_LINE_found = 0\n",
    "        #########################################################################\n",
    "\n",
    "        #########################################################################\n",
    "        # 2. Chunk Identification\n",
    "        # Prep df with only 1 model for running phase II prediction module\n",
    "        tempdf = tempdf.rename(columns={\"Y_PRED\": \"Model1_Y_PRED\", \"P0\": \"Model1_P0\", \"P1\": \"Model1_P1\"})\n",
    "        # Creating empty model 2 predcition cols for running phase II\n",
    "        tempdf['Model2_Y_PRED'], tempdf['Model2_P0'], tempdf['Model2_P1']= 0, 0, 0\n",
    "           \n",
    "        if tempdf[tempdf.Model1_Y_PRED==1].shape[0] > 0:\n",
    "            # By Prob...\n",
    "            # > PHASE II MODEL PREDICTION\n",
    "            FINAL_SN_list, SN_list, CN_list = chunk_identification(tempdf)\n",
    "            Final_Pred_SN_byprob = SN_list[0][0]\n",
    "            S1 = fuzz.partial_ratio(actual_SN.lower(), Final_Pred_SN_byprob.lower())\n",
    "            if S1 < 90: S1 = 0\n",
    "\n",
    "            # By Order...\n",
    "            tempdf = tempdf.sort_values(by='Model1_P1', ascending=False)\n",
    "            Final_Pred_SN_byorder = tempdf[tempdf.Model1_Y_PRED==1].LINES.values[0]\n",
    "            S2 = fuzz.partial_ratio(actual_SN.lower(), Final_Pred_SN_byorder.lower())\n",
    "            if S2 < 90: S2 = 0\n",
    "    \n",
    "        else:\n",
    "            Final_Pred_SN_byprob, Final_Pred_SN_byorder, S1, S2 = \"\", \"\", 0, 0\n",
    "        #########################################################################\n",
    "        \n",
    "        # Store in common df\n",
    "        final_df.append({\"FILE\": f, \"SN\": actual_SN, \"CL\": correct_LINE_found, \n",
    "                         \"PRED_SN_Prob\":Final_Pred_SN_byprob, \"S1\":S1, \n",
    "                         \"PRED_SN_Order\":Final_Pred_SN_byorder, \"S2\":S2, \n",
    "                         \"PRED_CN\": CN_list[:3]})\n",
    "    \n",
    "    # DISPLACY\n",
    "    pred_df = pd.DataFrame.from_dict(final_df)\n",
    "    print(\"Total Files = {}; Correct Lines = {}; Lines Missed = {}\\nScore By Prob = {};  Score by Order = {}\".format(pred_df.shape[0], pred_df.CL.sum(), pred_df.shape[0] - pred_df.CL.sum(), pred_df.S1.mean(), pred_df.S2.mean()))\n",
    "    pred_df = pred_df[['FILE', 'SN', 'PRED_SN_Prob', 'PRED_SN_Order', 'PRED_CN']]    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.223353,
     "end_time": "2021-02-04T08:31:22.708306",
     "exception": false,
     "start_time": "2021-02-04T08:31:22.484953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.225821,
     "end_time": "2021-02-04T08:31:23.161761",
     "exception": false,
     "start_time": "2021-02-04T08:31:22.935940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "................................................................................................................................................................................................................................................\n",
    "## GENERIC METHODS\n",
    "................................................................................................................................................................................................................................................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.220043,
     "end_time": "2021-02-04T08:31:23.604492",
     "exception": false,
     "start_time": "2021-02-04T08:31:23.384449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 1: `  Linguistic [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.223295,
     "end_time": "2021-02-04T08:31:24.099321",
     "exception": false,
     "start_time": "2021-02-04T08:31:23.876026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Linguistic Featrues used for training a generic model.\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training another generic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.221595,
     "end_time": "2021-02-04T08:31:24.540126",
     "exception": false,
     "start_time": "2021-02-04T08:31:24.318531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:25.004069Z",
     "iopub.status.busy": "2021-02-04T08:31:25.003085Z",
     "iopub.status.idle": "2021-02-04T08:31:25.009135Z",
     "shell.execute_reply": "2021-02-04T08:31:25.008432Z"
    },
    "papermill": {
     "duration": 0.248207,
     "end_time": "2021-02-04T08:31:25.009301",
     "exception": false,
     "start_time": "2021-02-04T08:31:24.761094",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-6094af0064b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load training, testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load testing data from another customer (called as 'UNSEEN_DF' henceforth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# unseen_df_labelled = unseendata_load()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Load training, testing data\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "\n",
    "# Load testing data from another customer (called as 'UNSEEN_DF' henceforth)\n",
    "# unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.219666,
     "end_time": "2021-02-04T08:31:25.453458",
     "exception": false,
     "start_time": "2021-02-04T08:31:25.233792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Calcuate feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:25.920838Z",
     "iopub.status.busy": "2021-02-04T08:31:25.919587Z",
     "iopub.status.idle": "2021-02-04T08:31:25.924792Z",
     "shell.execute_reply": "2021-02-04T08:31:25.924160Z"
    },
    "papermill": {
     "duration": 0.251198,
     "end_time": "2021-02-04T08:31:25.924966",
     "exception": false,
     "start_time": "2021-02-04T08:31:25.673768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-bd118755d807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Selected features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 2. Normalize the final features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Selected features\n",
    "X_train, X_test = train_df[Features_NUM], test_df[Features_NUM]\n",
    "Y_train, Y_test = train_df[Target], test_df[Target]\n",
    "\n",
    "# 2. Normalize the final features\n",
    "Normalize = MinMaxScaler()\n",
    "X_train[normalize_cols] =  Normalize.fit_transform(X_train[normalize_cols])\n",
    "X_test[normalize_cols] = Normalize.transform(X_test[normalize_cols])\n",
    "\n",
    "# 3. Sampling\n",
    "Sampler = SMOTE('minority')\n",
    "X_train_final, Y_train_final = Sampler.fit_sample(X_train, Y_train)\n",
    "\n",
    "# 4. Fitting model\n",
    "rnd_clf = RandomForestClassifier(n_jobs=-1)\n",
    "rnd_clf.fit(X_train_final, Y_train_final)\n",
    "\n",
    "# 5. Predictions\n",
    "print(\"Test Data Results :: \")\n",
    "ACCURACY = accuracy(test_df, X_test, Y_test, rnd_clf)\n",
    "\n",
    "# 6. Calcuate Feature Importance\n",
    "plt.figure(figsize=(8,8))\n",
    "features = X_train.columns\n",
    "importances = rnd_clf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.220931,
     "end_time": "2021-02-04T08:31:26.368492",
     "exception": false,
     "start_time": "2021-02-04T08:31:26.147561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Execute Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:26.830969Z",
     "iopub.status.busy": "2021-02-04T08:31:26.829947Z",
     "iopub.status.idle": "2021-02-04T08:31:26.835936Z",
     "shell.execute_reply": "2021-02-04T08:31:26.835355Z"
    },
    "papermill": {
     "duration": 0.246182,
     "end_time": "2021-02-04T08:31:26.836103",
     "exception": false,
     "start_time": "2021-02-04T08:31:26.589921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-0fd747cb313d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load training, testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1. Selected features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Load training, testing data\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "\n",
    "# 1. Selected features\n",
    "X_train, X_test = train_df[Features_NUM], test_df[Features_NUM]\n",
    "Y_train, Y_test = train_df[Target], test_df[Target]\n",
    "\n",
    "# 2. Normalize the final features\n",
    "Normalize = MinMaxScaler()\n",
    "X_train[normalize_cols] =  Normalize.fit_transform(X_train[normalize_cols])\n",
    "X_test[normalize_cols] = Normalize.transform(X_test[normalize_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.223175,
     "end_time": "2021-02-04T08:31:27.283298",
     "exception": false,
     "start_time": "2021-02-04T08:31:27.060123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:27.742756Z",
     "iopub.status.busy": "2021-02-04T08:31:27.742102Z",
     "iopub.status.idle": "2021-02-04T08:31:27.745274Z",
     "shell.execute_reply": "2021-02-04T08:31:27.744603Z"
    },
    "papermill": {
     "duration": 0.234773,
     "end_time": "2021-02-04T08:31:27.745423",
     "exception": false,
     "start_time": "2021-02-04T08:31:27.510650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # LR\n",
    "# model1 = LogisticRegression(solver='sag', multi_class='multinomial', C=1.8, penalty='l2', n_jobs=-1, class_weight='balanced')\n",
    "# model1 = execute_model(1, model1, X_train, Y_train, X_test, Y_test, X_unseen, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:28.196203Z",
     "iopub.status.busy": "2021-02-04T08:31:28.195537Z",
     "iopub.status.idle": "2021-02-04T08:31:28.198669Z",
     "shell.execute_reply": "2021-02-04T08:31:28.199334Z"
    },
    "papermill": {
     "duration": 0.229833,
     "end_time": "2021-02-04T08:31:28.199515",
     "exception": false,
     "start_time": "2021-02-04T08:31:27.969682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # SVM\n",
    "# model2 = SVC(kernel='linear', C=0.5, gamma=0.001, probability=True, tol=1e-3, class_weight='balanced')\n",
    "#model2 = execute_model(1, model2, X_train, Y_train, X_test, Y_test, X_unseen, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:28.650521Z",
     "iopub.status.busy": "2021-02-04T08:31:28.649716Z",
     "iopub.status.idle": "2021-02-04T08:31:28.652894Z",
     "shell.execute_reply": "2021-02-04T08:31:28.653601Z"
    },
    "papermill": {
     "duration": 0.230796,
     "end_time": "2021-02-04T08:31:28.653778",
     "exception": false,
     "start_time": "2021-02-04T08:31:28.422982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # AD\n",
    "# model4 = AdaBoostClassifier(random_state=1)\n",
    "# model4 = execute_model(1, model4, X_train, Y_train, X_test, Y_test, X_unseen, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:29.122482Z",
     "iopub.status.busy": "2021-02-04T08:31:29.121768Z",
     "iopub.status.idle": "2021-02-04T08:31:29.125562Z",
     "shell.execute_reply": "2021-02-04T08:31:29.126064Z"
    },
    "papermill": {
     "duration": 0.246637,
     "end_time": "2021-02-04T08:31:29.126243",
     "exception": false,
     "start_time": "2021-02-04T08:31:28.879606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-60b7459a432b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# XGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_DF\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mexecute_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "model3 = XGBClassifier(random_state=1, n_jobs=-1)\n",
    "Normalize, model31, model32, TEST_DF  = execute_model(2, model3, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:29.578113Z",
     "iopub.status.busy": "2021-02-04T08:31:29.577462Z",
     "iopub.status.idle": "2021-02-04T08:31:29.595314Z",
     "shell.execute_reply": "2021-02-04T08:31:29.595931Z"
    },
    "papermill": {
     "duration": 0.245927,
     "end_time": "2021-02-04T08:31:29.596126",
     "exception": false,
     "start_time": "2021-02-04T08:31:29.350199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-8b673cc1ae46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# RF -> BEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel51\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel52\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# RF -> BEST\n",
    "model5 = RandomForestClassifier(n_jobs=-1)\n",
    "Normalize, model51, model52, TEST_DF = execute_model(2, model5, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:30.091743Z",
     "iopub.status.busy": "2021-02-04T08:31:30.090931Z",
     "iopub.status.idle": "2021-02-04T08:31:30.102420Z",
     "shell.execute_reply": "2021-02-04T08:31:30.103099Z"
    },
    "papermill": {
     "duration": 0.282138,
     "end_time": "2021-02-04T08:31:30.103302",
     "exception": false,
     "start_time": "2021-02-04T08:31:29.821164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEST_DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-4e1bbe8550d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II: chunk identification module over above \"unseen_df_labelled\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST_DF' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II: chunk identification module over above \"unseen_df_labelled\"\n",
    "predict_chunk_lines_PHASE1(TEST_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:30.570298Z",
     "iopub.status.busy": "2021-02-04T08:31:30.569588Z",
     "iopub.status.idle": "2021-02-04T08:31:30.574326Z",
     "shell.execute_reply": "2021-02-04T08:31:30.574892Z"
    },
    "papermill": {
     "duration": 0.240503,
     "end_time": "2021-02-04T08:31:30.575072",
     "exception": false,
     "start_time": "2021-02-04T08:31:30.334569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # BEST GENERIC MODEL...\n",
    "# # SAVE DATE 19 May 2020 (Models = Normalize, model51, model52, unseen_df_labelled)\n",
    "\n",
    "# pickle.dump(Normalize, open(\"Models/RF_2Level_Generic_Num_19052020/Model_Normalize_19052020.pickle\", \"wb\"))\n",
    "# pickle.dump(model51, open(\"Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle\", \"wb\")) \n",
    "# pickle.dump(model52, open(\"Models/RF_2Level_Generic_Num_19052020/Model_model52_19052020.pickle\", \"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.283161,
     "end_time": "2021-02-04T08:31:31.091867",
     "exception": false,
     "start_time": "2021-02-04T08:31:30.808706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `BEST GENERIC METHOD USE-CASE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:31.560502Z",
     "iopub.status.busy": "2021-02-04T08:31:31.559747Z",
     "iopub.status.idle": "2021-02-04T08:31:31.576997Z",
     "shell.execute_reply": "2021-02-04T08:31:31.576389Z"
    },
    "papermill": {
     "duration": 0.253892,
     "end_time": "2021-02-04T08:31:31.577156",
     "exception": false,
     "start_time": "2021-02-04T08:31:31.323264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-b52ddd4afd74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Models/RF_2Level_Generic_Num_19052020/Model_model52_19052020.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Models/RF_2Level_Generic_Num_19052020/Model_Normalize_19052020.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle'"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model = pickle.load(open(\"Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle\", \"rb\")) \n",
    "model2 = pickle.load(open(\"Models/RF_2Level_Generic_Num_19052020/Model_model52_19052020.pickle\", \"rb\")) \n",
    "Normalizer = pickle.load(open(\"Models/RF_2Level_Generic_Num_19052020/Model_Normalize_19052020.pickle\", \"rb\"))\n",
    "\n",
    "# Load traing, testing and unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=10, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.236897,
     "end_time": "2021-02-04T08:31:32.045442",
     "exception": false,
     "start_time": "2021-02-04T08:31:31.808545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Testing above loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:32.528680Z",
     "iopub.status.busy": "2021-02-04T08:31:32.527623Z",
     "iopub.status.idle": "2021-02-04T08:31:32.547649Z",
     "shell.execute_reply": "2021-02-04T08:31:32.547000Z"
    },
    "papermill": {
     "duration": 0.265417,
     "end_time": "2021-02-04T08:31:32.547838",
     "exception": false,
     "start_time": "2021-02-04T08:31:32.282421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-ffb7815b6f4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LEVEL 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 1. Select Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# LEVEL 1\n",
    "# 1. Select Features\n",
    "X_train, X_test, X_unseen = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "Y_train, Y_test, Y_unseen = train_df[Target], test_df[Target], unseen_df_labelled[Target]\n",
    "\n",
    "# 2. Normalize features using loaded \"Normalizer\"\n",
    "X_train[normalize_cols] =  Normalizer.transform(X_train[normalize_cols])\n",
    "X_test[normalize_cols] = Normalizer.transform(X_test[normalize_cols])\n",
    "X_unseen[normalize_cols] = Normalizer.transform(X_unseen[normalize_cols])\n",
    "\n",
    "# # 3. Predictions using fitted \"model_1\"\n",
    "# print(\"Test Data Results :: \")\n",
    "# ACCURACY = accuracy(test_df, X_test, Y_test, model5)\n",
    "# print(\"Unseen Data Results :: \")\n",
    "# UDL = accuracy(unseen_df_labelled, X_unseen, Y_unseen, model5)\n",
    "\n",
    "# LEVEL 2\n",
    "# 3. Predictions using \"model_1\"\n",
    "train_df['P0'], train_df['P1'] = zip(*model.predict_proba(X_train))\n",
    "test_df['P0'], test_df['P1'] = zip(*model.predict_proba(X_test))\n",
    "unseen_df_labelled['P0'], unseen_df_labelled['P1'] = zip(*model.predict_proba(X_unseen))\n",
    "\n",
    "# 4. Select Features: Linguistic + P0, P1\n",
    "X_train_L2, X_test_L2, X_unseen_L2 = train_df[Features_L2], test_df[Features_L2], unseen_df_labelled[Features_L2]\n",
    "\n",
    "# 5. Normalize features using loaded \"Normalizer\"\n",
    "X_train_L2[normalize_cols] = Normalizer.transform(X_train_L2[normalize_cols])\n",
    "X_test_L2[normalize_cols] = Normalizer.transform(X_test_L2[normalize_cols])\n",
    "X_unseen_L2[normalize_cols] = Normalizer.transform(X_unseen_L2[normalize_cols])\n",
    "\n",
    "# 6. Predictions using \"model_2\"\n",
    "print(\"\\nTest Data Results :: \")\n",
    "ACCURACY = accuracy(test_df, X_test_L2, Y_test, model2)\n",
    "print(\"\\nUnseen Data Results :: \")\n",
    "UNSEEN_DF = accuracy(unseen_df_labelled, X_unseen_L2, Y_unseen, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:33.028283Z",
     "iopub.status.busy": "2021-02-04T08:31:33.027345Z",
     "iopub.status.idle": "2021-02-04T08:31:33.032884Z",
     "shell.execute_reply": "2021-02-04T08:31:33.032199Z"
    },
    "papermill": {
     "duration": 0.256501,
     "end_time": "2021-02-04T08:31:33.033041",
     "exception": false,
     "start_time": "2021-02-04T08:31:32.776540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UNSEEN_DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-3ddeab2d1d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II: chunk identification module over above \"UNSEEN_DF\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSEEN_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'UNSEEN_DF' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II: chunk identification module over above \"UNSEEN_DF\"\n",
    "predict_chunk_lines_PHASE1(UNSEEN_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.231613,
     "end_time": "2021-02-04T08:31:33.496713",
     "exception": false,
     "start_time": "2021-02-04T08:31:33.265100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.238015,
     "end_time": "2021-02-04T08:31:33.969382",
     "exception": false,
     "start_time": "2021-02-04T08:31:33.731367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 2: ` Text +  Liguistic via direct concat [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Liguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.236443,
     "end_time": "2021-02-04T08:31:34.440157",
     "exception": false,
     "start_time": "2021-02-04T08:31:34.203714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(sparse matrix) + Liguistic(dense matrix) via direct concat --> [Both Sparse] \n",
    "    used for training a generic model.\n",
    "    \n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training another generic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.237296,
     "end_time": "2021-02-04T08:31:34.910984",
     "exception": false,
     "start_time": "2021-02-04T08:31:34.673688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Very poor results \n",
    "- Discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.225871,
     "end_time": "2021-02-04T08:31:35.363536",
     "exception": false,
     "start_time": "2021-02-04T08:31:35.137665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.224685,
     "end_time": "2021-02-04T08:31:35.814485",
     "exception": false,
     "start_time": "2021-02-04T08:31:35.589800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 3: ` Text (*DR) + Liguistic via direct concat [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Liguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.22456,
     "end_time": "2021-02-04T08:31:36.267288",
     "exception": false,
     "start_time": "2021-02-04T08:31:36.042728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(DR using SVD) + Linguistic(dense matrix) via direct concatenation --> [Both Dense] \n",
    "    used for training a generic model.\n",
    "    \n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training another generic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.225416,
     "end_time": "2021-02-04T08:31:36.717008",
     "exception": false,
     "start_time": "2021-02-04T08:31:36.491592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Takes significant amount of time and gives Very poor results \n",
    "- Discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.229582,
     "end_time": "2021-02-04T08:31:37.182004",
     "exception": false,
     "start_time": "2021-02-04T08:31:36.952422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.227127,
     "end_time": "2021-02-04T08:31:37.637731",
     "exception": false,
     "start_time": "2021-02-04T08:31:37.410604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 4: ` Text [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.233641,
     "end_time": "2021-02-04T08:31:38.098099",
     "exception": false,
     "start_time": "2021-02-04T08:31:37.864458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text (Count Vectorized) used for training a generic model.\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training another generic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:38.563574Z",
     "iopub.status.busy": "2021-02-04T08:31:38.562889Z",
     "iopub.status.idle": "2021-02-04T08:31:38.580684Z",
     "shell.execute_reply": "2021-02-04T08:31:38.581301Z"
    },
    "papermill": {
     "duration": 0.251798,
     "end_time": "2021-02-04T08:31:38.581497",
     "exception": false,
     "start_time": "2021-02-04T08:31:38.329699",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-119bba16961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading training, testing, unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading training, testing, unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:39.058268Z",
     "iopub.status.busy": "2021-02-04T08:31:39.057349Z",
     "iopub.status.idle": "2021-02-04T08:31:39.079080Z",
     "shell.execute_reply": "2021-02-04T08:31:39.079701Z"
    },
    "papermill": {
     "duration": 0.266765,
     "end_time": "2021-02-04T08:31:39.079920",
     "exception": false,
     "start_time": "2021-02-04T08:31:38.813155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-9e30afc4635f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Countvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mVect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mVect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_train_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Countvectorizer\n",
    "Vect = CountVectorizer()\n",
    "Vect.fit(train_df.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "X_train_text = Vect.transform(train_df.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "X_test_text = Vect.transform(test_df.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "X_unseen_text = Vect.transform(unseen_df_labelled.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN\n",
    "\n",
    "# Running 2-level contextual classifier using above data\n",
    "model1 = RandomForestClassifier(n_jobs=-1)\n",
    "Normalize, model1, model2, unseen_df_labelled = execute_model(2, model1, X_train_text, Y_train, X_test_text, Y_test, X_unseen_text, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:39.559582Z",
     "iopub.status.busy": "2021-02-04T08:31:39.558906Z",
     "iopub.status.idle": "2021-02-04T08:31:39.565230Z",
     "shell.execute_reply": "2021-02-04T08:31:39.564512Z"
    },
    "papermill": {
     "duration": 0.255776,
     "end_time": "2021-02-04T08:31:39.565382",
     "exception": false,
     "start_time": "2021-02-04T08:31:39.309606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unseen_df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-bc46e7e45e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unseen_df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II\n",
    "predict_chunk_lines_PHASE1(unseen_df_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.244917,
     "end_time": "2021-02-04T08:31:40.042440",
     "exception": false,
     "start_time": "2021-02-04T08:31:39.797523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Underfiited results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.235376,
     "end_time": "2021-02-04T08:31:40.511402",
     "exception": false,
     "start_time": "2021-02-04T08:31:40.276026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.249585,
     "end_time": "2021-02-04T08:31:40.995471",
     "exception": false,
     "start_time": "2021-02-04T08:31:40.745886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 5: ` Text + Linguistic via h-stack [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.231894,
     "end_time": "2021-02-04T08:31:41.481856",
     "exception": false,
     "start_time": "2021-02-04T08:31:41.249962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(CountVectorized) + Linguistic(Numerical) using h-stack used for training a generic model.\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training another generic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:41.963183Z",
     "iopub.status.busy": "2021-02-04T08:31:41.962170Z",
     "iopub.status.idle": "2021-02-04T08:31:41.967792Z",
     "shell.execute_reply": "2021-02-04T08:31:41.967064Z"
    },
    "papermill": {
     "duration": 0.256276,
     "end_time": "2021-02-04T08:31:41.968102",
     "exception": false,
     "start_time": "2021-02-04T08:31:41.711826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-119bba16961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading training, testing, unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading training, testing, unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:42.448288Z",
     "iopub.status.busy": "2021-02-04T08:31:42.447103Z",
     "iopub.status.idle": "2021-02-04T08:31:42.451806Z",
     "shell.execute_reply": "2021-02-04T08:31:42.451017Z"
    },
    "papermill": {
     "duration": 0.250837,
     "end_time": "2021-02-04T08:31:42.452028",
     "exception": false,
     "start_time": "2021-02-04T08:31:42.201191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # word2vec local\n",
    "# X_train_vector_w2v, X_test_vector_w2v, X_unseen_vector_w2v = X_train_vector_w2v, X_test_vector_w2v, X_unseen_vector_w2v\n",
    "# X_train_num, X_test_num, X_unseen_num = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "# Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN\n",
    "\n",
    "# X_train = hstack([X_train_vector_w2v, X_train_num]).tocsr() \n",
    "# X_test = hstack([X_test_vector_w2v, X_test_num]).tocsr()\n",
    "# X_unseen = hstack([X_unseen_vector_w2v, X_unseen_num]).tocsr() \n",
    "\n",
    "# model1 = RandomForestClassifier(n_jobs=-1)\n",
    "# model1 = execute_model(1, model1, X_train, Y_train, X_test, Y_test, X_unseen, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:42.929052Z",
     "iopub.status.busy": "2021-02-04T08:31:42.928285Z",
     "iopub.status.idle": "2021-02-04T08:31:42.931756Z",
     "shell.execute_reply": "2021-02-04T08:31:42.931238Z"
    },
    "papermill": {
     "duration": 0.244414,
     "end_time": "2021-02-04T08:31:42.931949",
     "exception": false,
     "start_time": "2021-02-04T08:31:42.687535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # word2vec glove\n",
    "# X_train_vector_glove, X_test_vector_glove, X_unseen_vector_glove = X_train_vector_glove, X_test_vector_glove, X_unseen_vector_glove\n",
    "# X_train_num, X_test_num, X_unseen_num = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "# Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN\n",
    "\n",
    "# X_train = hstack([X_train_vector_glove, X_train_num]).tocsr() \n",
    "# X_test = hstack([X_test_vector_glove, X_test_num]).tocsr() \n",
    "# X_unseen = hstack([X_unseen_vector_glove, X_unseen_num]).tocsr() \n",
    "\n",
    "# model1 = RandomForestClassifier(n_jobs=-1)\n",
    "# model1 = execute_model(1, model1, X_train, Y_train, X_test, Y_test, X_unseen, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:43.453444Z",
     "iopub.status.busy": "2021-02-04T08:31:43.452318Z",
     "iopub.status.idle": "2021-02-04T08:31:43.455691Z",
     "shell.execute_reply": "2021-02-04T08:31:43.455059Z"
    },
    "papermill": {
     "duration": 0.285787,
     "end_time": "2021-02-04T08:31:43.455852",
     "exception": false,
     "start_time": "2021-02-04T08:31:43.170065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Spacy\n",
    "# X_train_vector_spacy, X_test_vector_spacy, X_unseen_vector_spacy = X_train_vector_spacy, X_test_vector_spacy, X_unseen_vector_spacy\n",
    "# X_train_num, X_test_num, X_unseen_num = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "# Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN\n",
    "\n",
    "# X_train = hstack([X_train_vector_spacy, X_train_num]).tocsr() \n",
    "# X_test = hstack([X_test_vector_spacy, X_test_num]).tocsr()\n",
    "# X_unseen = hstack([X_unseen_vector_spacy, X_unseen_num]).tocsr() \n",
    "\n",
    "# model1 = RandomForestClassifier(n_jobs=-1)\n",
    "# model1 = execute_model(1, model1, X_train, Y_train, X_test, Y_test, X_unseen, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:43.932902Z",
     "iopub.status.busy": "2021-02-04T08:31:43.932194Z",
     "iopub.status.idle": "2021-02-04T08:31:43.936850Z",
     "shell.execute_reply": "2021-02-04T08:31:43.936124Z"
    },
    "papermill": {
     "duration": 0.254749,
     "end_time": "2021-02-04T08:31:43.937019",
     "exception": false,
     "start_time": "2021-02-04T08:31:43.682270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-12fc5f9c1b96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LABEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TEXT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mVect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# LABEL\n",
    "Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN\n",
    "\n",
    "# TEXT\n",
    "Vect = CountVectorizer()\n",
    "X_train_text, X_test_text, X_unseen_text = Vect.fit_transform(train_df.LINE_NER.apply(lambda x: \" \".join(x))), Vect.transform(test_df.LINE_NER.apply(lambda x: \" \".join(x))), Vect.transform(unseen_df_labelled.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# NUM\n",
    "X_train_NUM, X_test_NUM, X_unseen_NUM = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "Normalize = MinMaxScaler()\n",
    "X_train_NUM[normalize_cols], X_test_NUM[normalize_cols], X_unseen_NUM[normalize_cols] =  Normalize.fit_transform(X_train_NUM[normalize_cols]), Normalize.transform(X_test_NUM[normalize_cols]), Normalize.transform(X_unseen_NUM[normalize_cols])\n",
    "\n",
    "# COMBINE\n",
    "X_train, X_test, X_unseen = hstack([X_train_text, X_train_NUM]).tocsr(), hstack([X_test_text, X_test_NUM]).tocsr(), hstack([X_unseen_text, X_unseen_NUM]).tocsr()\n",
    "\n",
    "# Running 2-level contextual classifier using combined features above\n",
    "model1 = RandomForestClassifier(n_jobs=-1)\n",
    "Normalize, model1, model2, unseen_df_labelled= execute_model(2, model1, X_train, Y_train, X_test, Y_test, X_unseen, Y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:44.418878Z",
     "iopub.status.busy": "2021-02-04T08:31:44.417832Z",
     "iopub.status.idle": "2021-02-04T08:31:44.422565Z",
     "shell.execute_reply": "2021-02-04T08:31:44.421934Z"
    },
    "papermill": {
     "duration": 0.252888,
     "end_time": "2021-02-04T08:31:44.422715",
     "exception": false,
     "start_time": "2021-02-04T08:31:44.169827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unseen_df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-bc46e7e45e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unseen_df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II\n",
    "predict_chunk_lines_PHASE1(unseen_df_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.238623,
     "end_time": "2021-02-04T08:31:44.895624",
     "exception": false,
     "start_time": "2021-02-04T08:31:44.657001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Slightly better results but still underfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.23345,
     "end_time": "2021-02-04T08:31:45.362891",
     "exception": false,
     "start_time": "2021-02-04T08:31:45.129441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.230808,
     "end_time": "2021-02-04T08:31:45.834127",
     "exception": false,
     "start_time": "2021-02-04T08:31:45.603319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 6: ` Linguistic using Ensemble_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.229709,
     "end_time": "2021-02-04T08:31:46.301173",
     "exception": false,
     "start_time": "2021-02-04T08:31:46.071464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Linguistic(Numerical) features used for training a Voting classifier model(RF, XGB, AB).\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training Voting Classifier Ensemble model(RF, XGB, AB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.229139,
     "end_time": "2021-02-04T08:31:46.758070",
     "exception": false,
     "start_time": "2021-02-04T08:31:46.528931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were average and over-fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.236167,
     "end_time": "2021-02-04T08:31:47.224023",
     "exception": false,
     "start_time": "2021-02-04T08:31:46.987856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.228737,
     "end_time": "2021-02-04T08:31:47.683727",
     "exception": false,
     "start_time": "2021-02-04T08:31:47.454990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 7: ` Text + Linguistic using Ensemble_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.23178,
     "end_time": "2021-02-04T08:31:48.145347",
     "exception": false,
     "start_time": "2021-02-04T08:31:47.913567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(CountVectorized) + Linguistic(Numerical) via h-stack used for training a Voting classifier.\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training Voting Classifier Ensemble model(RF, XGB, AB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.230307,
     "end_time": "2021-02-04T08:31:48.605289",
     "exception": false,
     "start_time": "2021-02-04T08:31:48.374982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were average and over-fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.236522,
     "end_time": "2021-02-04T08:31:49.073412",
     "exception": false,
     "start_time": "2021-02-04T08:31:48.836890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.231023,
     "end_time": "2021-02-04T08:31:49.537426",
     "exception": false,
     "start_time": "2021-02-04T08:31:49.306403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "................................................................................................................................................................................................................................................\n",
    "## NN MODELS\n",
    "................................................................................................................................................................................................................................................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.231792,
     "end_time": "2021-02-04T08:31:50.002470",
     "exception": false,
     "start_time": "2021-02-04T08:31:49.770678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 8: ` Text using LSTM_CLF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.233085,
     "end_time": "2021-02-04T08:31:50.466752",
     "exception": false,
     "start_time": "2021-02-04T08:31:50.233667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(Normal/Glove/Spacy embeddings) used for training a LSTM Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:50.956328Z",
     "iopub.status.busy": "2021-02-04T08:31:50.955417Z",
     "iopub.status.idle": "2021-02-04T08:31:50.961058Z",
     "shell.execute_reply": "2021-02-04T08:31:50.960372Z"
    },
    "papermill": {
     "duration": 0.261335,
     "end_time": "2021-02-04T08:31:50.961209",
     "exception": false,
     "start_time": "2021-02-04T08:31:50.699874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-119bba16961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading training, testing, unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading training, testing, unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:51.438304Z",
     "iopub.status.busy": "2021-02-04T08:31:51.437348Z",
     "iopub.status.idle": "2021-02-04T08:31:51.442230Z",
     "shell.execute_reply": "2021-02-04T08:31:51.442801Z"
    },
    "papermill": {
     "duration": 0.252294,
     "end_time": "2021-02-04T08:31:51.443004",
     "exception": false,
     "start_time": "2021-02-04T08:31:51.190710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-f6e21c846fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Select Features: Tokenized LINES (df.LINE_T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Select Features: Tokenized LINES (df.LINE_T)\n",
    "X_train, X_test, X_unseen = train_df.LINES_T, test_df.LINES_T, unseen_df_labelled.LINES_T\n",
    "Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:51.908520Z",
     "iopub.status.busy": "2021-02-04T08:31:51.907841Z",
     "iopub.status.idle": "2021-02-04T08:31:51.910307Z",
     "shell.execute_reply": "2021-02-04T08:31:51.910963Z"
    },
    "papermill": {
     "duration": 0.237769,
     "end_time": "2021-02-04T08:31:51.911179",
     "exception": false,
     "start_time": "2021-02-04T08:31:51.673410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NN Settings\n",
    "max_features = 40000\n",
    "sequence_length = 30\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:52.430910Z",
     "iopub.status.busy": "2021-02-04T08:31:52.430210Z",
     "iopub.status.idle": "2021-02-04T08:31:52.433566Z",
     "shell.execute_reply": "2021-02-04T08:31:52.434169Z"
    },
    "papermill": {
     "duration": 0.252753,
     "end_time": "2021-02-04T08:31:52.434481",
     "exception": false,
     "start_time": "2021-02-04T08:31:52.181728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-d4892844e817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenizing\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train, X_test, X_unseen = tokenizer.texts_to_sequences(X_train), \\\n",
    "                            tokenizer.texts_to_sequences(X_test), \\\n",
    "                            tokenizer.texts_to_sequences(X_unseen)\n",
    "\n",
    "# Padding the sequences\n",
    "X_train, X_test, X_unseen = pad_sequences(X_train, padding='post', maxlen=sequence_length), \\\n",
    "                            pad_sequences(X_test, padding='post', maxlen=sequence_length), \\\n",
    "                            pad_sequences(X_unseen, padding='post', maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:52.909386Z",
     "iopub.status.busy": "2021-02-04T08:31:52.908702Z",
     "iopub.status.idle": "2021-02-04T08:31:52.926631Z",
     "shell.execute_reply": "2021-02-04T08:31:52.926043Z"
    },
    "papermill": {
     "duration": 0.260957,
     "end_time": "2021-02-04T08:31:52.926792",
     "exception": false,
     "start_time": "2021-02-04T08:31:52.665835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-0f9ee9b9a451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minority'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "sampler = SMOTE('minority')\n",
    "X_train, Y_train = sampler.fit_sample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:53.409576Z",
     "iopub.status.busy": "2021-02-04T08:31:53.408614Z",
     "iopub.status.idle": "2021-02-04T08:31:53.412640Z",
     "shell.execute_reply": "2021-02-04T08:31:53.413132Z"
    },
    "papermill": {
     "duration": 0.25214,
     "end_time": "2021-02-04T08:31:53.413311",
     "exception": false,
     "start_time": "2021-02-04T08:31:53.161171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-9147d2654e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "# Training Labels\n",
    "Y_train = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:53.883629Z",
     "iopub.status.busy": "2021-02-04T08:31:53.882637Z",
     "iopub.status.idle": "2021-02-04T08:31:53.899680Z",
     "shell.execute_reply": "2021-02-04T08:31:53.900185Z"
    },
    "papermill": {
     "duration": 0.255096,
     "end_time": "2021-02-04T08:31:53.900376",
     "exception": false,
     "start_time": "2021-02-04T08:31:53.645280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-f548430a4ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unique Tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s unique tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Unique Tokens\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:54.374395Z",
     "iopub.status.busy": "2021-02-04T08:31:54.373372Z",
     "iopub.status.idle": "2021-02-04T08:31:54.391584Z",
     "shell.execute_reply": "2021-02-04T08:31:54.392107Z"
    },
    "papermill": {
     "duration": 0.258199,
     "end_time": "2021-02-04T08:31:54.392308",
     "exception": false,
     "start_time": "2021-02-04T08:31:54.134109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-7fe79217a4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unique Tokens + 1 for Embedding Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "# Unique Tokens + 1 for Embedding Layer\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:54.877444Z",
     "iopub.status.busy": "2021-02-04T08:31:54.876428Z",
     "iopub.status.idle": "2021-02-04T08:31:54.881092Z",
     "shell.execute_reply": "2021-02-04T08:31:54.880565Z"
    },
    "papermill": {
     "duration": 0.256333,
     "end_time": "2021-02-04T08:31:54.881241",
     "exception": false,
     "start_time": "2021-02-04T08:31:54.624908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-81b3bd5f8389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Creating a inital matrix of zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# >> GLOVE EMBEDDINGS <<\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_words' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating EMBEDDING LAYER ....\n",
    "\n",
    "# Creating a inital matrix of zeros\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# >> GLOVE EMBEDDINGS <<\n",
    "# embedding_glove = KeyedVectors.load_word2vec_format('Models/glove.6B.300d.txt.word2vec', binary=False)\n",
    "# # for each word in out tokenizer lets try to find that work in local/pretrained embeddings model\n",
    "# for word, i in word_index.items():\n",
    "#     if i > max_features:\n",
    "#         continue\n",
    "#     if word in embedding_glove:\n",
    "#         # we found the word - add that words vector to the matrix\n",
    "#         embedding_vector = embedding_glove[word]\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "#     else:\n",
    "#         # doesn't exist, assign a random vector\n",
    "#         embedding_matrix[i] = np.random.randn(embedding_dim)\n",
    "         \n",
    "# >> SPACY EMBEDDINGS <<\n",
    "nlp = spacy.load('en_core_wen_lg')\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = nlp(word).vector\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:55.369495Z",
     "iopub.status.busy": "2021-02-04T08:31:55.368517Z",
     "iopub.status.idle": "2021-02-04T08:31:55.373575Z",
     "shell.execute_reply": "2021-02-04T08:31:55.372901Z"
    },
    "papermill": {
     "duration": 0.257331,
     "end_time": "2021-02-04T08:31:55.373729",
     "exception": false,
     "start_time": "2021-02-04T08:31:55.116398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-d2fcc07a8338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# NN MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# NN MODEL\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=sequence_length, trainable=True))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:55.857514Z",
     "iopub.status.busy": "2021-02-04T08:31:55.856855Z",
     "iopub.status.idle": "2021-02-04T08:31:55.861097Z",
     "shell.execute_reply": "2021-02-04T08:31:55.860536Z"
    },
    "papermill": {
     "duration": 0.253181,
     "end_time": "2021-02-04T08:31:55.861247",
     "exception": false,
     "start_time": "2021-02-04T08:31:55.608066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:56.338292Z",
     "iopub.status.busy": "2021-02-04T08:31:56.337534Z",
     "iopub.status.idle": "2021-02-04T08:31:56.353238Z",
     "shell.execute_reply": "2021-02-04T08:31:56.353784Z"
    },
    "papermill": {
     "duration": 0.259546,
     "end_time": "2021-02-04T08:31:56.354009",
     "exception": false,
     "start_time": "2021-02-04T08:31:56.094463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-4dc40414d7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=5000, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:31:56.829541Z",
     "iopub.status.busy": "2021-02-04T08:31:56.828888Z",
     "iopub.status.idle": "2021-02-04T08:31:56.847921Z",
     "shell.execute_reply": "2021-02-04T08:31:56.847324Z"
    },
    "papermill": {
     "duration": 0.257064,
     "end_time": "2021-02-04T08:31:56.848089",
     "exception": false,
     "start_time": "2021-02-04T08:31:56.591025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-d1bbc9a32d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mACC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mACC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "ACC = accuracy(test_df, X_test, Y_test, model)\n",
    "ACC = accuracy(unseen_df_labelled, X_unseen, Y_unseen, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.236601,
     "end_time": "2021-02-04T08:31:57.320188",
     "exception": false,
     "start_time": "2021-02-04T08:31:57.083587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were average and under-fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.233966,
     "end_time": "2021-02-04T08:31:57.791844",
     "exception": false,
     "start_time": "2021-02-04T08:31:57.557878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.237535,
     "end_time": "2021-02-04T08:31:58.264578",
     "exception": false,
     "start_time": "2021-02-04T08:31:58.027043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 9: ` Text using LSTM_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.2356,
     "end_time": "2021-02-04T08:31:58.737326",
     "exception": false,
     "start_time": "2021-02-04T08:31:58.501726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(Normal/Glove/Spacy embeddings) used for training a LSTM Classifier model\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues used for training a Dense NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.239037,
     "end_time": "2021-02-04T08:31:59.211994",
     "exception": false,
     "start_time": "2021-02-04T08:31:58.972957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were under-fitted and poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.237419,
     "end_time": "2021-02-04T08:31:59.686855",
     "exception": false,
     "start_time": "2021-02-04T08:31:59.449436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.235191,
     "end_time": "2021-02-04T08:32:00.159593",
     "exception": false,
     "start_time": "2021-02-04T08:31:59.924402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 10: ` Text using LSTM_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic + Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.234275,
     "end_time": "2021-02-04T08:32:00.630015",
     "exception": false,
     "start_time": "2021-02-04T08:32:00.395740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(Normal/Glove/Spacy embeddings) used for training a LSTM Classifier model\n",
    "    2. 2nd  Classifier: Text(CountVectorized) + Linguistic Featrues + Prob(0,1) via h-stack for training a Dense NN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.244155,
     "end_time": "2021-02-04T08:32:01.109743",
     "exception": false,
     "start_time": "2021-02-04T08:32:00.865588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were under-fitted and poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.233557,
     "end_time": "2021-02-04T08:32:01.581973",
     "exception": false,
     "start_time": "2021-02-04T08:32:01.348416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.236673,
     "end_time": "2021-02-04T08:32:02.052405",
     "exception": false,
     "start_time": "2021-02-04T08:32:01.815732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 11: ` Text  - Keras ELMO Embeddings  using LSTM_CLF [BASE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.233739,
     "end_time": "2021-02-04T08:32:02.524569",
     "exception": false,
     "start_time": "2021-02-04T08:32:02.290830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text in Keras's ELMO embeddings used for training a LSTM Classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.25319,
     "end_time": "2021-02-04T08:32:03.013489",
     "exception": false,
     "start_time": "2021-02-04T08:32:02.760299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results could not be determined as embedding creation took more than 2 days of training. Rejected the method mid-way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:03.515125Z",
     "iopub.status.busy": "2021-02-04T08:32:03.514091Z",
     "iopub.status.idle": "2021-02-04T08:32:03.517979Z",
     "shell.execute_reply": "2021-02-04T08:32:03.518474Z"
    },
    "papermill": {
     "duration": 0.25663,
     "end_time": "2021-02-04T08:32:03.518650",
     "exception": false,
     "start_time": "2021-02-04T08:32:03.262020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-119bba16961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading training, testing, unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading training, testing, unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:03.997574Z",
     "iopub.status.busy": "2021-02-04T08:32:03.996872Z",
     "iopub.status.idle": "2021-02-04T08:32:04.012707Z",
     "shell.execute_reply": "2021-02-04T08:32:04.013357Z"
    },
    "papermill": {
     "duration": 0.257603,
     "end_time": "2021-02-04T08:32:04.013540",
     "exception": false,
     "start_time": "2021-02-04T08:32:03.755937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-1f291f128acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ELMO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/elmo/2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "# ELMO\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:04.496579Z",
     "iopub.status.busy": "2021-02-04T08:32:04.495850Z",
     "iopub.status.idle": "2021-02-04T08:32:04.512466Z",
     "shell.execute_reply": "2021-02-04T08:32:04.512990Z"
    },
    "papermill": {
     "duration": 0.258703,
     "end_time": "2021-02-04T08:32:04.513183",
     "exception": false,
     "start_time": "2021-02-04T08:32:04.254480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elmo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-546ec5993fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Extract ELMo features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'elmo' is not defined"
     ]
    }
   ],
   "source": [
    "# just a random sentence\n",
    "x = [\"Roasted ants are a popular snack in Columbia\"]\n",
    "\n",
    "# Extract ELMo features \n",
    "embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:04.997082Z",
     "iopub.status.busy": "2021-02-04T08:32:04.996304Z",
     "iopub.status.idle": "2021-02-04T08:32:05.011531Z",
     "shell.execute_reply": "2021-02-04T08:32:05.012173Z"
    },
    "papermill": {
     "duration": 0.26176,
     "end_time": "2021-02-04T08:32:05.012364",
     "exception": false,
     "start_time": "2021-02-04T08:32:04.750604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-d9429f44d365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get elmo from tensorflow hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/elmo/2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ELMo Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mELMoEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "# get elmo from tensorflow hub\n",
    "embed = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "# ELMo Embedding\n",
    "def ELMoEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:05.494105Z",
     "iopub.status.busy": "2021-02-04T08:32:05.493328Z",
     "iopub.status.idle": "2021-02-04T08:32:05.515621Z",
     "shell.execute_reply": "2021-02-04T08:32:05.514899Z"
    },
    "papermill": {
     "duration": 0.264212,
     "end_time": "2021-02-04T08:32:05.515779",
     "exception": false,
     "start_time": "2021-02-04T08:32:05.251567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-891a71280e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInput_Numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Numeric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mInput_Text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Text layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayer_Embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mELMoEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_Text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "Input_Numeric = Input(shape=(X_train_num.values.shape[1],), name='Input_Numeric')\n",
    "Input_Text = Input(shape=(sequence_length,), name='Input_Text')\n",
    "\n",
    "# Text layer\n",
    "layer_Embedding = Lambda(ELMoEmbedding, output_shape=(1024,))(Input_Text)\n",
    "layer_LSTM = Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01)))(layer_Embedding)      \n",
    "layer_LSTM_norm = BatchNormalization()(layer_LSTM)\n",
    "\n",
    "# Num layer\n",
    "layer_Dense1 = Dense(128, activation='relu')(Input_Numeric)\n",
    "#layer_Dense2 = Dense(64, activation='relu')(layer_Dense1)\n",
    "\n",
    "x = concatenate([layer_LSTM_norm, layer_Dense2])\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(2, activation='softmax')(x)       \n",
    "\n",
    "model = Model(inputs=[Input_Text, Input_Numeric] , outputs=[x])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:06.012111Z",
     "iopub.status.busy": "2021-02-04T08:32:06.011152Z",
     "iopub.status.idle": "2021-02-04T08:32:06.015402Z",
     "shell.execute_reply": "2021-02-04T08:32:06.016027Z"
    },
    "papermill": {
     "duration": 0.262942,
     "end_time": "2021-02-04T08:32:06.016234",
     "exception": false,
     "start_time": "2021-02-04T08:32:05.753292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-8fd30e6eb14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "    history = model.fit([X_train_text, X_train_num], Y_train_final, \n",
    "                        batch_size=5000, epochs=5, verbose=1, class_weight=class_weights)\n",
    "    model.save_weights('Models/Response-elmo-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:06.500329Z",
     "iopub.status.busy": "2021-02-04T08:32:06.499343Z",
     "iopub.status.idle": "2021-02-04T08:32:06.516956Z",
     "shell.execute_reply": "2021-02-04T08:32:06.517503Z"
    },
    "papermill": {
     "duration": 0.260674,
     "end_time": "2021-02-04T08:32:06.517676",
     "exception": false,
     "start_time": "2021-02-04T08:32:06.257002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-c6501b0b8b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    \n",
    "    model.load_weights('Models/Response-elmo-model.h5')\n",
    "    predicts = model.predict([X_test_text, X_test_num], batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.239347,
     "end_time": "2021-02-04T08:32:06.994083",
     "exception": false,
     "start_time": "2021-02-04T08:32:06.754736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results could not be determined as embedding creation took more than 2 days of training. Rejected the method mid-way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.240491,
     "end_time": "2021-02-04T08:32:07.478498",
     "exception": false,
     "start_time": "2021-02-04T08:32:07.238007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.246919,
     "end_time": "2021-02-04T08:32:07.974803",
     "exception": false,
     "start_time": "2021-02-04T08:32:07.727884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 12: ` Linguistic features only using a Dense NN_CLF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.242816,
     "end_time": "2021-02-04T08:32:08.466349",
     "exception": false,
     "start_time": "2021-02-04T08:32:08.223533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Linguistic(Numerical) features used for training a Dense NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.243331,
     "end_time": "2021-02-04T08:32:08.950406",
     "exception": false,
     "start_time": "2021-02-04T08:32:08.707075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were under-fitted and poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.241629,
     "end_time": "2021-02-04T08:32:09.436613",
     "exception": false,
     "start_time": "2021-02-04T08:32:09.194984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.243656,
     "end_time": "2021-02-04T08:32:09.920798",
     "exception": false,
     "start_time": "2021-02-04T08:32:09.677142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 13: ` Linguistic using NN_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.246734,
     "end_time": "2021-02-04T08:32:10.413945",
     "exception": false,
     "start_time": "2021-02-04T08:32:10.167211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Linguistic(Numerical) features used for training a Dense NN Model\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Features used for training a Dense Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.294896,
     "end_time": "2021-02-04T08:32:10.959181",
     "exception": false,
     "start_time": "2021-02-04T08:32:10.664285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were under-fitted and poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.24824,
     "end_time": "2021-02-04T08:32:11.483408",
     "exception": false,
     "start_time": "2021-02-04T08:32:11.235168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.240907,
     "end_time": "2021-02-04T08:32:11.969589",
     "exception": false,
     "start_time": "2021-02-04T08:32:11.728682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 14: ` Linguistic using NN_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic + Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.251697,
     "end_time": "2021-02-04T08:32:12.463595",
     "exception": false,
     "start_time": "2021-02-04T08:32:12.211898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Linguistic(Numerical) features used for training a Dense NN Model\n",
    "    2. 2nd  Classifier: Text(CountVectorized) + Linguistic Features + Prob(0,1) via h-stack used for training a Dense Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.242207,
     "end_time": "2021-02-04T08:32:12.945339",
     "exception": false,
     "start_time": "2021-02-04T08:32:12.703132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were under-fitted and poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.287607,
     "end_time": "2021-02-04T08:32:13.472498",
     "exception": false,
     "start_time": "2021-02-04T08:32:13.184891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.252104,
     "end_time": "2021-02-04T08:32:13.965419",
     "exception": false,
     "start_time": "2021-02-04T08:32:13.713315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 15: ` Text + Linguistic via <ins>h-stack</ins> using NN_CLF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.243626,
     "end_time": "2021-02-04T08:32:14.480659",
     "exception": false,
     "start_time": "2021-02-04T08:32:14.237033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(CountVectorized) + Linguistic(Numerical) via h-stack used for training a Dense NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.243926,
     "end_time": "2021-02-04T08:32:14.966645",
     "exception": false,
     "start_time": "2021-02-04T08:32:14.722719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were over-fitted and poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:15.468057Z",
     "iopub.status.busy": "2021-02-04T08:32:15.462344Z",
     "iopub.status.idle": "2021-02-04T08:32:15.475054Z",
     "shell.execute_reply": "2021-02-04T08:32:15.474438Z"
    },
    "papermill": {
     "duration": 0.264937,
     "end_time": "2021-02-04T08:32:15.475206",
     "exception": false,
     "start_time": "2021-02-04T08:32:15.210269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-119bba16961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading training, testing, unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading training, testing, unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:15.986440Z",
     "iopub.status.busy": "2021-02-04T08:32:15.984701Z",
     "iopub.status.idle": "2021-02-04T08:32:15.992876Z",
     "shell.execute_reply": "2021-02-04T08:32:15.992145Z"
    },
    "papermill": {
     "duration": 0.276029,
     "end_time": "2021-02-04T08:32:15.993035",
     "exception": false,
     "start_time": "2021-02-04T08:32:15.717006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-769906829735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LABEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TEXT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mVect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# LABEL\n",
    "Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN\n",
    "\n",
    "# TEXT\n",
    "Vect = CountVectorizer()\n",
    "X_train_text = Vect.fit_transform(train_df.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "X_test_text = Vect.transform(test_df.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "X_unseen_text = Vect.transform(unseen_df_labelled.LINE_NER.apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# NUM\n",
    "X_train_NUM, X_test_NUM, X_unseen_NUM = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "\n",
    "# Normalize numerical features\n",
    "Normalize = MinMaxScaler()\n",
    "X_train_NUM[normalize_cols] =  Normalize.fit_transform(X_train_NUM[normalize_cols])\n",
    "X_test_NUM[normalize_cols] = Normalize.transform(X_test_NUM[normalize_cols])\n",
    "X_unseen_NUM[normalize_cols] = Normalize.transform(X_unseen_NUM[normalize_cols])\n",
    "\n",
    "# Combine Text + Num using h-stack\n",
    "X_train = hstack([X_train_text, X_train_NUM]).tocsr() \n",
    "X_test = hstack([X_test_text, X_test_NUM]).tocsr()\n",
    "X_unseen = hstack([X_unseen_text, X_unseen_NUM]).tocsr()\n",
    "\n",
    "# Sampling combined features\n",
    "Sampler = SMOTE('minority')\n",
    "X_train_final, Y_train_final = Sampler.fit_sample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:16.503238Z",
     "iopub.status.busy": "2021-02-04T08:32:16.502297Z",
     "iopub.status.idle": "2021-02-04T08:32:16.506986Z",
     "shell.execute_reply": "2021-02-04T08:32:16.506388Z"
    },
    "papermill": {
     "duration": 0.269873,
     "end_time": "2021-02-04T08:32:16.507144",
     "exception": false,
     "start_time": "2021-02-04T08:32:16.237271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-1b3ff85542df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Conversion to categorical values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_train_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "# Conversion to categorical values\n",
    "Y_train_final = to_categorical(Y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:17.013851Z",
     "iopub.status.busy": "2021-02-04T08:32:17.013072Z",
     "iopub.status.idle": "2021-02-04T08:32:17.019429Z",
     "shell.execute_reply": "2021-02-04T08:32:17.018565Z"
    },
    "papermill": {
     "duration": 0.268959,
     "end_time": "2021-02-04T08:32:17.019584",
     "exception": false,
     "start_time": "2021-02-04T08:32:16.750625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-ad31def25850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DENSE NN MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mInput_Numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlayer_Dense1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_Numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayer_Dense2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_Dense1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "# DENSE NN MODEL\n",
    "Input_Numeric = Input(shape=(X_train.toarray().shape[1],))\n",
    "\n",
    "layer_Dense1 = Dense(128, activation='relu')(Input_Numeric)\n",
    "layer_Dense2 = Dense(64, activation='relu')(layer_Dense1)\n",
    "layer_Dense3 = Dense(2, activation='softmax')(layer_Dense2)\n",
    "\n",
    "model = Model(inputs=Input_Numeric, outputs=layer_Dense3)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:17.524975Z",
     "iopub.status.busy": "2021-02-04T08:32:17.524276Z",
     "iopub.status.idle": "2021-02-04T08:32:17.528563Z",
     "shell.execute_reply": "2021-02-04T08:32:17.527938Z"
    },
    "papermill": {
     "duration": 0.263264,
     "end_time": "2021-02-04T08:32:17.528705",
     "exception": false,
     "start_time": "2021-02-04T08:32:17.265441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-2c1d5b81c49c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_final, Y_train_final, batch_size=256, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:18.042436Z",
     "iopub.status.busy": "2021-02-04T08:32:18.041712Z",
     "iopub.status.idle": "2021-02-04T08:32:18.046215Z",
     "shell.execute_reply": "2021-02-04T08:32:18.045626Z"
    },
    "papermill": {
     "duration": 0.26848,
     "end_time": "2021-02-04T08:32:18.046494",
     "exception": false,
     "start_time": "2021-02-04T08:32:17.778014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-e239ab70bdb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mACC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "ACC = accuracy(test_df, X_test, Y_test, model)\n",
    "unseen_df_labelled = accuracy(unseen_df_labelled, X_unseen, Y_unseen, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:18.544029Z",
     "iopub.status.busy": "2021-02-04T08:32:18.543359Z",
     "iopub.status.idle": "2021-02-04T08:32:18.560725Z",
     "shell.execute_reply": "2021-02-04T08:32:18.561498Z"
    },
    "papermill": {
     "duration": 0.264189,
     "end_time": "2021-02-04T08:32:18.561710",
     "exception": false,
     "start_time": "2021-02-04T08:32:18.297521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unseen_df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-f36522b517bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unseen_df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "predict_chunk_lines_PHASE1(unseen_df_labelled)[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.251486,
     "end_time": "2021-02-04T08:32:19.059130",
     "exception": false,
     "start_time": "2021-02-04T08:32:18.807644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.246897,
     "end_time": "2021-02-04T08:32:19.556125",
     "exception": false,
     "start_time": "2021-02-04T08:32:19.309228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 16: ` Text + Linguistic via h-stack using NN_CLF [BASE] + 2nd-level Ensemble_CLF(Features: Base(Probs) + Linguistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.249337,
     "end_time": "2021-02-04T08:32:20.051133",
     "exception": false,
     "start_time": "2021-02-04T08:32:19.801796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(CountVectorized) + Linguistic(numerical) via h-stack used for training a Dense NN Model\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Features + Prob(0,1) used for training a Dense Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.2454,
     "end_time": "2021-02-04T08:32:20.544408",
     "exception": false,
     "start_time": "2021-02-04T08:32:20.299008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results were under-fitted and poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.250612,
     "end_time": "2021-02-04T08:32:21.043456",
     "exception": false,
     "start_time": "2021-02-04T08:32:20.792844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.245713,
     "end_time": "2021-02-04T08:32:21.531287",
     "exception": false,
     "start_time": "2021-02-04T08:32:21.285574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 17: ` Text + Linguistic via 2-input Channel using LSTM_CLF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.244666,
     "end_time": "2021-02-04T08:32:22.024054",
     "exception": false,
     "start_time": "2021-02-04T08:32:21.779388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Text(df.LINE_NER) + Linguistic(numerical) in a 2-channel input bi-LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.245873,
     "end_time": "2021-02-04T08:32:22.515586",
     "exception": false,
     "start_time": "2021-02-04T08:32:22.269713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results are the best in terms of Recall and is used in final architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.244266,
     "end_time": "2021-02-04T08:32:23.005232",
     "exception": false,
     "start_time": "2021-02-04T08:32:22.760966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:23.562988Z",
     "iopub.status.busy": "2021-02-04T08:32:23.562282Z",
     "iopub.status.idle": "2021-02-04T08:32:23.565971Z",
     "shell.execute_reply": "2021-02-04T08:32:23.566504Z"
    },
    "papermill": {
     "duration": 0.267243,
     "end_time": "2021-02-04T08:32:23.566675",
     "exception": false,
     "start_time": "2021-02-04T08:32:23.299432",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-119bba16961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading training, testing, unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading training, testing, unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.25436,
     "end_time": "2021-02-04T08:32:24.067080",
     "exception": false,
     "start_time": "2021-02-04T08:32:23.812720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:24.586473Z",
     "iopub.status.busy": "2021-02-04T08:32:24.585513Z",
     "iopub.status.idle": "2021-02-04T08:32:24.589856Z",
     "shell.execute_reply": "2021-02-04T08:32:24.590378Z"
    },
    "papermill": {
     "duration": 0.274223,
     "end_time": "2021-02-04T08:32:24.590563",
     "exception": false,
     "start_time": "2021-02-04T08:32:24.316340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-9443d87a92f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TEXT (using df.LINE_NER --> a \"line\" in terms of context NER-GPE tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# NUM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# TEXT (using df.LINE_NER --> a \"line\" in terms of context NER-GPE tags)\n",
    "X_train_text, X_test_text, X_unseen_text = train_df.LINE_NER, test_df.LINE_NER, unseen_df_labelled.LINE_NER\n",
    "\n",
    "# NUM\n",
    "X_train_num, X_test_num, X_unseen_num = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "\n",
    "# LABELS\n",
    "Y_train, Y_test, Y_unseen = train_df.Y_SN, test_df.Y_SN, unseen_df_labelled.Y_SN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.295551,
     "end_time": "2021-02-04T08:32:25.132149",
     "exception": false,
     "start_time": "2021-02-04T08:32:24.836598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. Text layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:25.631865Z",
     "iopub.status.busy": "2021-02-04T08:32:25.630982Z",
     "iopub.status.idle": "2021-02-04T08:32:25.634006Z",
     "shell.execute_reply": "2021-02-04T08:32:25.633444Z"
    },
    "papermill": {
     "duration": 0.256173,
     "end_time": "2021-02-04T08:32:25.634216",
     "exception": false,
     "start_time": "2021-02-04T08:32:25.378043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NN Settings\n",
    "max_features = 5\n",
    "sequence_length = 6\n",
    "embedding_dim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:26.137273Z",
     "iopub.status.busy": "2021-02-04T08:32:26.136608Z",
     "iopub.status.idle": "2021-02-04T08:32:26.156926Z",
     "shell.execute_reply": "2021-02-04T08:32:26.156130Z"
    },
    "papermill": {
     "duration": 0.275514,
     "end_time": "2021-02-04T08:32:26.157083",
     "exception": false,
     "start_time": "2021-02-04T08:32:25.881569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-4fa0225dba21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unseen_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "X_train_text, X_test_text, X_unseen_text = tokenizer.texts_to_sequences(X_train_text), tokenizer.texts_to_sequences(X_test_text), tokenizer.texts_to_sequences(X_unseen_text)\n",
    "\n",
    "# Padding sequences (to make same len)\n",
    "X_train_text, X_test_text, X_unseen_text = pad_sequences(X_train_text, padding='post', maxlen=sequence_length), pad_sequences(X_test_text, padding='post', maxlen=sequence_length), pad_sequences(X_unseen_text, padding='post', maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:26.672314Z",
     "iopub.status.busy": "2021-02-04T08:32:26.671393Z",
     "iopub.status.idle": "2021-02-04T08:32:26.678006Z",
     "shell.execute_reply": "2021-02-04T08:32:26.677493Z"
    },
    "papermill": {
     "duration": 0.270699,
     "end_time": "2021-02-04T08:32:26.678149",
     "exception": false,
     "start_time": "2021-02-04T08:32:26.407450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-f548430a4ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unique Tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s unique tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Unique Tokens\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:27.187579Z",
     "iopub.status.busy": "2021-02-04T08:32:27.186939Z",
     "iopub.status.idle": "2021-02-04T08:32:27.192172Z",
     "shell.execute_reply": "2021-02-04T08:32:27.191482Z"
    },
    "papermill": {
     "duration": 0.268558,
     "end_time": "2021-02-04T08:32:27.192329",
     "exception": false,
     "start_time": "2021-02-04T08:32:26.923771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-7fe79217a4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unique Tokens + 1 for Embedding Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "# Unique Tokens + 1 for Embedding Layer\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:27.700195Z",
     "iopub.status.busy": "2021-02-04T08:32:27.694309Z",
     "iopub.status.idle": "2021-02-04T08:32:27.711979Z",
     "shell.execute_reply": "2021-02-04T08:32:27.711239Z"
    },
    "papermill": {
     "duration": 0.269237,
     "end_time": "2021-02-04T08:32:27.712132",
     "exception": false,
     "start_time": "2021-02-04T08:32:27.442895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-1b4406b70828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minority'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "sampler = SMOTE('minority')\n",
    "X_train_text, Y_train_final = sampler.fit_sample(X_train_text, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.250357,
     "end_time": "2021-02-04T08:32:28.212718",
     "exception": false,
     "start_time": "2021-02-04T08:32:27.962361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Numerical Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:28.716990Z",
     "iopub.status.busy": "2021-02-04T08:32:28.716238Z",
     "iopub.status.idle": "2021-02-04T08:32:28.732204Z",
     "shell.execute_reply": "2021-02-04T08:32:28.732794Z"
    },
    "papermill": {
     "duration": 0.272362,
     "end_time": "2021-02-04T08:32:28.732999",
     "exception": false,
     "start_time": "2021-02-04T08:32:28.460637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-78c6864ffb03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Normalize features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unseen_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_num' is not defined"
     ]
    }
   ],
   "source": [
    "# Normalize features\n",
    "N = MinMaxScaler()\n",
    "X_train_num[normalize_cols], X_test_num[normalize_cols], X_unseen_num[normalize_cols] = N.fit_transform(X_train_num[normalize_cols]), N.transform(X_test_num[normalize_cols]), N.transform(X_unseen_num[normalize_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:29.248154Z",
     "iopub.status.busy": "2021-02-04T08:32:29.247453Z",
     "iopub.status.idle": "2021-02-04T08:32:29.265027Z",
     "shell.execute_reply": "2021-02-04T08:32:29.265644Z"
    },
    "papermill": {
     "duration": 0.279717,
     "end_time": "2021-02-04T08:32:29.265844",
     "exception": false,
     "start_time": "2021-02-04T08:32:28.986127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-4c4cb558b67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minority'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_final_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "Sampler = SMOTE('minority')\n",
    "X_train_num, Y_train_final_2 = Sampler.fit_sample(X_train_num, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:29.767335Z",
     "iopub.status.busy": "2021-02-04T08:32:29.766632Z",
     "iopub.status.idle": "2021-02-04T08:32:29.784024Z",
     "shell.execute_reply": "2021-02-04T08:32:29.783427Z"
    },
    "papermill": {
     "duration": 0.268012,
     "end_time": "2021-02-04T08:32:29.784190",
     "exception": false,
     "start_time": "2021-02-04T08:32:29.516178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-7e6919ae7309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_train_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "# Training labels\n",
    "Y_train_final = to_categorical(Y_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.248744,
     "end_time": "2021-02-04T08:32:30.285223",
     "exception": false,
     "start_time": "2021-02-04T08:32:30.036479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:30.804497Z",
     "iopub.status.busy": "2021-02-04T08:32:30.802365Z",
     "iopub.status.idle": "2021-02-04T08:32:30.808386Z",
     "shell.execute_reply": "2021-02-04T08:32:30.808981Z"
    },
    "papermill": {
     "duration": 0.275715,
     "end_time": "2021-02-04T08:32:30.809156",
     "exception": false,
     "start_time": "2021-02-04T08:32:30.533441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-c00a4452126e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# finetuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mInput_Numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Numeric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mInput_Text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Text layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "# finetuning\n",
    "Input_Numeric = Input(shape=(X_train_num.values.shape[1],), name='Input_Numeric')\n",
    "Input_Text = Input(shape=(sequence_length,), name='Input_Text')\n",
    "\n",
    "# Text layer\n",
    "layer_Embedding = Embedding(num_words, embedding_dim, trainable=True)(Input_Text)\n",
    "layer_LSTM = Bidirectional(LSTM(32, dropout=0.25, recurrent_dropout=0.25, kernel_regularizer=regularizers.l2(0.01)))(layer_Embedding)\n",
    "\n",
    "# Num layer\n",
    "layer_Dense1 = Dense(256, activation='relu')(Input_Numeric)\n",
    "layer_Dropout1 = Dropout(0.25)(layer_Dense1)\n",
    "\n",
    "x = concatenate([layer_LSTM, layer_Dropout1])\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(2, activation='softmax')(x)       \n",
    "\n",
    "model = Model(inputs=[Input_Text, Input_Numeric] , outputs=[x])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:31.317248Z",
     "iopub.status.busy": "2021-02-04T08:32:31.316615Z",
     "iopub.status.idle": "2021-02-04T08:32:31.333114Z",
     "shell.execute_reply": "2021-02-04T08:32:31.333609Z"
    },
    "papermill": {
     "duration": 0.268949,
     "end_time": "2021-02-04T08:32:31.333786",
     "exception": false,
     "start_time": "2021-02-04T08:32:31.064837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "\n",
    "history = model.fit([X_train_text, X_train_num], Y_train_final, \n",
    "                    batch_size=256, epochs=30, verbose=1, class_weight=class_weights)\n",
    "\n",
    "modelNN = model\n",
    "\n",
    "ACC = accuracy(test_df, [X_test_text, X_test_num], Y_test, model)\n",
    "unseen_df_labelled = accuracy(unseen_df_labelled, [X_unseen_text, X_unseen_num], Y_unseen, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.253951,
     "end_time": "2021-02-04T08:32:31.840085",
     "exception": false,
     "start_time": "2021-02-04T08:32:31.586134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:32.354173Z",
     "iopub.status.busy": "2021-02-04T08:32:32.353451Z",
     "iopub.status.idle": "2021-02-04T08:32:32.369419Z",
     "shell.execute_reply": "2021-02-04T08:32:32.370034Z"
    },
    "papermill": {
     "duration": 0.274137,
     "end_time": "2021-02-04T08:32:32.370231",
     "exception": false,
     "start_time": "2021-02-04T08:32:32.096094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unseen_df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-e652623da4f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II: chunk identification module over above \"unseen_df_labelled\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unseen_df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II: chunk identification module over above \"unseen_df_labelled\"\n",
    "predict_chunk_lines_PHASE1(unseen_df_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:32.879308Z",
     "iopub.status.busy": "2021-02-04T08:32:32.878631Z",
     "iopub.status.idle": "2021-02-04T08:32:32.894662Z",
     "shell.execute_reply": "2021-02-04T08:32:32.895327Z"
    },
    "papermill": {
     "duration": 0.274482,
     "end_time": "2021-02-04T08:32:32.895518",
     "exception": false,
     "start_time": "2021-02-04T08:32:32.621036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ACC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-7dfaa60cbacc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II: chunk identification module over above \"new testing df \"ahdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mahdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_AH_testdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mahdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ACC' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II: chunk identification module over above \"new testing df \"ahdf\"\n",
    "ahdf = create_AH_testdf(ACC)\n",
    "predict_chunk_lines_PHASE1(ahdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.249591,
     "end_time": "2021-02-04T08:32:33.396392",
     "exception": false,
     "start_time": "2021-02-04T08:32:33.146801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `BEST NN METHOD USE-CASE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:33.906974Z",
     "iopub.status.busy": "2021-02-04T08:32:33.906268Z",
     "iopub.status.idle": "2021-02-04T08:32:33.910043Z",
     "shell.execute_reply": "2021-02-04T08:32:33.910558Z"
    },
    "papermill": {
     "duration": 0.260753,
     "end_time": "2021-02-04T08:32:33.910756",
     "exception": false,
     "start_time": "2021-02-04T08:32:33.650003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Saving \n",
    "\n",
    "# pickle.dump(modelNN, open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/ModelNN_21052020.pickle\", 'wb'))\n",
    "# pickle.dump(tokenizer, open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/Tokenizer_21052020.pickle\", 'wb'))\n",
    "# pickle.dump(N, open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/Normalizer_21052020.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:34.421844Z",
     "iopub.status.busy": "2021-02-04T08:32:34.421153Z",
     "iopub.status.idle": "2021-02-04T08:32:34.424837Z",
     "shell.execute_reply": "2021-02-04T08:32:34.425289Z"
    },
    "papermill": {
     "duration": 0.262256,
     "end_time": "2021-02-04T08:32:34.425478",
     "exception": false,
     "start_time": "2021-02-04T08:32:34.163222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Loading Best NN Model\n",
    "\n",
    "# modelNN = pickle.load(open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/ModelNN_21052020.pickle\", 'rb'))\n",
    "# Tokenizer = pickle.load(open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/Tokenizer_21052020.pickle\", 'rb'))\n",
    "# N = pickle.load(open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/Normalizer_21052020.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.297802,
     "end_time": "2021-02-04T08:32:34.977511",
     "exception": false,
     "start_time": "2021-02-04T08:32:34.679709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Experimentation on fine-tuning above network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.252511,
     "end_time": "2021-02-04T08:32:35.480193",
     "exception": false,
     "start_time": "2021-02-04T08:32:35.227682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. Finetuning - Alternative Model I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:36.022232Z",
     "iopub.status.busy": "2021-02-04T08:32:36.021329Z",
     "iopub.status.idle": "2021-02-04T08:32:36.044641Z",
     "shell.execute_reply": "2021-02-04T08:32:36.045395Z"
    },
    "papermill": {
     "duration": 0.314559,
     "end_time": "2021-02-04T08:32:36.045573",
     "exception": false,
     "start_time": "2021-02-04T08:32:35.731014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVED_test_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-aeacbee2d873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-4f68b7eec93b>\u001b[0m in \u001b[0;36munseendata_load\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mUNSEEN_DATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAVED_test_data_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loading unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtotal_saved_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSEEN_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAVED_test_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "unseen_df_labelled = unseendata_load()\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:36.554498Z",
     "iopub.status.busy": "2021-02-04T08:32:36.553517Z",
     "iopub.status.idle": "2021-02-04T08:32:36.573682Z",
     "shell.execute_reply": "2021-02-04T08:32:36.574250Z"
    },
    "papermill": {
     "duration": 0.275001,
     "end_time": "2021-02-04T08:32:36.574444",
     "exception": false,
     "start_time": "2021-02-04T08:32:36.299443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-c00a4452126e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# finetuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mInput_Numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Numeric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mInput_Text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Text layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "# finetuning\n",
    "Input_Numeric = Input(shape=(X_train_num.values.shape[1],), name='Input_Numeric')\n",
    "Input_Text = Input(shape=(sequence_length,), name='Input_Text')\n",
    "\n",
    "# Text layer\n",
    "layer_Embedding = Embedding(num_words, embedding_dim, trainable=True)(Input_Text)\n",
    "layer_LSTM = Bidirectional(LSTM(32, dropout=0.25, recurrent_dropout=0.25, kernel_regularizer=regularizers.l2(0.01)))(layer_Embedding)\n",
    "\n",
    "# Num layer\n",
    "layer_Dense1 = Dense(256, activation='relu')(Input_Numeric)\n",
    "layer_Dropout1 = Dropout(0.25)(layer_Dense1)\n",
    "\n",
    "x = concatenate([layer_LSTM, layer_Dropout1])\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(2, activation='softmax')(x)       \n",
    "\n",
    "model = Model(inputs=[Input_Text, Input_Numeric] , outputs=[x])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:37.080951Z",
     "iopub.status.busy": "2021-02-04T08:32:37.079921Z",
     "iopub.status.idle": "2021-02-04T08:32:37.096025Z",
     "shell.execute_reply": "2021-02-04T08:32:37.096509Z"
    },
    "papermill": {
     "duration": 0.272013,
     "end_time": "2021-02-04T08:32:37.096697",
     "exception": false,
     "start_time": "2021-02-04T08:32:36.824684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "\n",
    "history = model.fit([X_train_text, X_train_num], Y_train_final, \n",
    "                    batch_size=256, epochs=30, verbose=1, class_weight=class_weights)\n",
    "\n",
    "ACC = accuracy(test_df, [X_test_text, X_test_num], Y_test, model)\n",
    "unseen_df_labelled_1 = accuracy(unseen_df_labelled, [X_unseen_text, X_unseen_num], Y_unseen, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:37.618244Z",
     "iopub.status.busy": "2021-02-04T08:32:37.617536Z",
     "iopub.status.idle": "2021-02-04T08:32:37.621877Z",
     "shell.execute_reply": "2021-02-04T08:32:37.622456Z"
    },
    "papermill": {
     "duration": 0.273552,
     "end_time": "2021-02-04T08:32:37.622647",
     "exception": false,
     "start_time": "2021-02-04T08:32:37.349095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unseen_df_labelled_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-8277707dd622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unseen_df_labelled_1' is not defined"
     ]
    }
   ],
   "source": [
    "predict_chunk_lines_PHASE1(unseen_df_labelled_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.259127,
     "end_time": "2021-02-04T08:32:38.133842",
     "exception": false,
     "start_time": "2021-02-04T08:32:37.874715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Finetuning - Alternative Model II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:38.659773Z",
     "iopub.status.busy": "2021-02-04T08:32:38.658052Z",
     "iopub.status.idle": "2021-02-04T08:32:38.688058Z",
     "shell.execute_reply": "2021-02-04T08:32:38.687370Z"
    },
    "papermill": {
     "duration": 0.293677,
     "end_time": "2021-02-04T08:32:38.688253",
     "exception": false,
     "start_time": "2021-02-04T08:32:38.394576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVED_test_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-aeacbee2d873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-4f68b7eec93b>\u001b[0m in \u001b[0;36munseendata_load\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mUNSEEN_DATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAVED_test_data_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loading unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtotal_saved_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSEEN_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAVED_test_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "unseen_df_labelled = unseendata_load()\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:39.212834Z",
     "iopub.status.busy": "2021-02-04T08:32:39.212106Z",
     "iopub.status.idle": "2021-02-04T08:32:39.239405Z",
     "shell.execute_reply": "2021-02-04T08:32:39.238800Z"
    },
    "papermill": {
     "duration": 0.288971,
     "end_time": "2021-02-04T08:32:39.239565",
     "exception": false,
     "start_time": "2021-02-04T08:32:38.950594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-bb270e5d9ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInput_Numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Numeric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mInput_Text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input_Text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Text layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayer_Embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_Text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "Input_Numeric = Input(shape=(X_train_num.values.shape[1],), name='Input_Numeric')\n",
    "Input_Text = Input(shape=(sequence_length,), name='Input_Text')\n",
    "\n",
    "# Text layer\n",
    "layer_Embedding = Embedding(num_words, embedding_dim, trainable=True)(Input_Text)\n",
    "layer_Conv1D1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(layer_Embedding)\n",
    "layer_pool1 = MaxPooling1D(pool_size=2)(layer_Conv1D1)\n",
    "layer_LSTM = Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01)))(layer_pool1)\n",
    "\n",
    "# Num layer\n",
    "layer_Dense1 = Dense(128, activation='relu')(Input_Numeric)\n",
    "layer_Dense2 = Dense(64, activation='relu')(layer_Dense1)\n",
    "layer_Droput1 = Dropout(0.2)(layer_Dense2)\n",
    "\n",
    "x = concatenate([layer_LSTM, layer_Droput1])\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(2, activation='softmax')(x)       \n",
    "\n",
    "model = Model(inputs=[Input_Text, Input_Numeric] , outputs=[x])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:39.777290Z",
     "iopub.status.busy": "2021-02-04T08:32:39.776543Z",
     "iopub.status.idle": "2021-02-04T08:32:39.780036Z",
     "shell.execute_reply": "2021-02-04T08:32:39.780600Z"
    },
    "papermill": {
     "duration": 0.280335,
     "end_time": "2021-02-04T08:32:39.780782",
     "exception": false,
     "start_time": "2021-02-04T08:32:39.500447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "\n",
    "history = model.fit([X_train_text, X_train_num], Y_train_final, \n",
    "                    batch_size=256, epochs=30, verbose=1, class_weight=class_weights)\n",
    "\n",
    "ACC = accuracy(test_df, [X_test_text, X_test_num], Y_test, model)\n",
    "unseen_df_labelled_2 = accuracy(unseen_df_labelled, [X_unseen_text, X_unseen_num], Y_unseen, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:40.302268Z",
     "iopub.status.busy": "2021-02-04T08:32:40.301548Z",
     "iopub.status.idle": "2021-02-04T08:32:40.317689Z",
     "shell.execute_reply": "2021-02-04T08:32:40.318376Z"
    },
    "papermill": {
     "duration": 0.277558,
     "end_time": "2021-02-04T08:32:40.318582",
     "exception": false,
     "start_time": "2021-02-04T08:32:40.041024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unseen_df_labelled_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-d36eb57bb728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unseen_df_labelled_2' is not defined"
     ]
    }
   ],
   "source": [
    "predict_chunk_lines_PHASE1(unseen_df_labelled_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.257389,
     "end_time": "2021-02-04T08:32:40.830951",
     "exception": false,
     "start_time": "2021-02-04T08:32:40.573562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.258642,
     "end_time": "2021-02-04T08:32:41.355041",
     "exception": false,
     "start_time": "2021-02-04T08:32:41.096399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Different NN MODELS Finetuning ANALYSIS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": 0.25917,
     "end_time": "2021-02-04T08:32:41.875356",
     "exception": false,
     "start_time": "2021-02-04T08:32:41.616186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---------------------------------------------------------------------------\n",
    "# ANALYSIS\n",
    "filters 32 lstm 64 dense1(128)Drp  denseX(64) - 15 74\n",
    "filters 32 lstm 64 dense1(64)Drp  denseX(64) - reduced\n",
    "filters 32 lstm 64 dense1(128)Drp  denseX(128,64) - 15 75\n",
    "filters 32 lstm 64 dense1(128)Drp  denseX(256,64) - 15 68\n",
    "filters 32 lstm 64 dense1(128)...  denseX(256,64) - 15 66\n",
    "filters 32 lstm 64 dense1(128)dense2(64)Drp  denseX(256,64) - 16 68\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.267386,
     "end_time": "2021-02-04T08:32:42.404097",
     "exception": false,
     "start_time": "2021-02-04T08:32:42.136711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.260854,
     "end_time": "2021-02-04T08:32:42.925868",
     "exception": false,
     "start_time": "2021-02-04T08:32:42.665014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `EXPERIMENTAL METHOD 18: ` TEXT(NER -> Clustering -> OHE) + LINGUSITIC via h-stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.255983,
     "end_time": "2021-02-04T08:32:43.449960",
     "exception": false,
     "start_time": "2021-02-04T08:32:43.193977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Text Vector Representation in Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.303428,
     "end_time": "2021-02-04T08:32:44.010735",
     "exception": false,
     "start_time": "2021-02-04T08:32:43.707307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- This feature implemented usage of KMeans clustering algorithm to represent text clusters into a vectorized format. Each line was tokenized and then used for creating a vocabulary using Word2vec.\n",
    "\n",
    "\n",
    "- This vocabulary was fitted on a KMeans algorithm to predict cluster information for each word in a line. Using elbow method number of clusters was chosen as 5. \n",
    "\n",
    "\n",
    "- Cluster information for each token in a line is then padded and converted using OneHotEncoder to vector format.\n",
    "\n",
    "\n",
    "- For all new words that are not in present in word2vec vocab were assigned a default cluster like Cn\n",
    "\n",
    "...\n",
    "\n",
    "if word in word2vec.wv.vocab\n",
    "    # cal score -> KM.predict(score)\n",
    "\n",
    "else:\n",
    "    # SKIP WORD2VEC SOCRE and put directly in some cluster where ORG(i.e NAME) is present (lets suppose CN)\n",
    "    km_label = CN\n",
    "    \n",
    "...\n",
    "\n",
    "<ins>Description</ins>\n",
    "   \n",
    "    Line \t\t\t    =  \"ABC Ltd is located in India\"     \t\n",
    "    Line_tokenized\t   =  [âABCâ,  âLtdâ,  âisâ,  âlocatedâ,  âinâ,  âIndiaâ]\n",
    "    Line_word2vec\t\t=  [embed(âABCâ), embed(âLtdâ),  â¦,  âembed(âIndiaâ)]\n",
    "    Line_clustered\t   =  [c1,  c2,  c3,  c3,  c3,  c1]\n",
    "    Line_padded \t\t =  [1,   2,   3,   3,   3,   1,  -1,  -1,  -1,  â¦,  -1]\n",
    "    Line_OHE\t\t     =  [[0 1 0 0 0],  [0 0 1 0 0],  [0 0 0 1 0],  â¦,  [0 0 0 0 0]]\n",
    "    Line_Vector\t      =  [0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 â¦. 0 0 0 0 0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.261436,
     "end_time": "2021-02-04T08:32:44.532438",
     "exception": false,
     "start_time": "2021-02-04T08:32:44.271002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:45.068102Z",
     "iopub.status.busy": "2021-02-04T08:32:45.067418Z",
     "iopub.status.idle": "2021-02-04T08:32:45.083505Z",
     "shell.execute_reply": "2021-02-04T08:32:45.084257Z"
    },
    "papermill": {
     "duration": 0.28557,
     "end_time": "2021-02-04T08:32:45.084447",
     "exception": false,
     "start_time": "2021-02-04T08:32:44.798877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-eeffb5b259a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load sample of df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Load sample of df\n",
    "a = df_labelled[:10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:45.623688Z",
     "iopub.status.busy": "2021-02-04T08:32:45.604564Z",
     "iopub.status.idle": "2021-02-04T08:32:45.629407Z",
     "shell.execute_reply": "2021-02-04T08:32:45.628696Z"
    },
    "papermill": {
     "duration": 0.287924,
     "end_time": "2021-02-04T08:32:45.629560",
     "exception": false,
     "start_time": "2021-02-04T08:32:45.341636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-41150879dc0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Preprocess lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LINES_T'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "def Preprocess(doc):\n",
    "    doc = doc.strip()\n",
    "    doc = re.sub(r\"\\s+\", \" \", re.sub(\"[^A-z0-9\\r\\t\\n\\,\\.\\?\\!\\;\\:\\â\\'\\\"\\-\\_\\`\\@\\(\\)\\[\\]\\{\\}\\#]\", \" \", doc).strip())\n",
    "    doc = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9\\&\\-\\.\\,]\", \" \", doc)).strip()\n",
    "    doc = nlp(doc)\n",
    "    line = \" \".join([\"__\" + str(token.ent_type_) + \"__\" if token.ent_type_ == 'ORG' or token.ent_type_ == 'GPE' else token.text for token in doc])\n",
    "    line = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9\\&]\", \" \", line)).strip()\n",
    "    line = re.sub(r\"\\s+\", \" \", re.sub(r\"\\w+\\d+\\w+\", \"__D__\", line)).strip()\n",
    "    return word_tokenize(line.lower())\n",
    "\n",
    "# Preprocess lines\n",
    "a['LINES_T'] = a.LINES.apply(Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:46.165744Z",
     "iopub.status.busy": "2021-02-04T08:32:46.164802Z",
     "iopub.status.idle": "2021-02-04T08:32:46.171079Z",
     "shell.execute_reply": "2021-02-04T08:32:46.170325Z"
    },
    "papermill": {
     "duration": 0.28413,
     "end_time": "2021-02-04T08:32:46.171280",
     "exception": false,
     "start_time": "2021-02-04T08:32:45.887150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-28c8810cef47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.LINES_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:46.706770Z",
     "iopub.status.busy": "2021-02-04T08:32:46.705792Z",
     "iopub.status.idle": "2021-02-04T08:32:46.711232Z",
     "shell.execute_reply": "2021-02-04T08:32:46.710563Z"
    },
    "papermill": {
     "duration": 0.283242,
     "end_time": "2021-02-04T08:32:46.711509",
     "exception": false,
     "start_time": "2021-02-04T08:32:46.428267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-1178b278d4f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "a_train = a.LINES_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:47.288997Z",
     "iopub.status.busy": "2021-02-04T08:32:47.288299Z",
     "iopub.status.idle": "2021-02-04T08:32:47.305087Z",
     "shell.execute_reply": "2021-02-04T08:32:47.305652Z"
    },
    "papermill": {
     "duration": 0.319432,
     "end_time": "2021-02-04T08:32:47.305900",
     "exception": false,
     "start_time": "2021-02-04T08:32:46.986468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-90ee43328eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word2vec Vectorize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Word2vec Vectorize\n",
    "sentences = a_train.tolist()\n",
    "word2vec = Word2Vec(sentences, size=300, min_count=1)\n",
    "a_words = word2vec.wv[word2vec.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:47.834601Z",
     "iopub.status.busy": "2021-02-04T08:32:47.833867Z",
     "iopub.status.idle": "2021-02-04T08:32:47.837341Z",
     "shell.execute_reply": "2021-02-04T08:32:47.837805Z"
    },
    "papermill": {
     "duration": 0.267769,
     "end_time": "2021-02-04T08:32:47.838009",
     "exception": false,
     "start_time": "2021-02-04T08:32:47.570240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ELBOW - calculate distortion for a range of number of cluster\n",
    "# distortions = []\n",
    "# for i in range(1, 15):\n",
    "#     km = KMeans(n_clusters=i, init='random', n_init=30, max_iter=500, tol=1e-04, random_state=7, n_jobs=-1)\n",
    "#     km.fit(a_words)\n",
    "#     distortions.append(km.inertia_)\n",
    "# # plot\n",
    "# plt.plot(range(1, 15), distortions, marker='o')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:48.375284Z",
     "iopub.status.busy": "2021-02-04T08:32:48.374563Z",
     "iopub.status.idle": "2021-02-04T08:32:48.392410Z",
     "shell.execute_reply": "2021-02-04T08:32:48.392974Z"
    },
    "papermill": {
     "duration": 0.291515,
     "end_time": "2021-02-04T08:32:48.393166",
     "exception": false,
     "start_time": "2021-02-04T08:32:48.101651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-8be017fff812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# KMeans Clustering (n_clusters=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Display of clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "# KMeans Clustering (n_clusters=5)\n",
    "km = KMeans(n_clusters=5, init='random', n_init=30, max_iter=500, tol=1e-04, random_state=7, n_jobs=-1)\n",
    "km.fit(a_words)\n",
    "\n",
    "# Display of clusters\n",
    "y_km = km.predict(a_words)\n",
    "words = list(word2vec.wv.vocab)\n",
    "c0, c1, c2, c3, c4 = [], [], [], [], []\n",
    "for word, label in zip(words,y_km):   \n",
    "    if label == 0:\n",
    "        c0.append(word)\n",
    "    elif label == 1:\n",
    "        c1.append(word)\n",
    "    elif label == 2:\n",
    "        c2.append(word)\n",
    "    elif label == 3:\n",
    "        c3.append(word)\n",
    "    else:\n",
    "        c4.append(word)\n",
    "c0 = c0 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c0))))]\n",
    "c1 = c1 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c1))))]\n",
    "c2 = c2 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c2))))]\n",
    "c3 = c3 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c3))))]\n",
    "c4 = c4 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c4))))]\n",
    "cluster_df = pd.DataFrame({'C0_abbrv': c0, \"C1_units\": c1, \"C2_company\":c2, \"C3_Prepos\":c3, \"C4_Names\":c4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:48.915567Z",
     "iopub.status.busy": "2021-02-04T08:32:48.914720Z",
     "iopub.status.idle": "2021-02-04T08:32:48.930485Z",
     "shell.execute_reply": "2021-02-04T08:32:48.931026Z"
    },
    "papermill": {
     "duration": 0.277312,
     "end_time": "2021-02-04T08:32:48.931226",
     "exception": false,
     "start_time": "2021-02-04T08:32:48.653914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-9ec478e5721a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_df' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:49.472860Z",
     "iopub.status.busy": "2021-02-04T08:32:49.472127Z",
     "iopub.status.idle": "2021-02-04T08:32:49.484007Z",
     "shell.execute_reply": "2021-02-04T08:32:49.483299Z"
    },
    "papermill": {
     "duration": 0.288455,
     "end_time": "2021-02-04T08:32:49.484170",
     "exception": false,
     "start_time": "2021-02-04T08:32:49.195715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-01882d498815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# KMeans vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0ma_train_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kmeans Vector = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_train_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "def kmeans_transform(X):\n",
    "    doc_vector = []\n",
    "    for doc in X:  \n",
    "        word_vector_doc = []  \n",
    "        if doc != []:\n",
    "            for word in doc:    \n",
    "                if word in word2vec.wv:\n",
    "                    word_vector_doc.append(km.predict([word2vec.wv[word]])[0])\n",
    "                else:\n",
    "                    word_vector_doc.append(4)\n",
    "            doc_vector.append(word_vector_doc)\n",
    "        else:\n",
    "            doc_vector.append([])     \n",
    "    return doc_vector\n",
    "\n",
    "# KMeans vector\n",
    "a_train_vector = kmeans_transform(a_train.tolist())\n",
    "print(\"Kmeans Vector = \", a_train_vector[5])\n",
    "\n",
    "# Padding\n",
    "a_train_vector_padded = pad_sequences(a_train_vector, padding='post', value=-1, maxlen=10)\n",
    "print(\"\\nKmeans Vector Padded = \", a_train_vector_padded[5])\n",
    "\n",
    "# OneHotEncoding Kmeans padded vector\n",
    "d = {0:[1,0,0,0,0], 1:[0,1,0,0,0], 2:[0,0,1,0,0], 3:[0,0,0,1,0], 4:[0,0,0,0,1], -1:[0,0,0,0,0]}\n",
    "a_train_vector_padded_OHE = [[d[b] for b in i] for i in a_train_vector_padded]\n",
    "print(\"\\nOneHotEncoder - Kmeans Vector Padded: \\n\", a_train_vector_padded_OHE[5])\n",
    "\n",
    "# Final OHE Vector\n",
    "a_train_vector_padded_OHE_vector = np.array(list(map(lambda x: sum(x, []), a_train_vector_padded_OHE)))\n",
    "print(\"\\nOHE Kmeans Vector: \\n\", a_train_vector_padded_OHE_vector[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:50.023196Z",
     "iopub.status.busy": "2021-02-04T08:32:50.022464Z",
     "iopub.status.idle": "2021-02-04T08:32:50.028836Z",
     "shell.execute_reply": "2021-02-04T08:32:50.028166Z"
    },
    "papermill": {
     "duration": 0.283636,
     "end_time": "2021-02-04T08:32:50.028985",
     "exception": false,
     "start_time": "2021-02-04T08:32:49.745349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_train_vector_padded_OHE_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-5ffa03ab47d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Shape = padding(pad) x 1hotencoding(5clusters) = pad x 5 dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma_train_vector_padded_OHE_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a_train_vector_padded_OHE_vector' is not defined"
     ]
    }
   ],
   "source": [
    "# Shape = padding(pad) x 1hotencoding(5clusters) = pad x 5 dim\n",
    "a_train_vector_padded_OHE_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:50.590033Z",
     "iopub.status.busy": "2021-02-04T08:32:50.587063Z",
     "iopub.status.idle": "2021-02-04T08:32:50.595956Z",
     "shell.execute_reply": "2021-02-04T08:32:50.595358Z"
    },
    "papermill": {
     "duration": 0.307145,
     "end_time": "2021-02-04T08:32:50.596120",
     "exception": false,
     "start_time": "2021-02-04T08:32:50.288975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_train_vector_padded_OHE_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-d785712c6b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma_train_vector_padded_OHE_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a_train_vector_padded_OHE_vector' is not defined"
     ]
    }
   ],
   "source": [
    "a_train_vector_padded_OHE_vector[5:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.276536,
     "end_time": "2021-02-04T08:32:51.159453",
     "exception": false,
     "start_time": "2021-02-04T08:32:50.882917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Executing for df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:51.701544Z",
     "iopub.status.busy": "2021-02-04T08:32:51.700523Z",
     "iopub.status.idle": "2021-02-04T08:32:51.712982Z",
     "shell.execute_reply": "2021-02-04T08:32:51.712240Z"
    },
    "papermill": {
     "duration": 0.285472,
     "end_time": "2021-02-04T08:32:51.713137",
     "exception": false,
     "start_time": "2021-02-04T08:32:51.427665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-2303725b2c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Load df\n",
    "df_cluster = df_labelled[:10000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:52.260329Z",
     "iopub.status.busy": "2021-02-04T08:32:52.254133Z",
     "iopub.status.idle": "2021-02-04T08:32:52.264829Z",
     "shell.execute_reply": "2021-02-04T08:32:52.264140Z"
    },
    "papermill": {
     "duration": 0.287215,
     "end_time": "2021-02-04T08:32:52.264984",
     "exception": false,
     "start_time": "2021-02-04T08:32:51.977769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Preprocess(doc):\n",
    "    doc = doc.strip()\n",
    "    doc = re.sub(r\"\\s+\", \" \", re.sub(\"[^A-z0-9\\r\\t\\n\\,\\.\\?\\!\\;\\:\\â\\'\\\"\\-\\_\\`\\@\\(\\)\\[\\]\\{\\}\\#]\", \" \", doc).strip())\n",
    "    doc = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9\\&\\-\\.\\,]\", \" \", doc)).strip()\n",
    "    doc = nlp(doc)\n",
    "    line = \" \".join([\"__\" + str(token.ent_type_) + \"__\" if token.ent_type_ == 'ORG' or token.ent_type_ == 'GPE' else token.text for token in doc])\n",
    "    line = re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-z0-9\\&]\", \" \", line)).strip()\n",
    "    line = re.sub(r\"\\s+\", \" \", re.sub(r\"\\w+\\d+\\w+\", \"__D__\", line)).strip()\n",
    "    return word_tokenize(line.lower())\n",
    "\n",
    "# Preprocess lines\n",
    "df_cluster['LINES_T'] = df_cluster.LINES.apply(Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:52.814361Z",
     "iopub.status.busy": "2021-02-04T08:32:52.813612Z",
     "iopub.status.idle": "2021-02-04T08:32:52.819054Z",
     "shell.execute_reply": "2021-02-04T08:32:52.818377Z"
    },
    "papermill": {
     "duration": 0.288526,
     "end_time": "2021-02-04T08:32:52.819227",
     "exception": false,
     "start_time": "2021-02-04T08:32:52.530701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-78633df70c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training and Testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINES_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "# Training and Testing data\n",
    "train_df, test_df = train_test_split(df_cluster, shuffle=True, random_state=7, stratify=df_cluster.Y_SN)\n",
    "X_train, X_test = train_df.LINES_T, test_df.LINES_T\n",
    "Y_train, Y_test = train_df.Y_SN, test_df.Y_SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:53.400241Z",
     "iopub.status.busy": "2021-02-04T08:32:53.399527Z",
     "iopub.status.idle": "2021-02-04T08:32:53.417181Z",
     "shell.execute_reply": "2021-02-04T08:32:53.416343Z"
    },
    "papermill": {
     "duration": 0.335063,
     "end_time": "2021-02-04T08:32:53.417370",
     "exception": false,
     "start_time": "2021-02-04T08:32:53.082307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-60ca1950a737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word2vec Vectorize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Word2vec Vectorize\n",
    "sentences = X_train.tolist()\n",
    "word2vec = Word2Vec(sentences, size=300, min_count=1)\n",
    "X_train_words = word2vec.wv[word2vec.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:53.963231Z",
     "iopub.status.busy": "2021-02-04T08:32:53.962489Z",
     "iopub.status.idle": "2021-02-04T08:32:53.966058Z",
     "shell.execute_reply": "2021-02-04T08:32:53.966557Z"
    },
    "papermill": {
     "duration": 0.287138,
     "end_time": "2021-02-04T08:32:53.966737",
     "exception": false,
     "start_time": "2021-02-04T08:32:53.679599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Elbow Method\n",
    "distortions = []\n",
    "for i in range(1, 20):\n",
    "    km = KMeans(n_clusters=i, init='random', n_init=30, max_iter=500, tol=1e-04, random_state=7, n_jobs=-1)\n",
    "    km.fit(X_train_words)\n",
    "    distortions.append(km.inertia_)\n",
    "# plot\n",
    "plt.plot(range(1, 20), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.264071,
     "end_time": "2021-02-04T08:32:54.498634",
     "exception": false,
     "start_time": "2021-02-04T08:32:54.234563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- SELECTED: n_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:55.036421Z",
     "iopub.status.busy": "2021-02-04T08:32:55.035616Z",
     "iopub.status.idle": "2021-02-04T08:32:55.056625Z",
     "shell.execute_reply": "2021-02-04T08:32:55.057279Z"
    },
    "papermill": {
     "duration": 0.29098,
     "end_time": "2021-02-04T08:32:55.057495",
     "exception": false,
     "start_time": "2021-02-04T08:32:54.766515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# KMeans\n",
    "km = KMeans(n_clusters=5, init='random', n_init=30, max_iter=500, tol=1e-04, random_state=7, n_jobs=-1)\n",
    "km.fit(X_train_words)\n",
    "\n",
    "# Display of clusters...\n",
    "y_km = km.predict(X_train_words)\n",
    "words = list(word2vec.wv.vocab)\n",
    "c0, c1, c2, c3, c4 = [], [], [], [], []\n",
    "for word, label in zip(words,y_km):   \n",
    "    if label == 0:\n",
    "        c0.append(word)\n",
    "    elif label == 1:\n",
    "        c1.append(word)\n",
    "    elif label == 2:\n",
    "        c2.append(word)\n",
    "    elif label == 3:\n",
    "        c3.append(word)\n",
    "    else:\n",
    "        c4.append(word)\n",
    "        \n",
    "c0 = c0 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c0))))]\n",
    "c1 = c1 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c1))))]\n",
    "c2 = c2 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c2))))]\n",
    "c3 = c3 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c3))))]\n",
    "c4 = c4 + [np.nan for i in range(int(np.abs(max(len(c0), len(c1), len(c2), len(c3), len(c4)) - len(c4))))]\n",
    "cluster_df = pd.DataFrame({'C0_abbrv': c0, \"C1_units\": c1, \"C2_company\":c2, \"C3_Prepos\":c3, \"C4_Names\":c4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:55.589176Z",
     "iopub.status.busy": "2021-02-04T08:32:55.588549Z",
     "iopub.status.idle": "2021-02-04T08:32:55.603939Z",
     "shell.execute_reply": "2021-02-04T08:32:55.604469Z"
    },
    "papermill": {
     "duration": 0.281706,
     "end_time": "2021-02-04T08:32:55.604644",
     "exception": false,
     "start_time": "2021-02-04T08:32:55.322938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-2a1a46cbc707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_df' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.26577,
     "end_time": "2021-02-04T08:32:56.134870",
     "exception": false,
     "start_time": "2021-02-04T08:32:55.869100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:56.670859Z",
     "iopub.status.busy": "2021-02-04T08:32:56.670165Z",
     "iopub.status.idle": "2021-02-04T08:32:56.695547Z",
     "shell.execute_reply": "2021-02-04T08:32:56.696249Z"
    },
    "papermill": {
     "duration": 0.294685,
     "end_time": "2021-02-04T08:32:56.696445",
     "exception": false,
     "start_time": "2021-02-04T08:32:56.401760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-e96920258cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# KMeans vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_train_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kmeans Vector = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "def kmeans_transform(X):\n",
    "    doc_vector = []\n",
    "    for doc in X:  \n",
    "        word_vector_doc = []  \n",
    "        if doc != []:\n",
    "            for word in doc:    \n",
    "                if word in word2vec.wv:\n",
    "                    word_vector_doc.append(km.predict([word2vec.wv[word]])[0])\n",
    "                else:\n",
    "                    # Words not in word2vec vocab are assigned Cluster 4\n",
    "                    word_vector_doc.append(4)\n",
    "            doc_vector.append(word_vector_doc)\n",
    "        else:\n",
    "            doc_vector.append([])     \n",
    "    return doc_vector\n",
    "\n",
    "# KMeans vector\n",
    "X_train_vector, X_test_vector = kmeans_transform(X_train.tolist()), kmeans_transform(X_test.tolist())\n",
    "print(\"Kmeans Vector = \", X_train_vector[0])\n",
    "\n",
    "# Padding\n",
    "X_train_vector_padded, X_test_vector_padded = pad_sequences(X_train_vector, padding='post', value=-1, maxlen=30), pad_sequences(X_test_vector, padding='post', value=-1, maxlen=30)\n",
    "print(\"\\nKmeans Vector Padded = \", X_train_vector_padded[0])\n",
    "\n",
    "# OneHotEncoding Kmeans padded vector\n",
    "d = {0:[1,0,0,0,0], 1:[0,1,0,0,0], 2:[0,0,1,0,0], 3:[0,0,0,1,0], 4:[0,0,0,0,1], -1:[0,0,0,0,0]}\n",
    "X_train_padded_OHE, X_test_padded_OHE = [[d[b] for b in i] for i in X_train_vector_padded], [[d[b] for b in i] for i in X_test_vector_padded]\n",
    "print(\"\\nOneHotEncoder - Kmeans Vector Padded: \\n\", X_train_padded_OHE[0])\n",
    "\n",
    "# Final OHE Vector\n",
    "X_train_padded_OHE_vector, X_test_padded_OHE_vector = np.array(list(map(lambda x: sum(x, []), X_train_padded_OHE))), np.array(list(map(lambda x: sum(x, []), X_test_padded_OHE)))\n",
    "print(\"\\nOHE Kmeans Vector: \\n\", X_train_padded_OHE_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:57.248105Z",
     "iopub.status.busy": "2021-02-04T08:32:57.236695Z",
     "iopub.status.idle": "2021-02-04T08:32:57.252347Z",
     "shell.execute_reply": "2021-02-04T08:32:57.252893Z"
    },
    "papermill": {
     "duration": 0.285429,
     "end_time": "2021-02-04T08:32:57.253097",
     "exception": false,
     "start_time": "2021-02-04T08:32:56.967668",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_padded_OHE_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-0cc0531c7adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_padded_OHE_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_padded_OHE_vector' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_padded_OHE_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.266065,
     "end_time": "2021-02-04T08:32:57.785671",
     "exception": false,
     "start_time": "2021-02-04T08:32:57.519606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:58.354454Z",
     "iopub.status.busy": "2021-02-04T08:32:58.353453Z",
     "iopub.status.idle": "2021-02-04T08:32:58.369895Z",
     "shell.execute_reply": "2021-02-04T08:32:58.370417Z"
    },
    "papermill": {
     "duration": 0.286631,
     "end_time": "2021-02-04T08:32:58.370602",
     "exception": false,
     "start_time": "2021-02-04T08:32:58.083971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-51ee867f97dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minority'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_padded_OHE_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "Sampler = SMOTE('minority')\n",
    "X_train_final, Y_train_final = Sampler.fit_sample(X_train_padded_OHE_vector, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.270039,
     "end_time": "2021-02-04T08:32:58.907550",
     "exception": false,
     "start_time": "2021-02-04T08:32:58.637511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:32:59.448444Z",
     "iopub.status.busy": "2021-02-04T08:32:59.447442Z",
     "iopub.status.idle": "2021-02-04T08:32:59.463625Z",
     "shell.execute_reply": "2021-02-04T08:32:59.464116Z"
    },
    "papermill": {
     "duration": 0.288192,
     "end_time": "2021-02-04T08:32:59.464295",
     "exception": false,
     "start_time": "2021-02-04T08:32:59.176103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit\n",
    "model5 = RandomForestClassifier(n_jobs=-1)\n",
    "model5.fit(X_train_final, Y_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.263916,
     "end_time": "2021-02-04T08:32:59.997883",
     "exception": false,
     "start_time": "2021-02-04T08:32:59.733967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:00.537795Z",
     "iopub.status.busy": "2021-02-04T08:33:00.536798Z",
     "iopub.status.idle": "2021-02-04T08:33:00.562042Z",
     "shell.execute_reply": "2021-02-04T08:33:00.562552Z"
    },
    "papermill": {
     "duration": 0.296509,
     "end_time": "2021-02-04T08:33:00.562737",
     "exception": false,
     "start_time": "2021-02-04T08:33:00.266228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-db6da787d0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtesting\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y_SN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y_PRED'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_padded_OHE_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_TRUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_PRED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_PRED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "testing = pd.DataFrame()\n",
    "testing['Y_SN'] = Y_test\n",
    "testing['Y_PRED'] = model5.predict(X_test_padded_OHE_vector)\n",
    "Y_TRUE, Y_PRED = np.array(testing.Y_SN.astype(int)), np.array(testing.Y_PRED.astype(int))\n",
    "\n",
    "# Metrics\n",
    "confusion = confusion_matrix(Y_TRUE, Y_PRED)     \n",
    "TP, TN, FP, FN = confusion[1, 1], confusion[0, 0], confusion[0, 1], confusion[1, 0]\n",
    "clf_accuracy = round(float((TP + TN)/(TP + TN + FP + FN))*100.0, 3)\n",
    "P = round(float(TP /(TP + FP))*100.0, 3)\n",
    "R = round(float(TP /(TP + FN))*100.0, 3)\n",
    "f1_score = round(float((2 * P * R)/(P + R + 0.01)), 3)\n",
    "print(\"ACC = {}; P = {}, R = {}, F1 Score = {}\\n\".format(clf_accuracy, P, R, f1_score))\n",
    "# Scores\n",
    "print(\"Test_df valueCounts() :: \\n{}\\n\".format(testing.Y_SN.value_counts()))\n",
    "# TP\n",
    "print(\"TP = \", testing[(testing.Y_SN == 1) & (testing.Y_PRED == 1)].shape)\n",
    "# TN\n",
    "print(\"TN = \", testing[(testing.Y_SN == 0) & (testing.Y_PRED == 0)].shape)\n",
    "# False Positive\n",
    "print(\"FP = \", testing[(testing.Y_SN == 0) & (testing.Y_PRED == 1)].shape)\n",
    "# FN\n",
    "print(\"FN = \", testing[(testing.Y_SN == 1) & (testing.Y_PRED == 0)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.269496,
     "end_time": "2021-02-04T08:33:01.103058",
     "exception": false,
     "start_time": "2021-02-04T08:33:00.833562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Vectorization in this way helps, but exploring NER tags proved to be better than this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.264621,
     "end_time": "2021-02-04T08:33:01.633144",
     "exception": false,
     "start_time": "2021-02-04T08:33:01.368523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.290027,
     "end_time": "2021-02-04T08:33:02.192693",
     "exception": false,
     "start_time": "2021-02-04T08:33:01.902666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Method 19: ` Ensembling via Stacking - Linguistic using VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.271149,
     "end_time": "2021-02-04T08:33:02.730775",
     "exception": false,
     "start_time": "2021-02-04T08:33:02.459626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    ARCHITECTURE\n",
    "\n",
    "    1. BASE Classifier: Linguistic(numerical) for training a Voting Classifier Model (RF, XGB, AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.311308,
     "end_time": "2021-02-04T08:33:03.311653",
     "exception": false,
     "start_time": "2021-02-04T08:33:03.000345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Results are manageable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.266137,
     "end_time": "2021-02-04T08:33:03.846032",
     "exception": false,
     "start_time": "2021-02-04T08:33:03.579895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:04.398226Z",
     "iopub.status.busy": "2021-02-04T08:33:04.397191Z",
     "iopub.status.idle": "2021-02-04T08:33:04.401302Z",
     "shell.execute_reply": "2021-02-04T08:33:04.401802Z"
    },
    "papermill": {
     "duration": 0.286514,
     "end_time": "2021-02-04T08:33:04.402019",
     "exception": false,
     "start_time": "2021-02-04T08:33:04.115505",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-c733846629bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load training, testing, unseen df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Load training, testing, unseen df\n",
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.272476,
     "end_time": "2021-02-04T08:33:04.941873",
     "exception": false,
     "start_time": "2021-02-04T08:33:04.669397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Voting Classifier (XGB + RF 1 layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:05.501974Z",
     "iopub.status.busy": "2021-02-04T08:33:05.501250Z",
     "iopub.status.idle": "2021-02-04T08:33:05.505230Z",
     "shell.execute_reply": "2021-02-04T08:33:05.505899Z"
    },
    "papermill": {
     "duration": 0.292145,
     "end_time": "2021-02-04T08:33:05.506074",
     "exception": false,
     "start_time": "2021-02-04T08:33:05.213929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-bd74ca4a8027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ensemble of models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# ensemble of models \n",
    "estimator = [] \n",
    "estimator.append(('XGBoost',  XGBClassifier(random_state=1, n_jobs=-1))) \n",
    "estimator.append(('RandomForest', RandomForestClassifier(random_state=1, n_jobs=-1))) \n",
    "  \n",
    "# Voting Classifier with hard voting \n",
    "ensemble_VC = VotingClassifier(estimators=estimator, voting ='soft') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.270179,
     "end_time": "2021-02-04T08:33:06.044194",
     "exception": false,
     "start_time": "2021-02-04T08:33:05.774015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### `INPUT 1`: Input = Linguistic(numerical) features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:06.579929Z",
     "iopub.status.busy": "2021-02-04T08:33:06.579140Z",
     "iopub.status.idle": "2021-02-04T08:33:06.598620Z",
     "shell.execute_reply": "2021-02-04T08:33:06.599301Z"
    },
    "papermill": {
     "duration": 0.288294,
     "end_time": "2021-02-04T08:33:06.599493",
     "exception": false,
     "start_time": "2021-02-04T08:33:06.311199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-a6dd99ea7080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Selected features: Lingusitic(Numerical)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 2. Normalize features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Selected features: Lingusitic(Numerical)\n",
    "X_train, X_test, X_unseen = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "Y_train, Y_test, Y_unseen = train_df[Target], test_df[Target], unseen_df_labelled[Target]\n",
    "\n",
    "# 2. Normalize features\n",
    "Normalize = MinMaxScaler()\n",
    "X_train[normalize_cols] =  Normalize.fit_transform(X_train[normalize_cols])\n",
    "X_test[normalize_cols] = Normalize.transform(X_test[normalize_cols])\n",
    "X_unseen[normalize_cols] = Normalize.transform(X_unseen[normalize_cols])\n",
    "\n",
    "# 3. Sampling\n",
    "Sampler = SMOTE('minority')\n",
    "X_train_final, Y_train_final = Sampler.fit_sample(X_train, Y_train)\n",
    "\n",
    "# 4. Fitting model\n",
    "ensemble_VC = VotingClassifier(estimators=estimator, voting ='soft') \n",
    "ensemble_VC.fit(X_train_final, Y_train_final)\n",
    "\n",
    "# 5. Making predictions\n",
    "print(\"Test Data Results :: \")\n",
    "ACCURACY = accuracy(test_df, X_test, Y_test, ensemble_VC)\n",
    "print(\"Unseen Data Results :: \")\n",
    "UDL = accuracy(unseen_df_labelled, X_unseen, Y_unseen, ensemble_VC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:07.143902Z",
     "iopub.status.busy": "2021-02-04T08:33:07.143252Z",
     "iopub.status.idle": "2021-02-04T08:33:07.157390Z",
     "shell.execute_reply": "2021-02-04T08:33:07.157972Z"
    },
    "papermill": {
     "duration": 0.288085,
     "end_time": "2021-02-04T08:33:07.158148",
     "exception": false,
     "start_time": "2021-02-04T08:33:06.870063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UDL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-416263b1399e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II: chunk identification using above \"Unseen Df (UDL)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUDL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'UDL' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II: chunk identification using above \"Unseen Df (UDL)\"\n",
    "predict_chunk_lines_PHASE1(UDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:07.720366Z",
     "iopub.status.busy": "2021-02-04T08:33:07.719652Z",
     "iopub.status.idle": "2021-02-04T08:33:07.726674Z",
     "shell.execute_reply": "2021-02-04T08:33:07.727229Z"
    },
    "papermill": {
     "duration": 0.294469,
     "end_time": "2021-02-04T08:33:07.727422",
     "exception": false,
     "start_time": "2021-02-04T08:33:07.432953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ACCURACY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-b61d9cd5391a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running phase II: chunk identification using new testing data \"ahdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mahdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_AH_testdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACCURACY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredict_chunk_lines_PHASE1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mahdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ACCURACY' is not defined"
     ]
    }
   ],
   "source": [
    "# Running phase II: chunk identification using new testing data \"ahdf\"\n",
    "ahdf = create_AH_testdf(ACCURACY)\n",
    "predict_chunk_lines_PHASE1(ahdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.275302,
     "end_time": "2021-02-04T08:33:08.279686",
     "exception": false,
     "start_time": "2021-02-04T08:33:08.004384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### `INPUT 2: ` Input = Text(CountVectorized) + Linguistic(numerical) via <ins>h-stack</ins> for training above Voting CLF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.282597,
     "end_time": "2021-02-04T08:33:08.835021",
     "exception": false,
     "start_time": "2021-02-04T08:33:08.552424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Model is highly over-fitting and hence discarded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.290639,
     "end_time": "2021-02-04T08:33:09.464677",
     "exception": false,
     "start_time": "2021-02-04T08:33:09.174038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.282949,
     "end_time": "2021-02-04T08:33:10.030197",
     "exception": false,
     "start_time": "2021-02-04T08:33:09.747248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <ins>FINAL ARCHITECTURE</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.28402,
     "end_time": "2021-02-04T08:33:10.597934",
     "exception": false,
     "start_time": "2021-02-04T08:33:10.313914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Final Architecture consists of 2 separate models each giving probabilites of labels(P0, P1).\n",
    "\n",
    "`Model 1: Generic Model`\n",
    "\n",
    "2-layer RandomForest model\n",
    "   \n",
    "    ARCHITECTURE\n",
    "    \n",
    "    - Input  : df[Linguistic_Features]\n",
    "    \n",
    "    1. BASE Classifier: Linguistic Featrues(numerical) used for training a generic RandomForest Model.\n",
    "    2. 2nd  Classifier: Prob(0,1) + Linguistic Featrues(numerical) used for training another generic RF model.\n",
    "    \n",
    "*Taken from **Method** 1*\n",
    "\n",
    "...\n",
    "\n",
    "`Model 2: NN Model`\n",
    "\n",
    "2-input Channel biLSTM network\n",
    "   \n",
    "    ARCHITECTURE\n",
    "    \n",
    "    - 1st Input : Text (*vectorized in terms of neighbouring context NER tags `df.LINE_NER`*) in an Embedding layer\n",
    "    - 2nd Input : Linguistic features(numerical) in a dense layer \n",
    "    - Concat    : Together in a Dense layer to give softmax probability outputs.\n",
    "    \n",
    "    1. BASE Classifier: Text(df.LINE_NER) + Linguistic(numerical) in a 2-channel input bi-LSTM network.\n",
    "\n",
    "*Taken from **Method** 17*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.287908,
     "end_time": "2021-02-04T08:33:11.175359",
     "exception": false,
     "start_time": "2021-02-04T08:33:10.887451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:11.872879Z",
     "iopub.status.busy": "2021-02-04T08:33:11.871985Z",
     "iopub.status.idle": "2021-02-04T08:33:12.038707Z",
     "shell.execute_reply": "2021-02-04T08:33:12.037317Z"
    },
    "papermill": {
     "duration": 0.581107,
     "end_time": "2021-02-04T08:33:12.038906",
     "exception": false,
     "start_time": "2021-02-04T08:33:11.457799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-120>\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                                \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                                \u001b[0mmax_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                                include_children=include_children)\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mmem_usage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36m_func_exec\u001b[0;34m(stmt, ns)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# helper for magic_memit, just a function proxy for the exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;31m# statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# Load Generic Model...\n",
    "model = pickle.load(open(\"Models/RF_2Level_Generic_Num_19052020/Model_model51_19052020.pickle\", \"rb\")) \n",
    "model2 = pickle.load(open(\"Models/RF_2Level_Generic_Num_19052020/Model_model52_19052020.pickle\", \"rb\")) \n",
    "Normalizer = pickle.load(open(\"Models/RF_2Level_Generic_Num_19052020/Model_Normalize_19052020.pickle\", \"rb\"))\n",
    "\n",
    "# Load NN Model...\n",
    "modelNN = pickle.load(open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/ModelNN_21052020.pickle\", 'rb'))\n",
    "Tokenizer = pickle.load(open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/Tokenizer_21052020.pickle\", 'rb'))\n",
    "N = pickle.load(open(\"Models/NN_TextNum_2SepLayers_30epochs_21052020/Normalizer_21052020.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.286818,
     "end_time": "2021-02-04T08:33:12.612299",
     "exception": false,
     "start_time": "2021-02-04T08:33:12.325481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:13.241521Z",
     "iopub.status.busy": "2021-02-04T08:33:13.240777Z",
     "iopub.status.idle": "2021-02-04T08:33:13.243866Z",
     "shell.execute_reply": "2021-02-04T08:33:13.243150Z"
    },
    "papermill": {
     "duration": 0.342087,
     "end_time": "2021-02-04T08:33:13.244032",
     "exception": false,
     "start_time": "2021-02-04T08:33:12.901945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SELECTED FEATURES\n",
    "# - All features are taken except 3 (i.e. SpacyNERLine, SpacyNERAddress, CrfNERAddress)\n",
    "\n",
    "Features_NUM = ['l_', 't_', 'r_', 'b_', 'FS_', 'F1_CONTAINSDIGIT', 'F1_CONTAINSALLDIGIT', 'F1_CONTAINSEMAIL',\n",
    "                'F1_CONTAINSURL', 'F1_CONTAINSDATE', 'F1_CONTAINSPHONE', 'F1_emails', 'F1_urls', 'F3_abbrv',\n",
    "                'F4_DictWords', 'F4_Cap1DictWords', 'F4_CapDictWords', 'F4_NonDictWords', 'F4_Cap1NonDictWords',\n",
    "                'F4_CapNonDictWords', 'F5_postalAB', 'F6_lineQuadrant', 'F7_gpe']\n",
    "\n",
    "normalize_cols = ['F4_DictWords', 'F4_Cap1DictWords', 'F4_CapDictWords', 'F4_NonDictWords', 'F4_Cap1NonDictWords', \n",
    "                  'F4_CapNonDictWords', 'F5_postalAB', 'F7_gpe']\n",
    "\n",
    "Features_L2 = Features_NUM + ['L1_P0', 'L1_P1']\n",
    "\n",
    "Target = 'Y_SN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.285839,
     "end_time": "2021-02-04T08:33:13.812582",
     "exception": false,
     "start_time": "2021-02-04T08:33:13.526743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:14.369344Z",
     "iopub.status.busy": "2021-02-04T08:33:14.368393Z",
     "iopub.status.idle": "2021-02-04T08:33:14.383615Z",
     "shell.execute_reply": "2021-02-04T08:33:14.384195Z"
    },
    "papermill": {
     "duration": 0.294812,
     "end_time": "2021-02-04T08:33:14.384403",
     "exception": false,
     "start_time": "2021-02-04T08:33:14.089591",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-6144512607ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_SN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0munseen_df_labelled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseendata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df_labelled, shuffle=True, random_state=7, stratify=df_labelled.Y_SN)\n",
    "unseen_df_labelled = unseendata_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.28008,
     "end_time": "2021-02-04T08:33:14.938266",
     "exception": false,
     "start_time": "2021-02-04T08:33:14.658186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <ins>Phase I: Line Classification</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.274816,
     "end_time": "2021-02-04T08:33:15.488532",
     "exception": false,
     "start_time": "2021-02-04T08:33:15.213716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model 1: Generic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:16.045039Z",
     "iopub.status.busy": "2021-02-04T08:33:16.044140Z",
     "iopub.status.idle": "2021-02-04T08:33:16.067572Z",
     "shell.execute_reply": "2021-02-04T08:33:16.068221Z"
    },
    "papermill": {
     "duration": 0.303006,
     "end_time": "2021-02-04T08:33:16.068419",
     "exception": false,
     "start_time": "2021-02-04T08:33:15.765413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-c8b59add4063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LEVEL 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 1. Select features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# LEVEL 1\n",
    "# 1. Select features\n",
    "X_train, X_test, X_unseen = train_df[Features_NUM], test_df[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "Y_train, Y_test, Y_unseen = train_df[Target], test_df[Target], unseen_df_labelled[Target]\n",
    "\n",
    "# 2. Normalize selected features\n",
    "X_train[normalize_cols] =  Normalizer.transform(X_train[normalize_cols])\n",
    "X_test[normalize_cols] = Normalizer.transform(X_test[normalize_cols])\n",
    "X_unseen[normalize_cols] = Normalizer.transform(X_unseen[normalize_cols])\n",
    "\n",
    "# LEVEL 2\n",
    "train_df['L1_P0'], train_df['L1_P1'] = zip(*model.predict_proba(X_train))\n",
    "test_df['L1_P0'], test_df['L1_P1'] = zip(*model.predict_proba(X_test))\n",
    "unseen_df_labelled['L1_P0'], unseen_df_labelled['L1_P1'] = zip(*model.predict_proba(X_unseen))\n",
    "\n",
    "# 3. Select features: L2 [P(0,1) + Linguistic Features]\n",
    "X_train_L2, X_test_L2, X_unseen_L2 = train_df[Features_L2], test_df[Features_L2], unseen_df_labelled[Features_L2]\n",
    "\n",
    "# 4. Normalize above L2 features\n",
    "X_train_L2[normalize_cols] = Normalizer.transform(X_train_L2[normalize_cols])\n",
    "X_test_L2[normalize_cols] = Normalizer.transform(X_test_L2[normalize_cols])\n",
    "X_unseen_L2[normalize_cols] = Normalizer.transform(X_unseen_L2[normalize_cols])\n",
    "\n",
    "# 5. Make Predictions\n",
    "print(\"\\n ----- AmeriHealth Test Data -------\")\n",
    "test_df__ = accuracy(test_df.copy(), X_test_L2, Y_test, model2)\n",
    "unseen_AHdf_labelled = create_AH_testdf(test_df__)\n",
    "unseen_AHdf_labelled = unseen_AHdf_labelled.rename(columns={\"Y_PRED\": \"Model1_Y_PRED\", \"P0\": \"Model1_P0\", \"P1\": \"Model1_P1\"})\n",
    "\n",
    "print(\"\\n ------- Porsche Test Data -------\")\n",
    "unseen_df_labelled = accuracy(unseen_df_labelled.copy(), X_unseen_L2, Y_unseen, model2)\n",
    "unseen_df_labelled = unseen_df_labelled.rename(columns={\"Y_PRED\": \"Model1_Y_PRED\", \"P0\": \"Model1_P0\", \"P1\": \"Model1_P1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.275655,
     "end_time": "2021-02-04T08:33:16.616253",
     "exception": false,
     "start_time": "2021-02-04T08:33:16.340598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model 2: NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:17.172935Z",
     "iopub.status.busy": "2021-02-04T08:33:17.172192Z",
     "iopub.status.idle": "2021-02-04T08:33:17.196566Z",
     "shell.execute_reply": "2021-02-04T08:33:17.197340Z"
    },
    "papermill": {
     "duration": 0.30609,
     "end_time": "2021-02-04T08:33:17.197543",
     "exception": false,
     "start_time": "2021-02-04T08:33:16.891453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-6201ab1ce5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Text (df.LINE_NER)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_AHdf_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_NER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unseen_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unseen_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unseen_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# NN Settings\n",
    "max_features = 5\n",
    "sequence_length = 6\n",
    "embedding_dim = 6\n",
    "\n",
    "# Text (df.LINE_NER)\n",
    "X_train_text, X_test_text, X_unseen_text = train_df.LINE_NER, unseen_AHdf_labelled.LINE_NER, unseen_df_labelled.LINE_NER\n",
    "X_train_text, X_test_text, X_unseen_text = Tokenizer.texts_to_sequences(X_train_text), Tokenizer.texts_to_sequences(X_test_text), Tokenizer.texts_to_sequences(X_unseen_text)\n",
    "X_train_text, X_test_text, X_unseen_text = pad_sequences(X_train_text, padding='post', maxlen=sequence_length), pad_sequences(X_test_text, padding='post', maxlen=sequence_length), pad_sequences(X_unseen_text, padding='post', maxlen=sequence_length)\n",
    "\n",
    "# Num\n",
    "X_train_num, X_test_num, X_unseen_num = train_df[Features_NUM], unseen_AHdf_labelled[Features_NUM], unseen_df_labelled[Features_NUM]\n",
    "X_train_num[normalize_cols], X_test_num[normalize_cols], X_unseen_num[normalize_cols] = N.transform(X_train_num[normalize_cols]), N.transform(X_test_num[normalize_cols]), N.transform(X_unseen_num[normalize_cols])\n",
    "\n",
    "# Both as 2-input channel\n",
    "X_train_NN, X_test_NN, X_unseen_NN = [X_train_text, X_train_num], [X_test_text, X_test_num], [X_unseen_text, X_unseen_num]\n",
    "Y_train_NN, Y_test_NN, Y_unseen_NN = train_df.Y_SN, unseen_AHdf_labelled.Y_SN, unseen_df_labelled.Y_SN\n",
    "\n",
    "# Predictions\n",
    "print(\"\\n ----- AmeriHealth Test Data -------\")\n",
    "unseen_AHdf_labelled = accuracy(unseen_AHdf_labelled, X_test_NN, Y_test_NN, modelNN)\n",
    "unseen_AHdf_labelled = unseen_AHdf_labelled.rename(columns={\"Y_PRED\": \"Model2_Y_PRED\", \"P0\": \"Model2_P0\", \"P1\": \"Model2_P1\"})\n",
    "\n",
    "print(\"\\n ------- Porsche Test Data -------\")\n",
    "unseen_df_labelled = accuracy(unseen_df_labelled, X_unseen_NN, Y_unseen_NN, modelNN)\n",
    "unseen_df_labelled = unseen_df_labelled.rename(columns={\"Y_PRED\": \"Model2_Y_PRED\", \"P0\": \"Model2_P0\", \"P1\": \"Model2_P1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.281484,
     "end_time": "2021-02-04T08:33:17.756754",
     "exception": false,
     "start_time": "2021-02-04T08:33:17.475270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <ins>Phase II: Chunk Identification</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.276477,
     "end_time": "2021-02-04T08:33:18.314490",
     "exception": false,
     "start_time": "2021-02-04T08:33:18.038013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Sample of unseen data from Amerihealth customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:18.879760Z",
     "iopub.status.busy": "2021-02-04T08:33:18.879094Z",
     "iopub.status.idle": "2021-02-04T08:33:18.883580Z",
     "shell.execute_reply": "2021-02-04T08:33:18.884065Z"
    },
    "papermill": {
     "duration": 0.296299,
     "end_time": "2021-02-04T08:33:18.884249",
     "exception": false,
     "start_time": "2021-02-04T08:33:18.587950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_chunk_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-d5309a54fc8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_chunk_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_AHdf_labelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_chunk_lines' is not defined"
     ]
    }
   ],
   "source": [
    "predict_chunk_lines(unseen_AHdf_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.276223,
     "end_time": "2021-02-04T08:33:19.442805",
     "exception": false,
     "start_time": "2021-02-04T08:33:19.166582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Unseen data from Porsche golden dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:20.016696Z",
     "iopub.status.busy": "2021-02-04T08:33:20.015938Z",
     "iopub.status.idle": "2021-02-04T08:33:20.035669Z",
     "shell.execute_reply": "2021-02-04T08:33:20.036620Z"
    },
    "papermill": {
     "duration": 0.31897,
     "end_time": "2021-02-04T08:33:20.036934",
     "exception": false,
     "start_time": "2021-02-04T08:33:19.717964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_chunk_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-4ac81d3cd67f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_chunk_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_chunk_lines' is not defined"
     ]
    }
   ],
   "source": [
    "predict_chunk_lines(unseen_df_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.275348,
     "end_time": "2021-02-04T08:33:20.601648",
     "exception": false,
     "start_time": "2021-02-04T08:33:20.326300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.276882,
     "end_time": "2021-02-04T08:33:21.156684",
     "exception": false,
     "start_time": "2021-02-04T08:33:20.879802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <ins> `Phase II: Chunk Identification Module` </ins> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.273112,
     "end_time": "2021-02-04T08:33:21.705313",
     "exception": false,
     "start_time": "2021-02-04T08:33:21.432201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Chunk are lines or tokens in lines..\n",
    "- Using a scoring decision matrix, set of regex rules, set of static rules, keyword analysis, filteration, noise reduction, AddressBlock Line reduction to separate the final \"chunk\" from predicted SN Line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.273331,
     "end_time": "2021-02-04T08:33:22.256949",
     "exception": false,
     "start_time": "2021-02-04T08:33:21.983618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:22.825142Z",
     "iopub.status.busy": "2021-02-04T08:33:22.820018Z",
     "iopub.status.idle": "2021-02-04T08:33:22.829153Z",
     "shell.execute_reply": "2021-02-04T08:33:22.829803Z"
    },
    "papermill": {
     "duration": 0.300328,
     "end_time": "2021-02-04T08:33:22.830020",
     "exception": false,
     "start_time": "2021-02-04T08:33:22.529692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'SUPPLIER_NAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-84a77b938f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPLIER_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPLIER_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Chars  :: Min={}; Mean={}; Max={}; MODE={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Words :: Min={}; Mean={}; Max={}; MODE={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5459\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5460\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'SUPPLIER_NAME'"
     ]
    }
   ],
   "source": [
    "a = df.SUPPLIER_NAME.apply(lambda x: len(x))\n",
    "b = df.SUPPLIER_NAME.apply(lambda x: len(word_tokenize(x)))\n",
    "print(\"Number of Chars  :: Min={}; Mean={}; Max={}; MODE={}\".format(np.min(a), np.mean(a), np.max(a), a.value_counts().index[0]))\n",
    "print(\"Number of Words :: Min={}; Mean={}; Max={}; MODE={}\".format(np.min(b), np.mean(b), np.max(b), b.value_counts().index[0]))\n",
    "sns.distplot(a)\n",
    "sns.distplot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:23.433483Z",
     "iopub.status.busy": "2021-02-04T08:33:23.432673Z",
     "iopub.status.idle": "2021-02-04T08:33:23.453577Z",
     "shell.execute_reply": "2021-02-04T08:33:23.452884Z"
    },
    "papermill": {
     "duration": 0.345412,
     "end_time": "2021-02-04T08:33:23.453728",
     "exception": false,
     "start_time": "2021-02-04T08:33:23.108316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ahdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-2b2aab748a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mahdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPLIER_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mahdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPLIER_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Chars  :: Min={}; Mean={}; Max={}; MODE={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Words :: Min={}; Mean={}; Max={}; MODE={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ahdf' is not defined"
     ]
    }
   ],
   "source": [
    "a = ahdf.SUPPLIER_NAME.apply(lambda x: len(x))\n",
    "b = ahdf.SUPPLIER_NAME.apply(lambda x: len(word_tokenize(x)))\n",
    "print(\"Number of Chars  :: Min={}; Mean={}; Max={}; MODE={}\".format(np.min(a), np.mean(a), np.max(a), a.value_counts().index[0]))\n",
    "print(\"Number of Words :: Min={}; Mean={}; Max={}; MODE={}\".format(np.min(b), np.mean(b), np.max(b), b.value_counts().index[0]))\n",
    "sns.distplot(a)\n",
    "sns.distplot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:24.015974Z",
     "iopub.status.busy": "2021-02-04T08:33:24.014526Z",
     "iopub.status.idle": "2021-02-04T08:33:24.038657Z",
     "shell.execute_reply": "2021-02-04T08:33:24.037941Z"
    },
    "papermill": {
     "duration": 0.307717,
     "end_time": "2021-02-04T08:33:24.038832",
     "exception": false,
     "start_time": "2021-02-04T08:33:23.731115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'SUPPLIER_NAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-c8ee0cb295a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPLIER_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Stopwords  :: Min={}; Mean={}; Max={}; MODE={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5459\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5460\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'SUPPLIER_NAME'"
     ]
    }
   ],
   "source": [
    "a = df.SUPPLIER_NAME.apply(lambda x: len([w for w in word_tokenize(x) if w.lower() in stop_words])*100.0/len(word_tokenize(x)))\n",
    "print(\"Number of Stopwords  :: Min={}; Mean={}; Max={}; MODE={}\".format(np.min(a), np.mean(a), np.max(a), a.value_counts().index[0]))\n",
    "sns.distplot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:24.612090Z",
     "iopub.status.busy": "2021-02-04T08:33:24.611385Z",
     "iopub.status.idle": "2021-02-04T08:33:24.618188Z",
     "shell.execute_reply": "2021-02-04T08:33:24.618803Z"
    },
    "papermill": {
     "duration": 0.30044,
     "end_time": "2021-02-04T08:33:24.618999",
     "exception": false,
     "start_time": "2021-02-04T08:33:24.318559",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ahdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-72533013da1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mahdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPLIER_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Stopwords  :: Min={}; Mean={}; Max={}; MODE={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ahdf' is not defined"
     ]
    }
   ],
   "source": [
    "a = ahdf.SUPPLIER_NAME.apply(lambda x: len([w for w in word_tokenize(x) if w.lower() in stop_words])*100.0/len(word_tokenize(x)))\n",
    "print(\"Number of Stopwords  :: Min={}; Mean={}; Max={}; MODE={}\".format(np.min(a), np.mean(a), np.max(a), a.value_counts().index[0]))\n",
    "sns.distplot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.278746,
     "end_time": "2021-02-04T08:33:25.178602",
     "exception": false,
     "start_time": "2021-02-04T08:33:24.899856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Final Chunk Identification Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:25.736890Z",
     "iopub.status.busy": "2021-02-04T08:33:25.733725Z",
     "iopub.status.idle": "2021-02-04T08:33:25.847661Z",
     "shell.execute_reply": "2021-02-04T08:33:25.847057Z"
    },
    "papermill": {
     "duration": 0.394484,
     "end_time": "2021-02-04T08:33:25.847828",
     "exception": false,
     "start_time": "2021-02-04T08:33:25.453344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Performs chunk identification for each file (i.e. df)\n",
    "#####################################################################################\n",
    "# VERSION 4.0\n",
    "# Author: Pranjal Pathak\n",
    "# INPUT   --> df per file\n",
    "# OUTPUT  --> list of Supplier Name chunks, list of Customer Name chunks\n",
    "\n",
    "def chunk_identification(df):\n",
    "    \n",
    "    '''\n",
    "    STATIC TAGS DESCRIPTION\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    SUPPLIER_TAGS = [r\"\\bdirected\\s+inquiries\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirect\\s+inquiries\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bdirect\\s+inquiries\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirecting\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bdirected\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirects\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirect\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bcrediting\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcredited\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcredits\\s+to\\b\\s*[\\:|\\-]+\\s*\",\n",
    "                     r\"\\bcredit\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremiting\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremittance\\s+to\\b\\s*[\\:|\\-]+\\s*\",\n",
    "                     r\"\\bremitted\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremits\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremit\\s+to\\b\\s*[\\:|\\-]+\\s*\",\n",
    "                     r\"f/b/o\\s*\", r\"\\baccount\\s+name\\b\\s*[\\:|\\-]+\\s*\", r\"\\bchecked\\s+payable\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bchecks\\s+payable\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bchecks\\s+payable\\s+at\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bcheck\\s+payable\\s+at\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcheck\\s+payable\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bchecks\\s+pay\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcheck\\s+pay\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bchecking\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bchecked\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bpayable\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bchecks\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcheck\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bpays\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "                     r\"\\bpay\\s+to\\b\\s*[\\:|\\-]+\\s*\"]\n",
    "    \n",
    "    NEGATIVE_SUPPLIERWORDS = [\"attn\", \"from\", \"subject\", \"terms\", \"please\", \"invoice\", \"invoices\", \"invoiceno\", \n",
    "                              \"amount\", \"amounts\", \"address\", \"addresses\", \"addrss\", \"telephone\", \"phone\", \"phones\", \"call\", \n",
    "                              \"callat\", \"phoneat\", \"fax\", \"cancel\", \"notice\", \"confidential\",  \"client\"]               \n",
    "                           \n",
    "    CUSTOMER_TAGS = [r\"\\bbilling\\s+to\\b\\s*\\:*\\s*\", r\"\\bbilled\\s+to\\b\\s*\\:*\\s*\", r\"\\bbills\\s+to\\b\\s*\\:*\\s*\", \n",
    "                     r\"\\bbill\\s+to\\b\\s*\\:*\\s*\", r\"\\bbilling\\s+at\\b\\s*\\:*\\s*\", r\"\\bbilled\\s+at\\b\\s*\\:*\\s*\", \n",
    "                     r\"\\bbills\\s+at\\b\\s*\\:*\\s*\", r\"\\bbill\\s+at\\b\\s*\\:*\\s*\", r\"\\bshipping\\s+to\\b\\s*\\:*\\s*\", \n",
    "                     r\"\\bshipped\\s+to\\b\\s*\\:*\\s*\", r\"\\bships\\s+to\\b\\s*\\:*\\s*\", r\"\\bship\\s+to\\b\\s*\\:*\\s*\", \n",
    "                     r\"\\bshipto\\b\\s*\\:*\\s*\", r\"\\bshippingto\\b\\s*\\:*\\s*\", r\"\\bshipsto\\b\\s*\\:*\\s*\"]\n",
    "    \n",
    "    MIXED_TAGS = [r\"\\bbelonging\\s+to\\b\\s*\\:*\\s*\", r\"\\bbelonged\\s+to\\b\\s*\\:*\\s*\", r\"\\bbelongs\\s+to\\b\\s*\\:*\\s*\", \n",
    "                  r\"\\bbelong\\s+to\\b\\s*\\:*\\s*\", r\"\\bdivisions\\s+of\\b\\s*\\:*\\s*\", r\"\\bdivision\\s+of\\b\\s*\\:*\\s*\", \n",
    "                  r\"\\bdivision\\s+of\\b\\s*\\:*\\s*\", r\"\\bdivs\\s+of\\b\\s*\\:*\\s*\", r\"\\bdiv\\s+of\\b\\s*\\:*\\s*\", \n",
    "                  r\"\\bconnect\\s+with\\b\\s*\\:*\\s*\", r\"\\bcompany\\s+of\\b\\s*\\:*\\s*\", r\"\\bcomp\\s+of\\b\\s*\\:*\\s*\", \n",
    "                  r\"\\bfirm\\s+of\\b\\s*\\:*\\s*\", r\"\\bname\\s+of\\b\\s*\\:*\\s*\", r\"\\baccount\\s+of\\b\\s*\\:*\\s*\", \n",
    "                  r\"\\bacc\\s+of\\b\\s*\\:*\\s*\", r\"\\baccount\\s+name\\b\\s*\\:*\\s*\", r\"\\bacc\\s+name\\b\\s*\\:*\\s*\", \n",
    "                  r\"\\bis\\s+of\\b\\s*\\:*\\s*\", r\"\\bis\\s+to\\b\\s*\\:*\\s*\", \n",
    "                  r\"\\bto\\b\\s*\\:*\\s+\", \n",
    "                  r\"^by\\b\\s*\\:*\\s+\", r\"^mailing\\b\\s*\\:*\\s+\", r\"^to\\b\\s*\\:*\\s+\", r\"^of\\b\\s*\\:*\\s+\",\n",
    "                  r\"^mail\\b\\s*\\:*\\s+\", r\"^payable\\s+to\\b\\s*\\:*\\s+\", r\"^checks\\s+payable\\s+to\\b\\s*\\:*\\s+\",\n",
    "                  r\"^check\\s+payable\\s+to\\b\\s*\\:*\\s+\", r\"^checks\\s+to\\b\\s*\\:*\\s+\", r\"^check\\s+to\\b\\s*\\:*\\s+\"]\n",
    "        \n",
    "    NOISE_KEYWORDS = [r\"\\battn\\b[^\\:\\s]*\\:+\", r\"\\bfrom\\b\\s*\\:+\", r\"\\bsubject\\b\\s*\\:+\", r\"\\bcc\\b\\s*\\:+\", r\"\\bbcc\\b\\s*\\:+\", \n",
    "                      r\"\\bterms\\s+and\\s+condition\\b\", r\"\\bterms\\s+\\&\\s+conditions\\b\", r\"\\bterms\\&conditions\\b\", \n",
    "                      r\"\\btermsandconditions\\b\", r\"\\bT\\&C\\b\", r\"\\bTC\\b\", r\"\\bif\\s+you\\s+have\\s+any\\s+questions\\b\",\n",
    "                      r\"\\bif\\s+you\\s+have\\s+any\\s+question\\b\", r\"\\byou\\s+have\\s+any\\s+questions\\b\", \n",
    "                      r\"\\byou\\s+have\\s+any\\s+question\\b\", r\"\\byou\\s+have\\s+questions\\b\", r\"\\byou\\s+have\\s+question\\b\",\n",
    "                      r\"\\bif\\s+you\\s+have\\s+questions\", r\"\\bplease\\s+write\\s+to\\b\\[^\\:\\s]*\\:+\", \n",
    "                      r\"\\bplease\\s+write\\s+at\\b\\[^\\:\\s]*\\:+\", r\"\\bwrite\\s+to\\b\\[^\\:\\s]*\\:+\", \n",
    "                      r\"\\bcontact\\s+to\\b[^\\:\\s]*\\:+\", r\"\\bemail\\s+to\\b[^\\:\\s]*\\:+\", r\"\\bemail\\s+at\\b[^\\:\\s]*\\:+\", \n",
    "                      r\"\\bemail\\b\\s+\\@\\s*\\:+\", \"\\bconfidential\\b\"]\n",
    "\n",
    "    PHONE_TAGS = [r\"\\bphone\\b\\s*\\:*\\s+\", r\"\\bphone\\s+at\\b\\s*\\:*\\s+\", r\"\\bphones\\b\\s*\\:*\\s+\", r\"\\bphone\\s+on\\b\\s*\\:*\\s+\", \n",
    "                  r\"\\bcall\\b\\s*\\:*\\s+\", r\"\\bcalling\\b\\s*\\:*\\s+\", r\"\\bcall\\s+at\\b\\s*\\:*\\s+\", r\"\\bcalls\\s+at\\b\\s*\\:*\\s+\", \n",
    "                  r\"\\btelephone\\b\\s*\\:*\\s+\", r\"\\bt\\b[^\\:\\s]*\\:+\\s+\", r\"\\btelephone\\s+at\\b\\s*\\:*\\s+\", \n",
    "                  r\"\\btelephones\\b\\s*\\:*\\s+\", r\"\\bdial\\b\\s*\\:*\\s+\", r\"\\bdial\\s+at\\b\\s*\\:*\\s+\", r\"\\bdate\\b\\s*\\:*\\s+\", \n",
    "                  r\"\\bday\\b\\s*\\:*\\s+\", r\"\\btime\\b\\s*\\:*\\s+\", r\"\\bcalendar\\b\\s*\\:*\\s+\", r\"\\bfax\\b\\s*\\:*\\s+\"]\n",
    "                              \n",
    "    # runs on each prediction...\n",
    "    SMART_STOPWORDS = [\"https\", \"http\", \"please\", \"pls\", \"lets\", \"know\", \"knowing\", \"knows\", \n",
    "                       \"respond\", \"responding\", \"responds\", \"naming\", \"names\", \"thanks\", \"thank\", \"thankyou\", \n",
    "                       \"thanksyou\", \"thanking\", \"thankingyou\", \"thanx\", \"thnx\", \"thx\", \"asap\", \n",
    "                       \"dear\", \"hello\", \"kindly\", \"regards\", \"hi\", \"hola\", \"hey\", \"yeah\", \"nope\", \n",
    "                       \"advise\", \"attached\", \"days\", \"months\", \"month\", \"invoice\", \"invoices\", \"invocing\", \n",
    "                       \"ordernumber\", \"invoiceno\", \"accountno\", \"remit\", \"remittance\", \"fbo\", \"inquiries\", \n",
    "                       \"payable\", \"billed\", \"billing\", \"bills\", \"shipped\", \"shipping\", \"ships\", \"shipto\", \"shippingto\", \n",
    "                       \"shipsto\", \"terms&conditions\", \"termsandconditions\", \"confidential\", \"t&c\",\"emailed\", \"customerno\", \n",
    "                       \"duration\", \"since\", \"address\", \"order\", \"make\", \"check\", \"checks\", \"submit\", \"submits\", \"sumitting\" , \n",
    "                       \"submitto\",  \"paying\", \"paymentsto\", \"notices\", \"phone\", \"phones\", \"phoning\", \"phono\", \"emailo\", \n",
    "                       \"cancel\", \"cancelling\", \"cancellation\", \"cancellations\", \"requires\", \"require\", \"required\", \n",
    "                       \"requiring\", \"noticeable\", \"noticing\", \"date\", \"dates\", \"months\", \"month\", \"day\", \"days\",\n",
    "                       \"addresses\", \"address\", \"inovicenumber\",\n",
    "                       \"if\", \"because\", \"is\", \"there\", \"always\", 'itself', 'shouldn', 'there', 'were', 'should', \n",
    "                       'who', 'hasn', \"you've\", 'he', \"mustn't\", 'whom', 'mustn', 'very', 'doesn', 'have', 'here', \n",
    "                       'those', 'wasn', 'having', \"aren't\", 'yourselves', \"hasn't\", 'themselves', 'until', \"you'd\", \n",
    "                       'shan', 'him', 'ourselves', 'aren', \"isn't\", \"haven't\", 'that', 'herself', \"hadn't\", 'both',\n",
    "                       'where', 'most', 'doing', 'further', 'any', 'didn', 'theirs', \"weren't\", 'same', 'you', 'hadn', \n",
    "                       'myself', 'yours', 'won', 'mightn', 'she', 'hers', 'weren', \"she's\", 'does', 'during', 'was', \n",
    "                       'wouldn', 'because', \"won't\", 'again', 'his', \"you'll\", 'once', 'between', 'couldn', 'how', \n",
    "                       'what', \"shouldn't\", 'then', 'own', 'above', \"doesn't\", 'had', \"wasn't\", 'them', 'when', 'such', \n",
    "                       'while', 'if', 'did', 'before', \"couldn't\", 'your', \"shan't\", 'other', 'which', 'himself', 'through', \n",
    "                       'below', 'under', 'too', \"mightn't\", 'been', \"should've\", 'few', 'these', \"you're\", 'their', \n",
    "                       'can', 'each', \"it's\", 'has', \"needn't\", 'but', \"wouldn't\", 'needn', 'they', \"didn't\", 'nor', \n",
    "                       \"that'll\", \"don't\", 'than', 'some']\n",
    "    \n",
    "    # POSTAL-TAG LABELS\n",
    "    S_LABELS = ['house_number', 'road', 'unit', 'level', 'po_box']\n",
    "    M_LABELS = ['suburb', 'city_district', 'city', 'island', 'state_district', 'state', 'country_region', 'country', 'world_region']\n",
    "    E_LABELS = ['postcode']\n",
    "    ''' ------------------------------------------------------------------------------------------------------------------'''\n",
    "    \n",
    "    \n",
    "    # ################################################################################################################## #\n",
    "    # ------------------------------------------ MODULES :: file level (FL) -------------------------------------------- #\n",
    "    # ################################################################################################################## #\n",
    "    \n",
    "    \"\"\" MODULE FL 1:> REDUCE NER ENTITIES AND AB IN PREDICTION LINES\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Replaces NER(PHONE, FAX, DATE) tags with 'whitespace' and reduces(or slices) \"AB LINES\" or potential \"AB LINES\".  \"\"\"\n",
    "    def check_entities(df):\n",
    "        \n",
    "        # Reducing line-by-line\n",
    "        def reduce_entities(x):\n",
    "            \"\"\"\n",
    "            Takes a line and replaces NER tags and tries to reduce \"AB LINE\".\n",
    "            :param: df.LINES\n",
    "            :returns: Reduced LINE or np.nan\n",
    "            \"\"\"\n",
    "            \n",
    "            # MODULE 1: Check if a line could be \"AB\"\n",
    "            def check_AB(line):\n",
    "                \"\"\"\n",
    "                Takes a line and finds if it could be an \"AB LINE\" or not. Returns True if found, else False.\n",
    "                :param: string line: A line\n",
    "                :returns: bool True or False\n",
    "                \"\"\"\n",
    "                \n",
    "                def checkPattern(string, pattern): \n",
    "                    if all([\"S\" in string, \"M\" in string, \"E\" in string]) == False:\n",
    "                        return False\n",
    "                    l = len(pattern) \n",
    "                    if len(string) < l: return False\n",
    "                    for i in range(l - 1): \n",
    "                        x, y = pattern[i], pattern[i + 1] \n",
    "                        last = string.rindex(x) \n",
    "                        first = string.index(y) \n",
    "                        if last == -1 or first == -1 or last > first: return False\n",
    "                    return True\n",
    "\n",
    "                # create a dic that maps S_LABELS to \"S\", M_LABELS to \"M\", E_LABELS to \"E\"\n",
    "                label_mapper = {**dict(zip(S_LABELS, [\"S\"]*len(S_LABELS))), **dict(zip(M_LABELS, [\"M\"]*len(M_LABELS))), **dict(zip(E_LABELS, [\"E\"]*len(E_LABELS)))}\n",
    "\n",
    "                # Remove NER(PHONE, FAX, DATE) tags from line, if present\n",
    "                line = line.strip()\n",
    "                find_PhoneDateRE = sum([x for x in [[i for x in find_phone(line) for i in x if i!= ''], find_date(line)] if any(x)], [])\n",
    "                if len(find_PhoneDateRE) > 0:\n",
    "                    find_PhoneDateRE += PHONE_TAGS\n",
    "                    line = re.sub(r\"\\((?=\\d+\\))|(?<=\\d)\\)\", \"\", line, flags=re.IGNORECASE).strip()\n",
    "                    line = re.sub(r\"\\s+\", \" \", re.sub(\"|\".join(find_PhoneDateRE), \" \", line, flags=re.IGNORECASE)).strip()\n",
    "\n",
    "                # Run \"libpostal\" on line to generate POSTAL-TAGS\n",
    "                line_AB = crf_NER(line)\n",
    "\n",
    "                # Use the above dict(label_mapper) to create a LABEL-pattern for line\n",
    "                tagged_line = [(tag[0], label_mapper[tag[1]]) if tag[1] in label_mapper.keys() else (tag[0], 'O') for tag in line_AB]\n",
    "                \n",
    "                # If LABEL-pattern follows \"AB\" rule return True, else False\n",
    "                tagged_line_pattern = checkPattern(\"\".join([tag[1] for tag in tagged_line]), 'SME')\n",
    "                return tagged_line_pattern\n",
    "\n",
    "\n",
    "            # MODULE 2: Reduce a line if its \"AB LINE\"\n",
    "            def check_softAB(line):\n",
    "                \"\"\"\n",
    "                Takes a line if it contains atleast 1 SpacyNER(GPE) tag and re-checks if the line could be a \"AB LINE\"\n",
    "                in a relaxed-tone. If qualified, slices the line before a comma(',').\n",
    "                :param: string line: A line which contains at-least one SpacyNER(GPE) tag\n",
    "                :returns: string line: Reduced line\n",
    "                \"\"\"\n",
    "                \n",
    "                original_line = line\n",
    "\n",
    "                # Runs \"libpostal\" on line to generate POSTAL-TAGS\n",
    "                line_AB = crf_NER(line)\n",
    "\n",
    "                # List of POSTAL-TAGS\n",
    "                line_AB_tags = [tag[1] for tag in line_AB]\n",
    "\n",
    "                # find all commas(\",\") in line\n",
    "                line_commas = len(re.findall(r\"\\,\", line))\n",
    "\n",
    "                # If POSTAL-TAG contains: City + State + Zipcode + >=3 commas ---> It's a \"AB LINE\" (relaxed-tone)\n",
    "                if all([\"city\" in line_AB_tags, \"state\" in line_AB_tags, \"postcode\" in line_AB_tags, line_commas >= 3]):\n",
    "                    line = line.strip().split(',')[0]\n",
    "                    # print(\"SOFT AB == \", original_line, \"; Transformed ==\", line)\n",
    "                return line\n",
    "        \n",
    "            # df.LINES\n",
    "            line = x.LINES\n",
    "            \n",
    "            # 1). Repalce NER(PHONE, FAX, DATE) with 'whitespace'\n",
    "            check_PhoneDate = sum([x for x in [[i for x in find_phone(line) for i in x if i!= ''], find_date(line)] if any(x)], [])\n",
    "            if len(check_PhoneDate) > 0:\n",
    "                line = re.sub(r\"\\((?=\\d+\\))|(?<=\\d)\\)\", \" \", line, flags=re.IGNORECASE)\n",
    "                line = re.sub(r\"\\s+\", \" \", re.sub(\"|\".join(check_PhoneDate), \" \", line, flags=re.IGNORECASE)).strip()\n",
    "            \n",
    "            # 2). Reduce \"AB LINE\"\n",
    "            # MODULE 1: Check if a line could be \"AB\"\n",
    "            if x.POSTAL_AB == 'AB' or check_AB(line):\n",
    "                # crf_NER\n",
    "                tagged_line = [tag[0] for tag in crf_NER(line) if tag[1] =='house']\n",
    "                tagged_line = tagged_line[0] if len(tagged_line) > 0 else line\n",
    "                # spacy\n",
    "                chunkNER = [ent.orth_ for ent in nlp(line).ents if ent.label_ in ['ORG', 'FAC']]\n",
    "                chunkNER = chunkNER[0] if chunkNER != [] else tagged_line\n",
    "                line = chunkNER.strip()\n",
    "                # print(\"HARD AB == \", x.LINES, \"; Transformed == \", line)\n",
    "            # MODULE 2: Reduce a line if its \"AB LINE\"\n",
    "            elif len([ent.label_ for ent in nlp(line).ents if ent.label_=='GPE']) > 0:\n",
    "                line = check_softAB(line)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # 3). Reduce empty lines\n",
    "            if len(line) == 0: line = np.nan\n",
    "            return line\n",
    "        \n",
    "        # Reducing line-by-line\n",
    "        df['LINES'] = df.apply(reduce_entities, axis=1)\n",
    "        df = df.dropna(subset=['LINES']).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "    \"\"\" MODULE FL 2:> FIND SUPPLIER TAGS IN FILE\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Looks for SupplierTag in PRESENT_LINE, ABOVE_LINE and if found, returns a SupplierName                           \"\"\"\n",
    "    def check_SupplierTags(df):\n",
    "        \n",
    "        # looking for SupplierTag line-by-line\n",
    "        MAX_TOKENS = 15\n",
    "        def find_SupplierTag(line):\n",
    "            \"\"\"\n",
    "            Finds a SupplierTag in line, and takes the following chunk of line as a SupplierName (based on rules).\n",
    "            :param: string line: A line\n",
    "            :returns: string line: SupplierName if SupplierTag is found else None\n",
    "            \"\"\"\n",
    "            SupplierTag = [t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line, flags=re.IGNORECASE)]\n",
    "            SupplierTag_value = \" \".join(re.findall(r\"{}(?!.*[\\:].*)(?=.*[A-Za-z].*)(.*)\".format(SupplierTag[0]), line.strip(), flags=re.IGNORECASE)) if len(SupplierTag) > 0 else None\n",
    "            SupplierTag_tokens = re.findall(r\"\\w+\", SupplierTag_value) if SupplierTag_value != None and len(SupplierTag_value) > 0 else []\n",
    "            if SupplierTag_tokens != []:\n",
    "                condition1 = 0 < len(SupplierTag_tokens) <= MAX_TOKENS \n",
    "                condition2 = SupplierTag_tokens[0].strip().lower() not in NEGATIVE_SUPPLIERWORDS\n",
    "                if all([condition1, condition2]): \n",
    "                    # print(\"SUPPLIER TAG == \", line, SupplierTag, SupplierTag_value)\n",
    "                    return SupplierTag_value \n",
    "            return None\n",
    "        \n",
    "        # looking for SupplierTag\n",
    "        SUPPLIER_TAG_FOUND = None\n",
    "        for line_index in df.index.values:\n",
    "            # looking in \"PRESENT_LINE\"\n",
    "            line_present = df.iloc[line_index:line_index+1].LINES.values[0]\n",
    "            SupplierTag_value = find_SupplierTag(line_present)\n",
    "            if SupplierTag_value != None:\n",
    "                SUPPLIER_NAME = SupplierTag_value\n",
    "                SUPPLIER_TAG_FOUND = \"line_present\"\n",
    "                # print(\"SupplierTag found in present line! Line = {}; SN = {}\".format(line_present, SUPPLIER_NAME))\n",
    "                break\n",
    "            else:\n",
    "                # looking in \"ABOVE_LINE\"\n",
    "                if line_index != 0:\n",
    "                    line_above = df.iloc[line_index-1:line_index].LINES.values[0]\n",
    "                else:\n",
    "                    line_above = \"\"\n",
    "                SupplierTag = [t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line_above, flags=re.IGNORECASE)]\n",
    "                SupplierTag_value = re.findall(r\"\\w+\", \" \".join(re.findall(r\"{}(.*)\".format(SupplierTag[0]), line_above.strip(), flags=re.IGNORECASE))) if len(SupplierTag) > 0 else None\n",
    "                # Return a SupplierName if both conditions are fulfilled\n",
    "                condition1 = len(SupplierTag) > 0\n",
    "                condition2 = SupplierTag_value == []\n",
    "                if all([condition1, condition2]):\n",
    "                    SUPPLIER_NAME = line_present\n",
    "                    SUPPLIER_TAG_FOUND = \"line_above\"\n",
    "                    # print(\"SupplierTag found in above line! Line = {}; SN = \".format(line_above, SUPPLIER_NAME))            \n",
    "                else:\n",
    "                    continue \n",
    "                    \n",
    "        if SUPPLIER_TAG_FOUND != None:\n",
    "            return SUPPLIER_NAME.strip()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "    \"\"\" MODULE FL 3:> FIND CUSTOMER TAGS IN FILE\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Looks for CustomerTag in PRESENT_LINE, ABOVE_LINE and if found, returns a CustomerName.                          \"\"\"\n",
    "    def check_CustomerTags(df):\n",
    "        \n",
    "        # looking for CustomerTag line-by-line\n",
    "        def find_CustomerTag(line):\n",
    "            \"\"\"\n",
    "            Finds a CustomerTag in line, and takes the following chunk of line as a CustomerName (based on rules).\n",
    "            :param: string line: A line\n",
    "            :returns: string line: CustomerName if CustomerTag is found else None\n",
    "            \"\"\"\n",
    "            CustomerTag = [t for t in CUSTOMER_TAGS if re.findall(r\"{}\".format(t), line, flags=re.IGNORECASE)]\n",
    "            CustomerTag_value = \" \".join(re.findall(r\"{}(?!.*[\\:].*)(?=.*[a-zA-Z].*)(.*[A-Za-z\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/\\s]+.*)\".format(CustomerTag[0]), line.strip(), flags=re.IGNORECASE)) if len(CustomerTag) > 0 else None\n",
    "            CustomerTag_tokens = re.findall(r\"\\w+\", CustomerTag_value) if CustomerTag_value != None and len(CustomerTag_value) > 0 else []\n",
    "            if CustomerTag_tokens != []: \n",
    "                return CustomerTag_value \n",
    "            return None\n",
    "        \n",
    "        # looking for CustomerTag\n",
    "        CUSTOMER_TAG_FOUND = None\n",
    "        for line_index in df.index.values:\n",
    "            # looking in \"PRESENT_LINE\"\n",
    "            line_present = df.iloc[line_index:line_index+1].LINES.values[0]\n",
    "            CustomerTag_value = find_CustomerTag(line_present)\n",
    "            if CustomerTag_value != None:\n",
    "                CUSTOMER_NAME = CustomerTag_value\n",
    "                CUSTOMER_TAG_FOUND = \"line_present\"\n",
    "                # print(\"CustomerTag found in present line! Line = {}; SN = {}\".format(line_present, CUSTOMER_NAME))\n",
    "                break\n",
    "            else:\n",
    "                # looking in \"ABOVE_LINE\"\n",
    "                if line_index != 0:\n",
    "                    line_above = df.iloc[line_index-1:line_index].LINES.values[0]\n",
    "                else:\n",
    "                    line_above = \"\"\n",
    "                CustomerTag = [t for t in CUSTOMER_TAGS if re.findall(r\"{}\".format(t), line_above, flags=re.IGNORECASE)]\n",
    "                CustomerTag_value = re.findall(r\"\\w+\", \" \".join(re.findall(r\"{}(.*)\".format(CustomerTag[0]), line_above.strip(), flags=re.IGNORECASE))) if len(CustomerTag) > 0 else None\n",
    "                # Return a CustomerName if both conditions are fulfilled\n",
    "                condition1 = len(CustomerTag) > 0\n",
    "                condition2 = CustomerTag_value == []\n",
    "                if all([condition1, condition2]): \n",
    "                    CUSTOMER_NAME = line_present\n",
    "                    CUSTOMER_TAG_FOUND = \"line_above\"\n",
    "                    # print(\"CustomerTag found in above line! Line = {}; SN = \".format(line_above, CUSTOMER_NAME))\n",
    "                else:\n",
    "                    continue \n",
    "        \n",
    "        if CUSTOMER_TAG_FOUND != None:\n",
    "            return CUSTOMER_NAME.strip()\n",
    "        else:\n",
    "            return None\n",
    "    # ------------------------------------------------------------------------------------------------------------------ #\n",
    "    \n",
    "    \n",
    "    # ################################################################################################################## #\n",
    "    # ------------------------------------------ MODULES :: line level (LL) -------------------------------------------- #\n",
    "    # ################################################################################################################## #\n",
    "\n",
    "    \"\"\" MODULE LL 1:> SORT df.LINES\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Sort the df.LINES giving more importance to some columns.                                                        \"\"\"\n",
    "    def sort(df):\n",
    "        \"\"\"\n",
    "        SORTING MECHANISM\n",
    "        ------------------\n",
    "        Sort df.LINES in priority order:\n",
    "        (1). Model_1 Probability    (i.e. generic model's probability of label 1)\n",
    "        (2). Model_2 Probability    (i.e. NN model's probability of label 1)\n",
    "        (3). Line-number            (i.e. position/quadrant of line in file)\n",
    "        \"\"\"\n",
    "        df['LINE_NO'] = [line_no for line_no in range(df.shape[0])]  \n",
    "        # OLD: df['Model1_P1_LINENO'] = [line_no if prob >=0.95 else 0 for prob, line_no in zip(df.Model1_P1, df.LINE_NO)]\n",
    "        # OLD: df = df.sort_values(by=['Model1_P1', 'Model1_P1_LINENO', 'Model2_P1', 'LINE_NO'], ascending=[False, True, False, True]) \n",
    "        df = df.sort_values(by=['Model1_P1', 'Model2_P1', 'LINE_NO'], ascending=[False, False, True]) \n",
    "        return df\n",
    "\n",
    "    \n",
    "    \"\"\" MODULE LL 2:> FILTER NOISE LINES\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Looks for lines that fulfill 'noise/outlier' condition and remove them from df.                                  \"\"\"\n",
    "    def check_NoiseLines(df):\n",
    "        \"\"\"\n",
    "        NOISE FILTERING MECHANISM\n",
    "        --------------------------\n",
    "        Remove a line if it does not contain any 'abbrv' AND :\n",
    "        (1). Number of tokens are more than 20\n",
    "        (2). Number of chars are more than 80\n",
    "        (3). Contains only digits\n",
    "        (4). Contains more than 70% stop-words\n",
    "        (5). Contains a negative keyword from a list of stored_negative_keywords\n",
    "        \"\"\"\n",
    "        ######################\n",
    "        ##  Noise Settings  ##\n",
    "        MAX_TOKENS = 20\n",
    "        MAX_LEN = 80\n",
    "        MAX_ALLDIGITS = 0\n",
    "        MAX_PERSTOPWORS = 70.0\n",
    "        MAX_NOISE = 0\n",
    "        ######################\n",
    "        def find_NoiseLines(x):           \n",
    "            tokens = word_tokenize(re.sub(r\"\\s+\", \" \", re.sub(r\"\\^\", \" \", re.sub(r\"[^A-Za-z]\", \" \", x.LINES.strip(), flags=re.IGNORECASE | re.MULTILINE))))\n",
    "            check_NumOfTokens = len(tokens)\n",
    "            check_Length = len(x.LINES.lower().strip())\n",
    "            check_AllDigits = int(x.F1_CONTAINSALLDIGIT)\n",
    "            check_PercentStopWords = len([w for w in tokens if w.lower() in stop_words]) * 100.0/check_NumOfTokens if check_NumOfTokens > 0 else 0\n",
    "            check_NegativeKeywords = len(re.findall(\"|\".join(NOISE_KEYWORDS) + \"|\".join(CUSTOMER_TAGS), str(x.LINES).strip(), flags=re.IGNORECASE | re.MULTILINE))\n",
    "            check_abbrv = len(re.findall(r\"(?=(\\b\" + '\\\\b|\\\\b'.join(list_abbrv_regex) + r\"\\b))\", x.LINES.lower().strip()))\n",
    "            QUALIFY = \"SN\"  \n",
    "            if check_abbrv == 0 and any([check_NumOfTokens > MAX_TOKENS, check_Length > MAX_LEN, \n",
    "                                         check_AllDigits > MAX_ALLDIGITS, check_PercentStopWords > MAX_PERSTOPWORS,\n",
    "                                         check_NegativeKeywords > MAX_NOISE]):\n",
    "                QUALIFY = \"CN\"\n",
    "            return QUALIFY        \n",
    "        df['QUALIFY'] = df.apply(find_NoiseLines, axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "    \"\"\" MODULE LL 3:> REMOVE DUPLICATE PREDICTIONS\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Removes multiple(duplicate) predictions from SupplierName list and CustomerName list.                            \"\"\"\n",
    "    def check_duplicates(prediction_list):\n",
    "        ##########################\n",
    "        ##  Duplicate Settings  ##\n",
    "        MAX_SIMILARITY = 30\n",
    "        ##########################\n",
    "        # Returns True(similar or duplicate) if 1st token is a match AND FuzzyWuzzy score > 30\n",
    "        def check_similarity(s1, s2):\n",
    "            return True if word_tokenize(s1)[0].strip().lower() == word_tokenize(s2)[0].strip().lower() and \\\n",
    "                           fuzz.ratio(s1.lower(), s2.lower()) > MAX_SIMILARITY else False\n",
    "        def eliminate_duplicates(a):\n",
    "            for index, name in enumerate(a):\n",
    "                for i, match_with in enumerate(a):\n",
    "                    if index != i and check_similarity(name[0], match_with[0]):\n",
    "                        a[i][0] = '<DUPLICATE>'\n",
    "            return a\n",
    "        PL = prediction_list\n",
    "        if len(PL) > 2:\n",
    "            PL = eliminate_duplicates(PL)\n",
    "            PL = [w for w in PL if w[0] != '<DUPLICATE>']\n",
    "            if len(PL) == 1:   # in case all were duplicates and hence got removed\n",
    "                PL += prediction_list\n",
    "        return PL\n",
    "\n",
    "\n",
    "    \"\"\" MODULE LL 4:> SPLIT PREDICTIONS INTO 2 GROUPS\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Splits the initial prediciton list into 2 lists: SupplierName list and CustomerName list.                        \"\"\"\n",
    "    def check_split(prediction_list, TAG_SupplierName, TAG_CustomerName):\n",
    "        \"\"\"\n",
    "        SPLITTING MECHANISM\n",
    "        --------------------\n",
    "        Split the list based on:\n",
    "        (1). list contains a SupplierTag\n",
    "        (2). list contains a CustomerTag\n",
    "        (3). list contains noise lines\n",
    "        \"\"\"\n",
    "        \n",
    "        # TAG_SupplierName(T_SN) - Contains potential SupplierName(SN) based on found SupplierTag (viz. MODULE FL 2)\n",
    "        # TAG_CustomerName(T_CN) - Contains potential CustomerName(CN) based on found CustomerTag (viz. MODULE FL 3)\n",
    "        \n",
    "        # If T_SN i.e. potential SN contains a email/url, in that case compare with remaining SN predictions\n",
    "        def update__TAG_SupplierName():\n",
    "            updated__TAG_SupplierName = TAG_SupplierName\n",
    "            if TAG_SupplierName != None:  \n",
    "                # Check for email/url\n",
    "                domains = sum([x[-1] for x in [find_email(TAG_SupplierName), find_url(TAG_SupplierName)] if any(x)], [])\n",
    "                # If T_SN is a email/url, Find its \"domain-name\" (e.g. abc@gmail.com -> \"gmail\")\n",
    "                domain = domains[0] if len(domains) > 0 else None\n",
    "                # If T_SN is a email/url, Assign a new T_SN\n",
    "                if domain != None:\n",
    "                    for chunk in prediction_list:\n",
    "                        # Assign a new T_SN if: Model_1 P1 > 70%  AND  FuzzyWuzzy(found-domain, line) > 50\n",
    "                        if all( [chunk[1] >= 0.70, fuzz.partial_ratio(domain.lower(), chunk[0].lower()) > 50] ):\n",
    "                            updated__TAG_SupplierName = chunk[0]\n",
    "                            #print(\"TAG_SupplierName is a email/url. Updated Tag_SupplierName = \", updated__TAG_SupplierName)\n",
    "                            break\n",
    "            return updated__TAG_SupplierName\n",
    "        \n",
    "        # Sanity Check: if T_SN contains a email/url -> Assign a new T_SN or keep same potenital SN\n",
    "        SupplierName = update__TAG_SupplierName()\n",
    "        \n",
    "        # Potential CN\n",
    "        CustomerName = TAG_CustomerName\n",
    "        \n",
    "        # 1. If both T_SN, T_CN were found in file!\n",
    "        if SupplierName != None and CustomerName != None: \n",
    "            # SN list = T_SN + predictions;  CN list = T_CN\n",
    "            SN_list = [(SupplierName, 1.0, 1.0, 'SN', 0)] + prediction_list\n",
    "            CN_list = [(CustomerName, 1.0, 1.0, 'CN', 0)]\n",
    "        \n",
    "        # 2. If only T_CN was found!\n",
    "        elif SupplierName == None and CustomerName != None:\n",
    "            # SN list = predictions - T_CN;  CN list = T_CN\n",
    "            SN_list = [x for x in prediction_list if x[0] != CustomerName or len(re.findall(CustomerName.strip(), x[0], flags=re.IGNORECASE)) == 0]\n",
    "            CN_list = [(CustomerName, 1.0, 1.0, 'CN', 0)]\n",
    "        \n",
    "        # 3. If only T_SN was found!\n",
    "        elif SupplierName != None and CustomerName == None:\n",
    "            # SN list = T_SN + predictions;  CN list = predictions - T_SN\n",
    "            SN_list = [(SupplierName, 1.0, 1.0, 'SN', 0)] + prediction_list\n",
    "            CN_list = [x for x in prediction_list if x[0] != SupplierName or len(re.findall(SupplierName.strip(), x[0], flags=re.IGNORECASE)) == 0]\n",
    "        \n",
    "        # 4. If none were found!\n",
    "        else: \n",
    "            # SN list = predictions;  CN list = predictions\n",
    "            SN_list = [x for x in prediction_list if x[3] == 'SN']\n",
    "            CN_list = [x for x in prediction_list if x[3] == 'CN']\n",
    "        \n",
    "        # Final Sanity Check: extracted SN, CN are subjected to final review to get separate lists.\n",
    "        SN_list = [chunk for chunk in SN_list if chunk[3] == 'SN']\n",
    "        CN_list = [chunk for chunk in CN_list if chunk[3] == 'CN']\n",
    "        return SN_list, CN_list\n",
    "\n",
    "\n",
    "    \"\"\" MODULE LL 5:> CLEAN SN_LIST \n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Cleans the final prediction SN list, and finds chunks (i.e. exact Supplier Name) from it.                         \"\"\"\n",
    "    def check_cleaning(chunk):\n",
    "        \"\"\"\n",
    "        CHUNK-IDENTIFICATION MECHANISM\n",
    "        -------------------------------\n",
    "        Find a chunk from the line if:\n",
    "        (1). Line contains a email/url               --> chunk = domain-name\n",
    "        (2). Line contains a \".com\"                  --> chunk = domain-name\n",
    "        (3). Line contains a SupplierTag             --> chunk = Supplier tag value\n",
    "        (4). Line contains more than '12'  words     --> chunk = Spacy NER(ORG, FAC) tag\n",
    "        (5). Line contains more than '40%' stopwords --> chunk = Removed stopwords\n",
    "        (6). Line contains whitespaces, symbols      --> chunk = Regex cleaned \n",
    "        \n",
    "        (7). Line does not fulfill (1) to (6)        --> chunk = Entire remaining line\n",
    "        \"\"\"\n",
    "        ######################\n",
    "        ##  Clean Settings  ##\n",
    "        MAX_TOKENS = 12\n",
    "        MAX_STOPWORDS_PERCENT = 40.0\n",
    "        ######################\n",
    "        \n",
    "        # chunk - Predicted lines from final SN list\n",
    "        \n",
    "        # 1. If line is a email/url --> reduce the line to it's domain-name\n",
    "        domains = sum([x[-1] for x in [find_url(chunk), find_email(chunk)] if any(x)], [])\n",
    "        domain = domains[0].upper() if len(domains) > 0 else None\n",
    "        if domain != None:\n",
    "            return domain.upper()\n",
    "        else:\n",
    "            # 2. If domain-name not found above, performing hard-check for domain-name --> reduce the line to it's domain-name\n",
    "            domain_2 = re.findall(r\"([A-Za-z]+)\\.com\\b\", chunk, flags=re.IGNORECASE)\n",
    "            if len(domain_2) > 0:\n",
    "                return domain_2[0].upper()\n",
    "        \n",
    "        # 3.If line contains a SupplierTag --> reduce the line to found SupplierTag_Value\n",
    "        ALL_TAGS = SUPPLIER_TAGS + CUSTOMER_TAGS + MIXED_TAGS\n",
    "        find_Tag = [tag for tag in ALL_TAGS if re.findall(r\"{}\".format(tag), chunk.strip(), flags=re.IGNORECASE)]\n",
    "        value_TAG = \" \".join(re.findall(r\"{}(?=.*[a-zA-Z].*)(.*)\".format(find_Tag[0]), chunk.strip(), flags=re.IGNORECASE)).strip() if len(find_Tag) > 0 else None\n",
    "        if value_TAG != None:\n",
    "            return value_TAG\n",
    "        \n",
    "        # Tokenize the line into tokens and calculate % of stop-words\n",
    "        tokens = re.findall(r\"\\w+\", chunk.strip())\n",
    "        tokens_stopwords_percentage = len([w for w in tokens if w.lower() in stop_words])*100.0/len(tokens) if len(tokens) > 0 else 0\n",
    "            \n",
    "        # 4. If line contains more than '12' words --> reduce the line to SpacyNER(ORG, FAC) tag\n",
    "        if len(tokens) > MAX_TOKENS:\n",
    "            chunkNER = [ent.orth_ for ent in nlp(chunk).ents if ent.label_ in ['ORG', 'FAC']]\n",
    "            chunkNER = chunkNER[0] if chunkNER != [] else None\n",
    "            if chunkNER != None: return chunkNER.strip()\n",
    "        \n",
    "        # 5. If line contains more than '40%' stopwords --> remove stop-words from the line\n",
    "        if tokens_stopwords_percentage > MAX_STOPWORDS_PERCENT:\n",
    "            chunk = \" \".join([w for w in tokens if w.lower() not in stop_words])\n",
    "        \n",
    "        # 6. Clean the line\n",
    "        chunk = re.sub(r\"\\s+\", \" \", re.sub(r\"\\^\", \" \", re.sub(r\"[^A-Za-z0-9\\@\\&\\(\\)\\-]\", \" \", chunk.strip(), flags=re.IGNORECASE))).strip()\n",
    "        chunk = \" \".join([w for w in word_tokenize(chunk) if w.lower() not in SMART_STOPWORDS])\n",
    "        return chunk\n",
    "    \n",
    "    \n",
    "    \"\"\" MODULE LL 6:> FINAL SN_LIST\n",
    "    ----------------------------------------------------------------------------------------------------------------------\n",
    "    - Chooses the final SN list from SN_list and CN_list.                                                              \"\"\"\n",
    "    def get_final_chunks(SN_list, CN_list):\n",
    "        \"\"\"\n",
    "        Picks top \"N\" from SN_list + \"1\" from CN_list (condition: if CN_list's top value prob(1) >= 0.90)\n",
    "        :param: list SN_list, list CN_list\n",
    "        :returns: list FINAL SN_LIST containing 'N' or 'N+1' Supplier Names\n",
    "        \"\"\"\n",
    "        # # EDIT THIS :: Gives top 'N' predictions from SN lists\n",
    "        N_VALUES = 3\n",
    "\n",
    "        TOP_SNlist = [(sn[0], round(np.mean([sn[1], sn[2]]) * 100.0, 2)) for sn in SN_list[:N_VALUES]]\n",
    "        if len(CN_list) != 0 and np.mean([CN_list[0][1], CN_list[0][2]]) >= 0.80:\n",
    "            TOP_CNlist = [(CN_list[0][0], float(TOP_SNlist[N_VALUES - 1][1]) - 10.0)]\n",
    "        else:\n",
    "            # borrow from SN_list if CN_list is empty!\n",
    "            TOP_CNlist = [(sn[0], round(np.mean([sn[1], sn[2]]) * 100.0, 2)) for sn in SN_list[N_VALUES:N_VALUES + 1]]\n",
    "        FINAL_LIST = TOP_SNlist + TOP_CNlist\n",
    "        return FINAL_LIST\n",
    "            \n",
    "    ######################################################################################################################\n",
    "    # Chunk Identification Starts......\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------ CHUNK :: file level ---------------------------------------------------- #\n",
    "    # 1. MODULE FL 1: Reduce NER entitites, AB Lines in predictions\n",
    "    df = check_entities(df)\n",
    "    \n",
    "    # 2. MODULE FL 2: Find Supplier Tags in whole file\n",
    "    TAG_SupplierName = check_SupplierTags(df)\n",
    "    \n",
    "    # 3. MODULE FL 3: Find Customer Tags in whole file\n",
    "    TAG_CustomerName = check_CustomerTags(df)\n",
    "    print(\"Found :: SN_Tag = {};  CN_Tag = {}\".format(TAG_SupplierName, TAG_CustomerName))\n",
    "    # ------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "    \n",
    "    # ------------------------------------------ CHUNK :: line level ---------------------------------------------------- #\n",
    "    final_SN, final_CN = [], []\n",
    "    \n",
    "    # 4. MODULE LL 1: Sort df.LINES\n",
    "    sorted_df = sort(df)  \n",
    "    print(\"\\nModule_LL_1: Sorted Lines, Predictions == \", sorted_df.LINES.tolist())\n",
    "    \n",
    "    # 5. MODULE LL 2: Filter noise(outliers/unwanted) df.LINES\n",
    "    sorted_df = check_NoiseLines(sorted_df)\n",
    "    print(\"\\nModule_LL_2: Removed Noise, Predictions == \", sorted_df.LINES.tolist())\n",
    "    \n",
    "    # 6. Create \"Prediction List\" with only necesseary columns\n",
    "    # chunk[0] = df.LINES\n",
    "    # chunk[1] = Model1_P1\n",
    "    # chunk[2] = Model2_P1\n",
    "    # chunk[3] = QUALIFY (SN/CN)\n",
    "    # chunk[4] = LINE_NO\n",
    "    prediction_list = [[L, P1, P2, Q, Ln] for L, P1, P2, Q, Ln in zip(sorted_df.LINES.tolist(), sorted_df.Model1_P1.tolist(), sorted_df.Model2_P1.tolist(), sorted_df.QUALIFY.tolist(), sorted_df.LINE_NO.tolist())]\n",
    "    \n",
    "    # 7. MODULE LL 3: Remove duplicates from predictions\n",
    "    prediction_list = check_duplicates(prediction_list) \n",
    "    print(\"\\nModule_LL_3: Removed Duplicates, Predictions == \", prediction_list)\n",
    "    \n",
    "    # 8. MODULE LL 4: Split predicitons list --> SN list, CN list\n",
    "    SN_list, CN_list = check_split(prediction_list, TAG_SupplierName, TAG_CustomerName)\n",
    "    print(\"\\nModule_LL_4: Splitted Predictions into two, SN_LIST == {}; CN_LIST == {};\".format(SN_list, CN_list))\n",
    "    \n",
    "    # 9. MODULE LL 5: Cleans the final prediction SN list, and finds chunks (i.e. exact Supplier Name) from it. \n",
    "    SN_list = list(map(lambda chunk: [check_cleaning(chunk[0]), chunk[1], chunk[2], chunk[3], chunk[4]], SN_list))\n",
    "    print(\"\\nModule_LL_5: Cleaned List, SN_LIST == {}\".format(SN_list))\n",
    "    \n",
    "    # 10. MODULE LL 6: Final List of SN...\n",
    "    FINAL_SN_list = get_final_chunks(SN_list, CN_list)\n",
    "    print(\"\\nModule_LL_6: Final SN List == {}\".format(FINAL_SN_list))\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------------- #    \n",
    "    ######################################################################################################################\n",
    "\n",
    "    return FINAL_SN_list, SN_list, CN_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:26.415240Z",
     "iopub.status.busy": "2021-02-04T08:33:26.414289Z",
     "iopub.status.idle": "2021-02-04T08:33:26.417404Z",
     "shell.execute_reply": "2021-02-04T08:33:26.416713Z"
    },
    "papermill": {
     "duration": 0.291999,
     "end_time": "2021-02-04T08:33:26.417559",
     "exception": false,
     "start_time": "2021-02-04T08:33:26.125560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Breaks the enitre df into file-by-file df and extracts chunk per file\n",
    "#####################################################################################\n",
    "# INPUT   --> all files (whole df)\n",
    "# OUTPUT  --> list of Supplier Name chunks, list of Customer Name chunks for all files\n",
    "\n",
    "def predict_chunk_lines(df):\n",
    "\n",
    "    # Execute for each filename...\n",
    "    final_df = []\n",
    "    for f in df.FILENAME.unique():\n",
    "        \n",
    "        # For each filename, create a temp df\n",
    "        tempdf = df[df.FILENAME == f].copy().reset_index(drop=True)\n",
    "\n",
    "        ###########################################################################\n",
    "        # 1. Line Classification\n",
    "        # TRUE\n",
    "        actual_SN = str(tempdf.SUPPLIER_NAME.values[0])\n",
    "        # IF SUPPLIER NAME LINE WAS FOUND\n",
    "        if 1 in tempdf.Y_SN.values:\n",
    "            correct_LINES = tempdf[(tempdf.Y_SN == 1) & (tempdf.Model1_Y_PRED == 1)]['LINES'].tolist()\n",
    "        else:\n",
    "            correct_LINES = []\n",
    "        if len(correct_LINES) > 0:\n",
    "            correct_LINE_found = 1\n",
    "        else:\n",
    "            correct_LINE_found = 0\n",
    "        ###########################################################################\n",
    "\n",
    "        ###########################################################################\n",
    "        # 2. Chunk Identification\n",
    "        FINAL_SN_list, SN_list, CN_list = chunk_identification(tempdf)\n",
    "        S1 = fuzz.partial_ratio(actual_SN.lower(), SN_list[0][0].lower())\n",
    "        if S1 < 90:\n",
    "            S1 = 0\n",
    "        ###########################################################################\n",
    "\n",
    "        # STORE\n",
    "        final_df.append({\"FILENAME\": f,\n",
    "                         \"SN\": actual_SN,\n",
    "                         \"FINAL_SN\": FINAL_SN_list,\n",
    "                         \"PRED_SN\": SN_list, \n",
    "                         \"CL\": correct_LINE_found,\n",
    "                         \"S1\": S1, \n",
    "                         \"PRED_CN\": CN_list})\n",
    "\n",
    "    # FINAL DATAFRAME\n",
    "    pred_df = pd.DataFrame.from_dict(final_df)\n",
    "    print(\"Total Files = {}; Correct Lines = {}; Lines Missed = {}\\n>> SCORE = {};\"\n",
    "          .format(pred_df.shape[0], pred_df.CL.sum(), pred_df.shape[0] - pred_df.CL.sum(), pred_df.S1.mean()))\n",
    "    \n",
    "    # DISPLAY RESULTS\n",
    "    pred_df = pred_df[['FILENAME', 'SN', 'FINAL_SN', 'PRED_SN', 'PRED_CN']]\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.281623,
     "end_time": "2021-02-04T08:33:26.979463",
     "exception": false,
     "start_time": "2021-02-04T08:33:26.697840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### SAMPLE CASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:27.541468Z",
     "iopub.status.busy": "2021-02-04T08:33:27.540532Z",
     "iopub.status.idle": "2021-02-04T08:33:27.555807Z",
     "shell.execute_reply": "2021-02-04T08:33:27.556313Z"
    },
    "papermill": {
     "duration": 0.300594,
     "end_time": "2021-02-04T08:33:27.556490",
     "exception": false,
     "start_time": "2021-02-04T08:33:27.255896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unseen_df_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-dd04fdb9fe0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# run Phase II: Chunk Identification Module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_chunk_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munseen_df_labelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# examine final_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unseen_df_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "\n",
    "# create unseen_df with probs\n",
    "# 1. load unseen_df\n",
    "# 2. run a fitted model on unseen_df\n",
    "# 3. store predictions in unseen_df\n",
    "\n",
    "# run Phase II: Chunk Identification Module\n",
    "final_df = predict_chunk_lines(unseen_df_labelled)\n",
    "\n",
    "# examine final_df\n",
    "pd.set_option('max_colwidth', 800)\n",
    "final_df[['FILENAME', 'SN', 'FINAL_SN']][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.27859,
     "end_time": "2021-02-04T08:33:28.113505",
     "exception": false,
     "start_time": "2021-02-04T08:33:27.834915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.297157,
     "end_time": "2021-02-04T08:33:28.686161",
     "exception": false,
     "start_time": "2021-02-04T08:33:28.389004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <ins>14. OLD Methods/Versions for Chunk Identification Module</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.282181,
     "end_time": "2021-02-04T08:33:29.270409",
     "exception": false,
     "start_time": "2021-02-04T08:33:28.988228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Version 3.0 - 21/05/2020\n",
    "- Version 2.0 - 10/05/2020\n",
    "- version 1.0 - 30/04/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.275829,
     "end_time": "2021-02-04T08:33:29.829689",
     "exception": false,
     "start_time": "2021-02-04T08:33:29.553860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Version 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:30.395021Z",
     "iopub.status.busy": "2021-02-04T08:33:30.392376Z",
     "iopub.status.idle": "2021-02-04T08:33:30.406903Z",
     "shell.execute_reply": "2021-02-04T08:33:30.407425Z"
    },
    "papermill": {
     "duration": 0.298219,
     "end_time": "2021-02-04T08:33:30.407635",
     "exception": false,
     "start_time": "2021-02-04T08:33:30.109416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OLD :: Version 3.0\n",
    "# def chunk_identification(df):\n",
    "#     ''' REGEX TAGS \"remit\\s+to[^\\:]*\\:+\",\n",
    "#     ----------------------------------------------------------------------------------------------------------------------'''\n",
    "#     SUPPLIER_TAGS = [r\"\\bdirected\\s+inquiries\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirect\\s+inquiries\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bdirect\\s+inquiries\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirecting\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bdirected\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirects\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bdirect\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bcrediting\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcredited\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcredits\\s+to\\b\\s*[\\:|\\-]+\\s*\",\n",
    "#                      r\"\\bcredit\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremiting\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremittance\\s+to\\b\\s*[\\:|\\-]+\\s*\",\n",
    "#                      r\"\\bremitted\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremits\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bremit\\s+to\\b\\s*[\\:|\\-]+\\s*\",\n",
    "#                      r\"f/b/o\\s*\", r\"\\baccount\\s+name\\b\\s*[\\:|\\-]+\\s*\", r\"\\bchecked\\s+payable\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bchecks\\s+payable\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bchecks\\s+payable\\s+at\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bcheck\\s+payable\\s+at\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcheck\\s+payable\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bchecks\\s+pay\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcheck\\s+pay\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bchecking\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bchecked\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bpayable\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bchecks\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bcheck\\s+to\\b\\s*[\\:|\\-]+\\s*\", r\"\\bpays\\s+to\\b\\s*[\\:|\\-]+\\s*\", \n",
    "#                      r\"\\bpay\\s+to\\b\\s*[\\:|\\-]+\\s*\"]\n",
    "    \n",
    "#     NEGATIVE_SUPPLIERWORDS = [\"attn\", \"from\", \"subject\", \"terms\", \"please\", \"invoice\", \"invoices\", \"invoiceno\", \n",
    "#                               \"amount\", \"amounts\", \"address\", \"addresses\", \"addrss\", \"telephone\", \"phone\", \"phones\", \"call\", \n",
    "#                               \"callat\", \"phoneat\", \"fax\", \"cancel\", \"notice\", \"confidential\",  \"client\"]               \n",
    "                           \n",
    "#     CUSTOMER_TAGS = [r\"\\bbilling\\s+to\\b\\s*\\:*\\s*\", r\"\\bbilled\\s+to\\b\\s*\\:*\\s*\", r\"\\bbills\\s+to\\b\\s*\\:*\\s*\", \n",
    "#                      r\"\\bbill\\s+to\\b\\s*\\:*\\s*\", r\"\\bbilling\\s+at\\b\\s*\\:*\\s*\", r\"\\bbilled\\s+at\\b\\s*\\:*\\s*\", \n",
    "#                      r\"\\bbills\\s+at\\b\\s*\\:*\\s*\", r\"\\bbill\\s+at\\b\\s*\\:*\\s*\", r\"\\bshipping\\s+to\\b\\s*\\:*\\s*\", \n",
    "#                      r\"\\bshipped\\s+to\\b\\s*\\:*\\s*\", r\"\\bships\\s+to\\b\\s*\\:*\\s*\", r\"\\bship\\s+to\\b\\s*\\:*\\s*\", \n",
    "#                      r\"\\bshipto\\b\\s*\\:*\\s*\", r\"\\bshippingto\\b\\s*\\:*\\s*\", r\"\\bshipsto\\b\\s*\\:*\\s*\"]\n",
    "    \n",
    "#     MIXED_TAGS = [r\"\\bbelonging\\s+to\\b\\s*\\:*\\s*\", r\"\\bbelonged\\s+to\\b\\s*\\:*\\s*\", r\"\\bbelongs\\s+to\\b\\s*\\:*\\s*\", \n",
    "#                   r\"\\bbelong\\s+to\\b\\s*\\:*\\s*\", r\"\\bdivisions\\s+of\\b\\s*\\:*\\s*\", r\"\\bdivision\\s+of\\b\\s*\\:*\\s*\", \n",
    "#                   r\"\\bdivision\\s+of\\b\\s*\\:*\\s*\", r\"\\bdivs\\s+of\\b\\s*\\:*\\s*\", r\"\\bdiv\\s+of\\b\\s*\\:*\\s*\", \n",
    "#                   r\"\\bconnect\\s+with\\b\\s*\\:*\\s*\", r\"\\bcompany\\s+of\\b\\s*\\:*\\s*\", r\"\\bcomp\\s+of\\b\\s*\\:*\\s*\", \n",
    "#                   r\"\\bfirm\\s+of\\b\\s*\\:*\\s*\", r\"\\bname\\s+of\\b\\s*\\:*\\s*\", r\"\\baccount\\s+of\\b\\s*\\:*\\s*\", \n",
    "#                   r\"\\bacc\\s+of\\b\\s*\\:*\\s*\", r\"\\baccount\\s+name\\b\\s*\\:*\\s*\", r\"\\bacc\\s+name\\b\\s*\\:*\\s*\", \n",
    "#                   r\"\\bis\\s+of\\b\\s*\\:*\\s*\", r\"\\bis\\s+to\\b\\s*\\:*\\s*\", \n",
    "#                   r\"\\bto\\b\\s*\\:*\\s+\", \n",
    "#                   r\"^by\\b\\s*\\:*\\s+\", r\"^mailing\\b\\s*\\:*\\s+\", r\"^to\\b\\s*\\:*\\s+\", r\"^of\\b\\s*\\:*\\s+\",\n",
    "#                   r\"^mail\\b\\s*\\:*\\s+\", r\"^payable\\s+to\\b\\s*\\:*\\s+\", r\"^checks\\s+payable\\s+to\\b\\s*\\:*\\s+\",\n",
    "#                   r\"^check\\s+payable\\s+to\\b\\s*\\:*\\s+\", r\"^checks\\s+to\\b\\s*\\:*\\s+\", r\"^check\\s+to\\b\\s*\\:*\\s+\"]\n",
    "        \n",
    "#     NOISE_KEYWORDS = [r\"\\battn\\b[^\\:\\s]*\\:+\", r\"\\bfrom\\b\\s*\\:+\", r\"\\bsubject\\b\\s*\\:+\", r\"\\bcc\\b\\s*\\:+\", r\"\\bbcc\\b\\s*\\:+\", \n",
    "#                       r\"\\bterms\\s+and\\s+condition\\b\", r\"\\bterms\\s+\\&\\s+conditions\\b\", r\"\\bterms\\&conditions\\b\", \n",
    "#                       r\"\\btermsandconditions\\b\", r\"\\bT\\&C\\b\", r\"\\bTC\\b\", r\"\\bif\\s+you\\s+have\\s+any\\s+questions\\b\",\n",
    "#                       r\"\\bif\\s+you\\s+have\\s+any\\s+question\\b\", r\"\\byou\\s+have\\s+any\\s+questions\\b\", \n",
    "#                       r\"\\byou\\s+have\\s+any\\s+question\\b\", r\"\\byou\\s+have\\s+questions\\b\", r\"\\byou\\s+have\\s+question\\b\",\n",
    "#                       r\"\\bif\\s+you\\s+have\\s+questions\", r\"\\bplease\\s+write\\s+to\\b\\[^\\:\\s]*\\:+\", \n",
    "#                       r\"\\bplease\\s+write\\s+at\\b\\[^\\:\\s]*\\:+\", r\"\\bwrite\\s+to\\b\\[^\\:\\s]*\\:+\", \n",
    "#                       r\"\\bcontact\\s+to\\b[^\\:\\s]*\\:+\", r\"\\bemail\\s+to\\b[^\\:\\s]*\\:+\", r\"\\bemail\\s+at\\b[^\\:\\s]*\\:+\", \n",
    "#                       r\"\\bemail\\b\\s+\\@\\s*\\:+\", \"\\bconfidential\\b\"]\n",
    "\n",
    "#     PHONE_TAGS = [r\"\\bphone\\b\\s*\\:*\\s+\", r\"\\bphone\\s+at\\b\\s*\\:*\\s+\", r\"\\bphones\\b\\s*\\:*\\s+\", r\"\\bphone\\s+on\\b\\s*\\:*\\s+\", \n",
    "#                   r\"\\bcall\\b\\s*\\:*\\s+\", r\"\\bcalling\\b\\s*\\:*\\s+\", r\"\\bcall\\s+at\\b\\s*\\:*\\s+\", r\"\\bcalls\\s+at\\b\\s*\\:*\\s+\", \n",
    "#                   r\"\\btelephone\\b\\s*\\:*\\s+\", r\"\\bt\\b[^\\:\\s]*\\:+\\s+\", r\"\\btelephone\\s+at\\b\\s*\\:*\\s+\", \n",
    "#                   r\"\\btelephones\\b\\s*\\:*\\s+\", r\"\\bdial\\b\\s*\\:*\\s+\", r\"\\bdial\\s+at\\b\\s*\\:*\\s+\", r\"\\bdate\\b\\s*\\:*\\s+\", \n",
    "#                   r\"\\bday\\b\\s*\\:*\\s+\", r\"\\btime\\b\\s*\\:*\\s+\", r\"\\bcalendar\\b\\s*\\:*\\s+\", r\"\\bfax\\b\\s*\\:*\\s+\"]\n",
    "                              \n",
    "#     # runs on each prediction...\n",
    "#     SMART_STOPWORDS = [\"https\", \"http\", \"please\", \"pls\", \"lets\", \"know\", \"knowing\", \"knows\", \n",
    "#                        \"respond\", \"responding\", \"responds\", \"naming\", \"names\", \"thanks\", \"thank\", \"thankyou\", \n",
    "#                        \"thanksyou\", \"thanking\", \"thankingyou\", \"thanx\", \"thnx\", \"thx\", \"asap\", \n",
    "#                        \"dear\", \"hello\", \"kindly\", \"regards\", \"hi\", \"hola\", \"hey\", \"yeah\", \"nope\", \n",
    "#                        \"advise\", \"attached\", \"days\", \"months\", \"month\", \"invoice\", \"invoices\", \"invocing\", \n",
    "#                        \"ordernumber\", \"invoiceno\", \"accountno\", \"remit\", \"remittance\", \"fbo\", \"inquiries\", \n",
    "#                        \"payable\", \"billed\", \"billing\", \"bills\", \"shipped\", \"shipping\", \"ships\", \"shipto\", \"shippingto\", \n",
    "#                        \"shipsto\", \"terms&conditions\", \"termsandconditions\", \"confidential\", \"t&c\",\"emailed\", \"customerno\", \n",
    "#                        \"duration\", \"since\", \"address\", \"order\", \"make\", \"check\", \"checks\", \"submit\", \"submits\", \"sumitting\" , \n",
    "#                        \"submitto\",  \"paying\", \"paymentsto\", \"notices\", \"phone\", \"phones\", \"phoning\", \"phono\", \"emailo\", \n",
    "#                        \"cancel\", \"cancelling\", \"cancellation\", \"cancellations\", \"requires\", \"require\", \"required\", \n",
    "#                        \"requiring\", \"noticeable\", \"noticing\", \"date\", \"dates\", \"months\", \"month\", \"day\", \"days\",\n",
    "#                        \"addresses\", \"address\", \"inovicenumber\",\n",
    "#                        \"if\", \"because\", \"is\", \"there\", \"always\", 'itself', 'shouldn', 'there', 'were', 'should', \n",
    "#                        'who', 'hasn', \"you've\", 'he', \"mustn't\", 'whom', 'mustn', 'very', 'doesn', 'have', 'here', \n",
    "#                        'those', 'wasn', 'having', \"aren't\", 'yourselves', \"hasn't\", 'themselves', 'until', \"you'd\", \n",
    "#                        'shan', 'him', 'ourselves', 'aren', \"isn't\", \"haven't\", 'that', 'herself', \"hadn't\", 'both',\n",
    "#                        'where', 'most', 'doing', 'further', 'any', 'didn', 'theirs', \"weren't\", 'same', 'you', 'hadn', \n",
    "#                        'myself', 'yours', 'won', 'mightn', 'she', 'hers', 'weren', \"she's\", 'does', 'during', 'was', \n",
    "#                        'wouldn', 'because', \"won't\", 'again', 'his', \"you'll\", 'once', 'between', 'couldn', 'how', \n",
    "#                        'what', \"shouldn't\", 'then', 'own', 'above', \"doesn't\", 'had', \"wasn't\", 'them', 'when', 'such', \n",
    "#                        'while', 'if', 'did', 'before', \"couldn't\", 'your', \"shan't\", 'other', 'which', 'himself', 'through', \n",
    "#                        'below', 'under', 'too', \"mightn't\", 'been', \"should've\", 'few', 'these', \"you're\", 'their', \n",
    "#                        'can', 'each', \"it's\", 'has', \"needn't\", 'but', \"wouldn't\", 'needn', 'they', \"didn't\", 'nor', \n",
    "#                        \"that'll\", \"don't\", 'than', 'some']\n",
    "\n",
    "#     ''' ------------------------------------------------------------------------------------------------------------------'''\n",
    "    \n",
    "    \n",
    "#     # ################################################################################################################## #\n",
    "#     # ------------------------------------------ MODULES :: file level ------------------------------------------------- #\n",
    "#     # ################################################################################################################## #\n",
    "    \n",
    "#     ''' FIND SUPPLIER TAGS IN FILE\n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Looks for supplier tags in whole file. Finds a matching supplier tag in: 'present_line' and 'above_line'.        '''\n",
    "#     def check_SupplierTags(df):\n",
    "#         # looking for SUPPLIER_TAG\n",
    "#         MAX_TOKENS = 15\n",
    "#         def find_SupplierTag(line):\n",
    "#             SupplierTag = [t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line, flags=re.IGNORECASE)]\n",
    "#             SupplierTag_value = \" \".join(re.findall(r\"{}(?!.*[\\:].*)(?=.*[A-Za-z].*)(.*)\".format(SupplierTag[0]), line.strip(), flags=re.IGNORECASE)) if len(SupplierTag) > 0 else None\n",
    "#             SupplierTag_tokens = re.findall(r\"\\w+\", SupplierTag_value) if SupplierTag_value != None and len(SupplierTag_value) > 0 else []\n",
    "#             if SupplierTag_tokens != []:\n",
    "#                 condition1 = 0 < len(SupplierTag_tokens) <= MAX_TOKENS \n",
    "#                 condition2 = SupplierTag_tokens[0].strip().lower() not in NEGATIVE_SUPPLIERWORDS\n",
    "#                 if all([condition1, condition2]): return SupplierTag_value \n",
    "#             return None\n",
    "        \n",
    "#         # if Tag not found, checks for Line context - AddressBlock(AB, Phone/Fax, Email/Url)\n",
    "#         def find_SupplierAddressBlock(line):\n",
    "#             return\n",
    "        \n",
    "#         SUPPLIER_TAG_FOUND = None\n",
    "#         for line_index in df.index.values:\n",
    "#             line_present = df.iloc[line_index:line_index+1].LINES.values[0]\n",
    "#             SupplierTag_value = find_SupplierTag(line_present)\n",
    "#             if SupplierTag_value != None:\n",
    "#                 # looking in present_line\n",
    "#                 SUPPLIER_NAME = SupplierTag_value\n",
    "#                 SUPPLIER_TAG_FOUND = \"line_present\"\n",
    "#                 break\n",
    "#             else:\n",
    "#                 # looking in above_line\n",
    "#                 if line_index != 0:\n",
    "#                     line_above = df.iloc[line_index-1:line_index].LINES.values[0]\n",
    "#                 else:\n",
    "#                     line_above = \"\"\n",
    "#                 if len([t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line_above, flags=re.IGNORECASE)]) > 0:\n",
    "#                     SUPPLIER_NAME = line_present\n",
    "#                     SUPPLIER_TAG_FOUND = \"line_above\"\n",
    "#                 else:\n",
    "#                     continue \n",
    "#         if SUPPLIER_TAG_FOUND != None:\n",
    "#             return SUPPLIER_NAME.strip()\n",
    "#         else:\n",
    "#             #find_SupplierAddressBlock()\n",
    "#             return None\n",
    "        \n",
    "\n",
    "#     ''' FIND CUSTOMER TAGS IN FILE\n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Looks for customer tags in whole file. Finds a matching customer tag in: 'present_line' and 'above_line'.        '''\n",
    "#     def check_CustomerTags(df):\n",
    "#         def find_CustomerTag(line):\n",
    "#             CustomerTag = [t for t in CUSTOMER_TAGS if re.findall(r\"{}\".format(t), line, flags=re.IGNORECASE)]\n",
    "#             CustomerTag_value = \" \".join(re.findall(r\"{}(?!.*[\\:].*)(?=.*[a-zA-Z].*)(.*[A-Za-z\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/\\s]+.*)\".format(CustomerTag[0]), line.strip(), flags=re.IGNORECASE)) if len(CustomerTag) > 0 else None\n",
    "#             CustomerTag_tokens = re.findall(r\"\\w+\", CustomerTag_value) if CustomerTag_value != None and len(CustomerTag_value) > 0 else []\n",
    "#             if CustomerTag_tokens != []: \n",
    "#                 return CustomerTag_value \n",
    "#             return None\n",
    "#         CUSTOMER_TAG_FOUND = None\n",
    "#         for line_index in df.index.values:\n",
    "#             line_present = df.iloc[line_index:line_index+1].LINES.values[0]\n",
    "#             CustomerTag_value = find_CustomerTag(line_present)\n",
    "#             if CustomerTag_value != None:\n",
    "#                 # looking in present_line\n",
    "#                 CUSTOMER_NAME = CustomerTag_value\n",
    "#                 CUSTOMER_TAG_FOUND = \"line_present\"\n",
    "#                 break\n",
    "#             else:\n",
    "#                 # looking in above_line\n",
    "#                 if line_index != 0:\n",
    "#                     line_above = df.iloc[line_index-1:line_index].LINES.values[0]\n",
    "#                 else:\n",
    "#                     line_above = \"\"\n",
    "#                 if len([t for t in CUSTOMER_TAGS if re.findall(r\"{}\".format(t), line_above, flags=re.IGNORECASE)]) > 0:\n",
    "#                     CUSTOMER_NAME = line_present\n",
    "#                     CUSTOMER_TAG_FOUND = \"line_above\"\n",
    "#                 else:\n",
    "#                     continue \n",
    "#         if CUSTOMER_TAG_FOUND != None:\n",
    "#             return CUSTOMER_NAME.strip()\n",
    "#         else:\n",
    "#             return None\n",
    "#     # ------------------------------------------------------------------------------------------------------------------ #\n",
    "    \n",
    "    \n",
    "#     # ################################################################################################################## #\n",
    "#     # ------------------------------------------ MODULES :: line level ------------------------------------------------- #\n",
    "#     # ################################################################################################################## #\n",
    "\n",
    "#     ''' SORT USING A SCORING DECISION MATRIX\n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Sort the df Lines based on a scoring mechanism. SORT on:\n",
    "#         (1). on model1_probability\n",
    "#         (2). on line_number for lines with model1_probability > 95%\n",
    "#         (3). on model2_probability ...(if model1_probability less than 95%, skip line_number and check model2_probability)\n",
    "#         (4). line_number                                                                                                '''\n",
    "#     def sort(df):\n",
    "#         # generic line_number\n",
    "#         df['LINE_NO'] = [line_no for line_no in range(df.shape[0])]  \n",
    "#         # mmodified line_number with prob > 0.95\n",
    "#         df['Model1_P1_LINENO'] = [line_no if prob >=0.95 else 0 for prob, line_no in zip(df.Model1_P1, df.LINE_NO)]\n",
    "#         # sorting\n",
    "#         df = df.sort_values(by=['Model1_P1', 'Model1_P1_LINENO', 'Model2_P1', 'LINE_NO'], ascending=[False, True, False, True]) \n",
    "#         return df\n",
    "\n",
    "\n",
    "# #     ''' REDUCE ENTITIES (AB, FAX, PHONE, DATE) IN PREDICTION LINES\n",
    "# #     ----------------------------------------------------------------------------------------------------------------------\n",
    "# #     - Finds and replaces or reduces NER with <address>, <phone>, <fax>, <date> phone-numbers, date in list of predicted names.                                                          '''\n",
    "# #     def check_entities(df):\n",
    "        \n",
    "# #         def replace_entities(x):\n",
    "# #             line = x.LINES\n",
    "            \n",
    "# #             # Reduce AB\n",
    "# #             if x.POSTAL_AB == 'AB':\n",
    "# #                 chunkNER = [ent.orth_ for ent in nlp(line).ents if ent.label_ in ['ORG', 'FAC']]\n",
    "# #                 chunkNER = chunkNER[0] if chunkNER != [] else '<address>'\n",
    "# #                 line = chunkNER.strip()\n",
    "            \n",
    "# #             # Reduce Phone, Date\n",
    "# #             check_PhoneDate = sum([x for x in [[i for x in find_phone(line) for i in x if i!= ''], find_date(line)] if any(x)], [])\n",
    "# #             if len(check_PhoneDate) > 0:\n",
    "# #                 line = re.sub(r\"\\((?=\\d+\\))|(?<=\\d)\\)\", \" \", line, flags=re.IGNORECASE)\n",
    "# #                 line = re.sub(r\"\\s+\", \" \", re.sub(\"|\".join(check_PhoneDate), \" \", line, flags=re.IGNORECASE)).strip()\n",
    "            \n",
    "# #             # Reduce empty lines\n",
    "# #             if len(line) == 0: line = np.nan\n",
    "# #             return line\n",
    "        \n",
    "# #         df['LINES'] = df.apply(replace_entities, axis=1)\n",
    "# #         df = df.dropna(subset=['LINES']).reset_index(drop=True)\n",
    "# #         return df\n",
    "         \n",
    "\n",
    "#     ''' FILTER NOISE LINES IN PREDICTION LINES\n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Looks for lines that fulfill the 'noise/outlier' condition as per below configuration.                           '''\n",
    "#     def check_NoiseLines(df):\n",
    "#         ######################\n",
    "#         ##  Noise Settings  ##\n",
    "#         MAX_TOKENS = 20\n",
    "#         MAX_LEN = 80\n",
    "#         MAX_ALLDIGITS = 0\n",
    "#         MAX_PERSTOPWORS = 70.0\n",
    "#         MAX_NOISE = 0\n",
    "#         MAX_ABBRV = 0\n",
    "#         ######################\n",
    "#         def find_NoiseLines(x):           \n",
    "#             tokens = word_tokenize(re.sub(r\"\\s+\", \" \", re.sub(r\"\\^\", \" \", re.sub(r\"[^A-Za-z]\", \" \", x.LINES.strip(), flags=re.IGNORECASE | re.MULTILINE))))\n",
    "#             check_NumOfTokens = len(tokens)\n",
    "#             check_Length = len(x.LINES.lower().strip())\n",
    "#             check_AllDigits = int(x.F1_CONTAINSALLDIGIT)\n",
    "#             check_PercentStopWords = len([w for w in tokens if w.lower() in stop_words]) * 100.0/check_NumOfTokens if check_NumOfTokens > 0 else 0\n",
    "#             check_NegativeKeywords = len(re.findall(\"|\".join(NOISE_KEYWORDS) + \"|\".join(CUSTOMER_TAGS), str(x.LINES).strip(), flags=re.IGNORECASE | re.MULTILINE))\n",
    "#             check_abbrv = len(re.findall(r\"(?=(\\b\" + '\\\\b|\\\\b'.join(list_abbrv_regex) + r\"\\b))\", x.LINES.lower().strip()))\n",
    "#             # adjust above noise settings\n",
    "#             QUALIFY = \"SN\"  \n",
    "#             if check_abbrv == 0 and any([check_NumOfTokens > MAX_TOKENS, check_Length > MAX_LEN, \n",
    "#                                          check_AllDigits > MAX_ALLDIGITS, check_PercentStopWords > MAX_PERSTOPWORS,\n",
    "#                                          check_NegativeKeywords > MAX_NOISE]):\n",
    "#                 QUALIFY = \"CN\"\n",
    "#             return QUALIFY        \n",
    "#         df['QUALIFY'] = df.apply(find_NoiseLines, axis=1)\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     ''' REMOVE DUPLICATE NAMES (SN/CN) IN PREDICTION LINES\n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Removes duplicate-names in list of predicted names.                                                              '''\n",
    "#     def check_duplicates(prediction_list):\n",
    "#         ##########################\n",
    "#         ##  Duplicate Settings  ##\n",
    "#         MAX_SIMILARITY = 30\n",
    "#         ##########################\n",
    "#         def check_similarity(s1, s2):\n",
    "#             return True if word_tokenize(s1)[0].strip().lower() == word_tokenize(s2)[0].strip().lower() and \\\n",
    "#                            fuzz.ratio(s1.lower(), s2.lower()) > MAX_SIMILARITY else False\n",
    "#         def eliminate_duplicates(a):\n",
    "#             for index, name in enumerate(a):\n",
    "#                 for i, match_with in enumerate(a):\n",
    "#                     if index != i and check_similarity(name[0], match_with[0]):\n",
    "#                         a[i][0] = '<DUPLICATE>'\n",
    "#             return a\n",
    "#         PL = prediction_list\n",
    "#         if len(PL) > 2:\n",
    "#             PL = eliminate_duplicates(PL)\n",
    "#             PL = [w for w in PL if w[0] != '<DUPLICATE>']\n",
    "#             if len(PL) == 1:   # in case all were duplicates and hence got removed\n",
    "#                 PL += prediction_list\n",
    "#         return PL\n",
    "\n",
    "\n",
    "#     ''' REMOVE ENTITIES (FAX, PHONE, DATE) IN PREDICTION LINES\n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Removes phone-numbers, date in list of predicted names.                                                          '''\n",
    "#     def check_entities(prediction_list):\n",
    "#         filtered_prediction_list = []\n",
    "#         for chunk in prediction_list:\n",
    "#             find_PhoneDateRE = sum([x for x in [[i for x in find_phone(chunk[0]) for i in x if i!= ''], find_date(chunk[0])] if any(x)], [])\n",
    "#             if len(find_PhoneDateRE) > 0:\n",
    "#                 find_PhoneDateRE += PHONE_TAGS\n",
    "#                 chunk[0] = re.sub(r\"\\((?=\\d+\\))|(?<=\\d)\\)\", \"\", chunk[0], flags=re.IGNORECASE).strip()\n",
    "#                 chunk[0] = re.sub(r\"\\s+\", \" \", re.sub(\"|\".join(find_PhoneDateRE), \" \", chunk[0], flags=re.IGNORECASE))\n",
    "#             filtered_prediction_list.append(chunk)\n",
    "#         return filtered_prediction_list\n",
    "\n",
    "\n",
    "#     ''' SPLIT PREDICTION LINES INTO \"Supplier_Names\" and \"Customer_Names\"\n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Splits the list of prediciton names into SN_list and CN_list based on findings of:-\n",
    "#         1. check_SupplierTags(); \n",
    "#         2. check_CustomerTags();    \n",
    "#         3. check_NoiseLines();                                                                                         '''\n",
    "#     def check_split(prediction_list, TAG_SupplierName, TAG_CustomerName):\n",
    "#         # Sanity Check :: \n",
    "#         # - checking if the \"TAG_SupplierName\" is Email/URL, in that case compare with remaining similar chunks.\n",
    "#         def update__TAG_SupplierName():\n",
    "#             updated__TAG_SupplierName = TAG_SupplierName\n",
    "#             if TAG_SupplierName != None:  \n",
    "#                 domains = sum([x[-1] for x in [find_email(TAG_SupplierName), find_url(TAG_SupplierName)] if any(x)], [])\n",
    "#                 domain = domains[0] if len(domains) > 0 else None\n",
    "#                 if domain != None:\n",
    "#                     for chunk in prediction_list:\n",
    "#                         if all( [chunk[1] >= 0.70, fuzz.partial_ratio(domain.lower(), chunk[0].lower()) > 50] ):\n",
    "#                             updated__TAG_SupplierName = chunk[0]\n",
    "#                             #print(\"Updated Tag_SupplierName = \", updated__TAG_SupplierName)\n",
    "#                             break\n",
    "#             return updated__TAG_SupplierName\n",
    "#         # Sanity Check for SN \n",
    "#         SupplierName = update__TAG_SupplierName()\n",
    "#         CustomerName = TAG_CustomerName\n",
    "#         # Conditions to select SN,CN between: <File-level> OR <Line-level>\n",
    "#         if SupplierName != None and CustomerName != None:       \n",
    "#             SN_list = [(SupplierName, 100.0, 100.0, 'SN', 0)] + prediction_list\n",
    "#             CN_list = [(CustomerName, 100.0, 100.0, 'CN', 0)]\n",
    "#         elif SupplierName == None and CustomerName != None:\n",
    "#             SN_list = [x for x in prediction_list if x[0] != CustomerName or len(re.findall(CustomerName.strip(), x[0], flags=re.IGNORECASE)) == 0]\n",
    "#             CN_list = [(CustomerName, 100.0, 100.0, 'CN', 0)]\n",
    "#         elif SupplierName != None and CustomerName == None:\n",
    "#             SN_list = [(SupplierName, 100.0, 100.0, 'SN', 0)] + prediction_list\n",
    "#             CN_list = [x for x in prediction_list if x[0] != SupplierName or len(re.findall(SupplierName.strip(), x[0], flags=re.IGNORECASE)) == 0]\n",
    "#         else: \n",
    "#             # SupplierName == None & CustomerName == None\n",
    "#             SN_list = [x for x in prediction_list if x[3] == 'SN']\n",
    "#             CN_list = [x for x in prediction_list if x[3] == 'CN']\n",
    "#         # Final Sanity Check for SN\n",
    "#         # - Extracted SN, CN are subjected to final review to get separate lists.\n",
    "#         SN_list = [chunk for chunk in SN_list if chunk[3] == 'SN']\n",
    "#         CN_list = [chunk for chunk in CN_list if chunk[3] == 'CN']\n",
    "#         return SN_list, CN_list\n",
    "\n",
    "\n",
    "#     ''' CLEAN SN_LIST AND CN_LIST \n",
    "#     ----------------------------------------------------------------------------------------------------------------------\n",
    "#     - Removes duplicate-names in list of predicted names.                                                              '''\n",
    "#     def check_cleaning(chunk):\n",
    "#         ######################\n",
    "#         ##  Clean Settings  ##\n",
    "#         MAX_TOKENS = 12\n",
    "#         MAX_STOPWORDS_PERCENT = 40.0\n",
    "#         ######################\n",
    "        \n",
    "#         # 1. if chunk is an email/url\n",
    "#         domains = sum([x[-1] for x in [find_url(chunk), find_email(chunk)] if any(x)], [])\n",
    "#         domain = domains[0].upper() if len(domains) > 0 else None\n",
    "#         if domain != None:\n",
    "#             return domain.upper()\n",
    "#         else:\n",
    "#             # 2nd check for domain name\n",
    "#             domain_2 = re.findall(r\"([A-Za-z]+)\\.com\\b\", chunk, flags=re.IGNORECASE)\n",
    "#             if len(domain_2) > 0:\n",
    "#                 return domain_2[0].upper()\n",
    "        \n",
    "#         # 2. if chunk contains a TAG\n",
    "#         ALL_TAGS = SUPPLIER_TAGS + CUSTOMER_TAGS + MIXED_TAGS\n",
    "#         find_Tag = [tag for tag in ALL_TAGS if re.findall(r\"{}\".format(tag), chunk.strip(), flags=re.IGNORECASE)]\n",
    "#         value_TAG = \" \".join(re.findall(r\"{}(?=.*[a-zA-Z].*)(.*)\".format(find_Tag[0]), chunk.strip(), \n",
    "#                                         flags=re.IGNORECASE)).strip() if len(find_Tag) > 0 else None\n",
    "#         if value_TAG != None:\n",
    "#             return value_TAG\n",
    "        \n",
    "#         ## tokenize\n",
    "#         tokens = re.findall(r\"\\w+\", chunk.strip())\n",
    "#         tokens_stopwords_percentage = len([w for w in tokens if w.lower() in stop_words])*100.0/len(tokens) if len(tokens) > 0 else 0\n",
    "            \n",
    "#         # 3. filter if len is > max_tokens :: Spacy's NER\n",
    "#         if len(tokens) > MAX_TOKENS:\n",
    "#             chunkNER = [ent.orth_ for ent in nlp(chunk).ents if ent.label_ in ['ORG', 'FAC']]\n",
    "#             chunkNER = chunkNER[0] if chunkNER != [] else None\n",
    "#             if chunkNER != None: return chunkNER.strip()\n",
    "        \n",
    "#         # 4. filter if it contains more than 40% stopwords :: remove stopwords\n",
    "#         if tokens_stopwords_percentage > MAX_STOPWORDS_PERCENT:\n",
    "#             chunk = \" \".join([w for w in tokens if w.lower() not in stop_words])\n",
    "        \n",
    "#         # 5. [FINAL] :: Regex based cleaning for all types of chunks...\n",
    "#         chunk = re.sub(r\"\\s+\", \" \", re.sub(r\"\\^\", \" \", re.sub(r\"[^A-Za-z0-9\\@\\&\\(\\)\\-]\", \" \", chunk.strip(), flags=re.IGNORECASE))).strip()\n",
    "#         chunk = \" \".join([w for w in word_tokenize(chunk) if w.lower() not in SMART_STOPWORDS])\n",
    "#         return chunk\n",
    "    \n",
    "            \n",
    "#     ##########################################\n",
    "#     # Chunk Identification Starts......\n",
    "#     ##########################################\n",
    "#     print(\"\\n Chunk Identification...\")\n",
    "    \n",
    "#     # ------------------------------------------ CHUNK :: file level ---------------------------------------------------- #\n",
    "#     TAG_SupplierName = check_SupplierTags(df)\n",
    "#     TAG_CustomerName = check_CustomerTags(df)\n",
    "#     print(\" --> :: TAG_SN = {}; TAG_CN = {}\".format(TAG_SupplierName, TAG_CustomerName))\n",
    "#     # ------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "#     # ------------------------------------------ CHUNK :: line level ---------------------------------------------------- #\n",
    "#     final_SN, final_CN = [], []\n",
    "    \n",
    "#     #################################################################\n",
    "#     # MODEL 1 + MOdel 2\n",
    "#     #  - Generic model running only numerical features in 2-layer\n",
    "#     #  - LSTM-CNN model running Text + Numerical features\n",
    "#     #################################################################\n",
    "    \n",
    "#     sorted_df = sort(df)  # 1. sort df\n",
    "#     print(\"\\n1. Sorted Predictions == \", sorted_df.LINES.tolist())\n",
    "    \n",
    "#     sorted_df = check_NoiseLines(sorted_df)  # 2. remove noise and unwanted long lines\n",
    "#     print(\"\\n2. Noise Predictions == \", sorted_df.LINES.tolist())\n",
    "        \n",
    "#     prediction_list = [[L, P1, P2, Q, Ln] for L, P1, P2, Q, Ln in zip(sorted_df.LINES.tolist(), \n",
    "#                                                                       sorted_df.Model1_P1.tolist(),\n",
    "#                                                                       sorted_df.Model2_P1.tolist(), \n",
    "#                                                                       sorted_df.QUALIFY.tolist(), \n",
    "#                                                                       sorted_df.LINE_NO.tolist())]\n",
    "#     print(\"\\n3. Scored Predictions == \", prediction_list)\n",
    "    \n",
    "#     prediction_list = check_duplicates(prediction_list) \n",
    "#     print(\"\\n4. Duplicated Predictions == \", prediction_list)\n",
    "    \n",
    "#     prediction_list = check_entities(prediction_list)  # 5. remove entitites\n",
    "#     print(\"\\n5. Ents Predictions == \", prediction_list)\n",
    "    \n",
    "#     SN_list, CN_list = check_split(prediction_list, TAG_SupplierName, TAG_CustomerName)  # 6. split pred_list -> SN, CN\n",
    "#     print(\"\\n6. SPLIT SN_LIST == {}; CN_LIST == {};\".format(SN_list, CN_list))\n",
    "    \n",
    "#     SN_list = list(map(lambda chunk: [check_cleaning(chunk[0]), chunk[1], chunk[2], chunk[3], chunk[4]], SN_list))  # 7. clean SN chunks\n",
    "#     print(\"\\n FINAL LIST :: SN_LIST == {}; CN_LIST == {}\".format(SN_list, CN_list))\n",
    "    \n",
    "    \n",
    "#     # MODEL 2 \n",
    "#     if SN_list[0][1] < 0.10:\n",
    "#         print('INSIDE NN MODELLING')\n",
    "#         SN_list = list(map(lambda chunk: [chunk[0], 'NN', chunk[2]], SN_list))\n",
    "    \n",
    "#     #------------------------------------------------------------------------------------------------------------------- #    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     return SN_list, CN_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.29338,
     "end_time": "2021-02-04T08:33:30.979007",
     "exception": false,
     "start_time": "2021-02-04T08:33:30.685627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.276077,
     "end_time": "2021-02-04T08:33:31.549704",
     "exception": false,
     "start_time": "2021-02-04T08:33:31.273627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:32.113536Z",
     "iopub.status.busy": "2021-02-04T08:33:32.111848Z",
     "iopub.status.idle": "2021-02-04T08:33:32.122370Z",
     "shell.execute_reply": "2021-02-04T08:33:32.122864Z"
    },
    "papermill": {
     "duration": 0.29577,
     "end_time": "2021-02-04T08:33:32.123062",
     "exception": false,
     "start_time": "2021-02-04T08:33:31.827292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OLD :: Version 2.0\n",
    "# def chunk_identification(df): \n",
    "#     # TAGS\n",
    "#     SUPPLIER_TAGS = [\"credit\\s+to[^\\:]*\\:+\", \"remit\\s+to[^\\:]*\\:+\", \"remittance\\s+to[^\\:]*\\:+\", \"f/b/o\\s*\", \n",
    "#                      \"direct\\s+\\w+inquiries\\s+to[^\\:]*\\:+\", \"payable\\s+to[^\\:]*\\:+\", \"checks\\s+to[^\\:]*\\:+\", \n",
    "#                      \"check\\s+to[^\\:]*\\:+\", \"account\\s+name[^\\:]*\\:+\", ]\n",
    "    \n",
    "#     CUSTOMER_TAGS = [\"billed\\s+to[^\\:]*\\:*\", \"bill\\s+to[^\\:]*\\:*\", \"billing\\s+to[^\\:]*\\:*\", 'bills\\s+to[^\\:]*\\:*',\n",
    "#                      \"bill\\s+to\\:*\", \"billed\\s+to\\:*\", \"ship\\s+to[^\\:]*\\:*\", \"shipped\\s+to[^\\:]*\\:*\", \"shipping\\s+to[^\\:]*\\:*\", \n",
    "#                      \"ships\\s+to[^\\:]*\\:*\", \"shipto[^\\:]*\\:*\", \"shippingto[^\\:]*\\:*\", \"shipsto[^\\:]*\\:*\"]\n",
    "    \n",
    "#     MIXED_TAGS = [\".*\\s+to\\:*\\s+\", \".*\\s+is\\:*\\s+\", \".*\\s+belongs\\s+to\\:*\\s+\", \".*\\s+belong\\s+to\\:*\\s+\", \".*\\s+is\\s+of\\:*\\s+\", \n",
    "#                   \".*\\s+is\\s+to\\:*\\s+\", \".*\\s+name\\:*\\s+\"]\n",
    "    \n",
    "#     smart_stop_words = [\"https\", \"http\", \"www\", \"com\", \"please\", \"pls\", \"let\", \"lets\", \"know\", \"knowing\", \"knows\", \n",
    "#                         \"respond\", \"responding\", \"responds\", \"naming\", \"names\", \"name\", \"thanks\", \"thank\", \"thankyou\", \n",
    "#                         \"thanksyou\", \"thanking\", \"thankingyou\", \"thanx\", \"thnx\", \"thx\", \"jan\", \"feb\", \"mar\", \"apr\", \n",
    "#                         \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\", \"asap\", \"january\", \"february\", \"march\", \n",
    "#                         \"april\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\", \n",
    "#                         \"dear\", \"hello\", \"kindly\", \"ok\", \"okay\", \"regards\", \"hi\", \"hola\", \"hey\", \"yeah\", \"nope\", \n",
    "#                         \"advise\", \"many\", \"th\", \"st\", \"nd\", \"rd\", \"attached\", \"screen\", \"shot\", \"screenshot\", \n",
    "#                         \"sunday\", \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"pfa\", \n",
    "#                         \"emp\", \"id\", \"invoice\", \"contract\", \"remit\", \"remittance\", \"fbo\", \"inquiries\", \n",
    "#                         \"payable\", \"billed\", \"billing\", \"bills\", \"shipped\", \"shipping\", \"ships\", \"shipto\", \"shippingto\", \n",
    "#                         \"shipsto\", \"terms&conditions\", \"termsandconditions\", \"confidential\", \"t&c\", \n",
    "#                         \"email\", \"emailing\", \"emails\", \"emailed\", \"mailing\", \"mails\", \"mail\", \"to\", \"is\", \n",
    "#                         \"customer\", \"client\", \"duration\", \"since\", \"address\", \"order\", \"invoices\",\n",
    "#                         \"invocing\", \"ordernumber\", \"invoiceno\", \"accountno\", \"customerno\", \"make\", \"check\", \"checks\",  \n",
    "#                         \"submit\", \"submits\", \"sumitting\" , \"submitto\", \"payment\", \"paying\", \"payments\", \"paymentsto\", \n",
    "#                         \"phone\", \"phones\", \"phoning\", \"phono\", \"emailo\", \"cancel\", \"cancelling\", \"cancellation\", \n",
    "#                         \"cancellations\", \"requires\", \"require\", \"required\", \"requiring\", \"noticeable\", \"noticing\", \"notice\", \n",
    "#                         \"notices\", \"day\", \"days\", \"months\", \"month\"]\n",
    "    \n",
    "        \n",
    "#     # 1. Look for \"SUPPLIER_TAGS\" in whole file\n",
    "#     def check_SupplierTags(df):\n",
    "#         def find_SupplierTag(line):\n",
    "#             SupplierTag = [t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line, flags=re.IGNORECASE)]\n",
    "#             if len(SupplierTag) > 0:\n",
    "#                 SupplierTag_value = \" \".join(re.findall(r\"{}(?!.*[\\:].*)(?=.*[a-zA-Z].*)(.*[A-Za-z\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/\\s]+.*)\".format(SupplierTag[0]), line, flags=re.IGNORECASE)).strip()\n",
    "#                 #print(SupplierTag, \"line\", line, \"==\", SupplierTag_value)\n",
    "#                 if len(SupplierTag_value) > 0:\n",
    "#                     return SupplierTag_value\n",
    "#             return None\n",
    "#         # find \"SUPPLIER_TAG\" in each 'line' or in 'line above'\n",
    "#         SUPPLIER_TAG_FOUND = None\n",
    "#         for line_index in df.index.values:\n",
    "#             line_present = df.iloc[line_index:line_index+1].LINES.values[0]\n",
    "#             SupplierTag_value = find_SupplierTag(line_present)\n",
    "#             if SupplierTag_value != None:\n",
    "#                 # > SUPPLIER_TAG found in 'line'\n",
    "#                 SUPPLIER_NAME = SupplierTag_value\n",
    "#                 SUPPLIER_TAG_FOUND = \"line_present\"\n",
    "#                 break\n",
    "#             else:\n",
    "#                 # > SUPPLIER_TAG NOT found in 'line'\n",
    "#                 if line_index != 0:\n",
    "#                     line_above = df.iloc[line_index-1:line_index].LINES.values[0]\n",
    "#                 else:\n",
    "#                     line_above = \"\"\n",
    "#                 if len([t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line_above, flags=re.IGNORECASE)]) > 0:\n",
    "#                     # > SUPPLIER_TAG found in 'line above'\n",
    "#                     SUPPLIER_NAME = line_present\n",
    "#                     SUPPLIER_TAG_FOUND = \"line_above\"\n",
    "#                 else:\n",
    "#                     # > SUPPLIER_TAG NOT found in 'line above'\n",
    "#                     continue \n",
    "#         if SUPPLIER_TAG_FOUND != None:\n",
    "#             return SUPPLIER_NAME.strip()\n",
    "#         else:\n",
    "#             return None\n",
    "        \n",
    "        \n",
    "#     # 2. Look for \"CUSTOMER_TAGS\" in whole file\n",
    "#     def check_CustomerTags(df):\n",
    "#         def find_CustomerTag(line):\n",
    "#             CustomerTag = [t for t in CUSTOMER_TAGS if re.findall(r\"{}\".format(t), line, flags=re.IGNORECASE)]\n",
    "#             if len(CustomerTag) > 0:\n",
    "#                 CustomerTag_value = \" \".join(re.findall(r\"{}(?!.*[\\:].*)(?=.*[a-zA-Z].*)(.*[A-Za-z\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/\\s]+.*)\".format(CustomerTag[0]), line, flags=re.IGNORECASE)).strip()\n",
    "#                 if len(CustomerTag_value) > 0:\n",
    "#                     return CustomerTag_value    \n",
    "#             return None\n",
    "#         # find \"CUSTOMER_TAG\" in each 'line' or in 'line above'\n",
    "#         CUSTOMER_TAG_FOUND = None\n",
    "#         for line_index in df.index.values:\n",
    "#             line_present = df.iloc[line_index:line_index+1].LINES.values[0]\n",
    "#             CustomerTag_value = find_CustomerTag(line_present)\n",
    "#             if CustomerTag_value != None:\n",
    "#                 # > CUSTOMER_TAG found in 'line'\n",
    "#                 CUSTOMER_NAME = CustomerTag_value\n",
    "#                 CUSTOMER_TAG_FOUND = \"line_present\"\n",
    "#                 break\n",
    "#             else:\n",
    "#                 # > CUSTOMER_TAG NOT found in 'line'\n",
    "#                 if line_index != 0:\n",
    "#                     line_above = df.iloc[line_index-1:line_index].LINES.values[0]\n",
    "#                 else:\n",
    "#                     line_above = \"\"\n",
    "#                 if len([t for t in CUSTOMER_TAGS if re.findall(r\"{}\".format(t), line_above, flags=re.IGNORECASE)]) > 0:\n",
    "#                     # > CUSTOMER_TAG found in 'line above'\n",
    "#                     CUSTOMER_NAME = line_present\n",
    "#                     CUSTOMER_TAG_FOUND = \"line_above\"\n",
    "                    \n",
    "#                 else:\n",
    "#                     # > CUSTOMER_TAG NOT found in 'line above'\n",
    "#                     continue \n",
    "#         if CUSTOMER_TAG_FOUND != None:\n",
    "#             return CUSTOMER_NAME.strip()\n",
    "#         else:\n",
    "#             return None\n",
    "\n",
    "#     # 3. Remove Noise lines\n",
    "#     def check_NoiseLines(df):\n",
    "#         # Check noise in predicted lines\n",
    "#         def find_NoiseLines(x):              \n",
    "#             tokens = word_tokenize(re.sub(r\"\\s+\", \" \", re.sub(r\"\\^\", \" \", re.sub(r\"[^A-Za-z]\", \" \", x.LINES.strip(), \n",
    "#                                                                                  flags=re.IGNORECASE | re.MULTILINE))))\n",
    "#             find_numofTokens = len(tokens)\n",
    "#             find_length = len(x.LINES.lower().strip())\n",
    "#             find_allDigits = x.F1_CONTAINSALLDIGIT\n",
    "#             find_noise = re.findall(\"attn[^\\:]*\\:+\\s*|from\\s*\\:+|subject\\s*\\:+|cc\\s*\\:+|bcc\\s*\\:+|\\\n",
    "#                                     terms\\s+and\\s+condition\\s*|terms\\s+&\\s+conditions\\s*|terms&conditions\\s*|\\\n",
    "#                                     termsandconditions\\s*|confidential\\s*|T&C\\s*|TC\\s*|if you have any questions\\s*|\\\n",
    "#                                     if you have questions\\s*|write\\s*to\\:+\\s*|bill\\s*to\\:+\\s*|billed\\s*to.*\\:+\\s*|\\\n",
    "#                                     contact\\s*to\\:+\\s*|email\\s*to\\:+\\s*|write\\s*at\\:+\\s*|email\\s*at\\:+\\s*|\\\n",
    "#                                     email\\s*\\@\\:+\\s*|client\\s*|\" + \"|\".join(CUSTOMER_TAGS),\n",
    "#                                     x.LINES, flags=re.IGNORECASE | re.MULTILINE)\n",
    "#             if len(tokens) > 0:\n",
    "#                 find_stopwords = len([w for w in tokens if w.lower() in stop_words])*100.0/len(tokens)\n",
    "#             else:\n",
    "#                 find_stopwords = 0\n",
    "#             # Condition for \"NOT a Supplier Name\"...\n",
    "#             QUALIFY = \"SN\"         \n",
    "#             if find_numofTokens > 30 or find_length > 80 or find_allDigits > 0 or len(find_noise) > 0 or find_stopwords > 60:\n",
    "#                 QUALIFY = \"CN\"\n",
    "#             return QUALIFY                \n",
    "#         df['QUALIFY'] = df.apply(find_NoiseLines, axis=1)\n",
    "#         return df\n",
    "        \n",
    "#     # Post-Processing\n",
    "#     def cleaning(prediction_chunks, TAG_SupplierName, TAG_CustomerName):\n",
    "        \n",
    "#         # A. Removing duplicate values from prediction\n",
    "#         def remove_duplicates(Top_lines):   \n",
    "#             def check_similarity(s1, s2):\n",
    "#                 if word_tokenize(s1)[0].strip().lower() == word_tokenize(s2)[0].strip().lower() and fuzz.ratio(s1.lower(), s2.lower()) > 30:\n",
    "#                     return True\n",
    "#                 else:\n",
    "#                     return False\n",
    "#             def eliminate_duplicates(a):\n",
    "#                 for index, name in enumerate(a):\n",
    "#                     for i, match_with in enumerate(a):\n",
    "#                         if index != i and check_similarity(name[0], match_with[0]):\n",
    "#                             a[i][0] = '<DUPLICATE>'\n",
    "#                 return a\n",
    "#             a = Top_lines\n",
    "#             if len(a) > 2:\n",
    "#                 a = eliminate_duplicates(a)\n",
    "#                 a = [w for w in a if w[0] != '<DUPLICATE>']\n",
    "#                 if len(a) == 1:\n",
    "#                     a += Top_lines\n",
    "#                 return a\n",
    "#             else:\n",
    "#                 return Top_lines\n",
    "\n",
    "#         # B. Splitting prediction chunks into SupplerName, CompanyName\n",
    "#         def split_chunklist(prediction_chunks):\n",
    "#             # CHECK: Checking if found \"TAG_SupplierName\" = Email/URL\n",
    "#             def check_TAGSupplierNameType():\n",
    "#                 UPDATED_TAG_SupplierName = TAG_SupplierName\n",
    "#                 if TAG_SupplierName != None:                    \n",
    "#                     _, email_domain = find_email(TAG_SupplierName)\n",
    "#                     _, _, url_domain = find_url(TAG_SupplierName)\n",
    "#                     if len(email_domain) > 0: \n",
    "#                         domain = email_domain[0]\n",
    "#                     elif len(url_domain) > 0: \n",
    "#                         domain = url_domain[0]\n",
    "#                     else:\n",
    "#                         domain = None\n",
    "#                     if domain != None:\n",
    "#                         # check in top 3\n",
    "#                         for chunk in prediction_chunks[:3]:\n",
    "#                             # if prob(1) is > 70%\n",
    "#                             if chunk[1] >= 0.70:\n",
    "#                                 score = fuzz.partial_ratio(domain.lower(), chunk[0].lower())\n",
    "#                                 if score > 50:\n",
    "#                                     UPDATED_TAG_SupplierName = chunk[0]\n",
    "#                                     #print(\"NEW TAG SN == \", UPDATED_TAG_SupplierName)\n",
    "#                                     break\n",
    "#                 return UPDATED_TAG_SupplierName\n",
    "\n",
    "#             # Store 'TAG_SupplierName/Updated_TAG_SupplierName' as SN; 'TAG_CustomerName' as CN\n",
    "#             SupplierName = check_TAGSupplierNameType()\n",
    "#             CustomerName = TAG_CustomerName\n",
    "#             # Conditions to check SN, CN present in whole file\n",
    "#             if SupplierName != None and CustomerName != None:       \n",
    "#                 SN_list = [(SupplierName, 100.0, 'SN')] + prediction_chunks\n",
    "#                 CN_list = [(CustomerName, 100.0, 'CN')]\n",
    "#             elif SupplierName == None and CustomerName != None:\n",
    "#                 SN_list = [x for x in prediction_chunks if x[0] != CustomerName or len(re.findall(CustomerName.strip(), x[0], flags=re.IGNORECASE)) == 0]\n",
    "#                 CN_list = [(CustomerName, 100.0, 'CN')]\n",
    "#             elif SupplierName != None and CustomerName == None:\n",
    "#                 SN_list = [(SupplierName, 100.0, 'SN')] + prediction_chunks\n",
    "#                 CN_list = [x for x in prediction_chunks if x[0] != SupplierName or len(re.findall(SupplierName.strip(), x[0], flags=re.IGNORECASE)) == 0]\n",
    "#             else: \n",
    "#                 # SupplierName == None and CustomerName == None\n",
    "#                 SN_list = [x for x in prediction_chunks if x[2] == 'SN']\n",
    "#                 CN_list = [x for x in prediction_chunks if x[2] == 'CN']\n",
    "#             return SN_list, CN_list\n",
    "            \n",
    "#         # C. Final Chunk Cleaning\n",
    "#         def extract_cleanchunk(chunk):\n",
    "#             # Clean - EMAIL CHUNKS\n",
    "#             _, email_domain = find_email(chunk)\n",
    "#             if len(email_domain) > 0:\n",
    "#                 chunk = email_domain[0].upper()\n",
    "#                 return chunk\n",
    "            \n",
    "#             _, _, url_domain = find_url(chunk)\n",
    "#             if len(url_domain) > 0:\n",
    "#                 chunk = url_domain[0].upper()\n",
    "#                 return chunk\n",
    "#             # Clean - TAG CHUNKS\n",
    "#             SupplierTags = [t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), chunk.strip(), flags=re.IGNORECASE)]\n",
    "#             CustomerTags = [t for t in CUSTOMER_TAGS if re.findall(r\"{}\".format(t), chunk.strip(), flags=re.IGNORECASE)]\n",
    "#             if len(SupplierTags) > 0:\n",
    "#                 chunk = \" \".join(re.findall(r\"{}([\\w\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/\\s]+)\".format(SupplierTags[0]), chunk, flags=re.IGNORECASE)).strip()\n",
    "#                 return chunk\n",
    "#             if len(CustomerTags) > 0:\n",
    "#                 chunk = \" \".join(re.findall(r\"{}([\\w\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/\\s]+)\".format(CustomerTags[0]), chunk, flags=re.IGNORECASE)).strip()\n",
    "#                 return chunk\n",
    "#             # Clean - Generic\n",
    "#             chunk = re.sub(r\"\\s+\", \" \", re.sub(r\"\\^\", \" \", re.sub(r\"[^A-Za-z0-9\\@\\&\\.\\,\\(\\)\\-]\", \" \", chunk.strip(), flags=re.IGNORECASE))).strip()\n",
    "#             if len(chunk.split(' ')) > 6: \n",
    "#                 chunk = \" \".join([w for w in word_tokenize(chunk) if w.lower() not in stop_words])\n",
    "#             return chunk\n",
    "        \n",
    "#         # D. Final SN_LIST Cleaning\n",
    "#         def final_cleaning(SN_list):\n",
    "#             cleaned_SN_list = []\n",
    "#             for SN_chunk in SN_list:\n",
    "#                 SN = SN_chunk[0]\n",
    "#                 # Remove if a Clean Tag is found...\n",
    "#                 CLEAN_TAGS = SUPPLIER_TAGS + CUSTOMER_TAGS + MIXED_TAGS\n",
    "#                 CLEAN_TAG_found = [t for t in CLEAN_TAGS if re.findall(r\"{}\".format(t), SN, flags=re.IGNORECASE | re.MULTILINE)]\n",
    "#                 if len(CLEAN_TAG_found) > 0:\n",
    "#                     SN = \" \".join(re.findall(r\"{}(?=.*[a-zA-Z].*)(.*)\".format(CLEAN_TAG_found[0]), SN, flags=re.IGNORECASE | re.MULTILINE))\n",
    "                \n",
    "#                 # Remove stop_words if len is more than 8 words...\n",
    "#                 if len(SN.split(' ')) > 8:\n",
    "#                     SN = \" \".join([w for w in word_tokenize(SN) if w.lower() not in stop_words])\n",
    "#                 # Remove Smart Stop_words if len is more than 3 words...\n",
    "#                 if len(SN.split(' ')) >= 4:\n",
    "#                     SN = \" \".join([w for w in word_tokenize(SN) if w.lower() not in smart_stop_words])\n",
    "#                 SN_chunk[0] = SN\n",
    "#                 cleaned_SN_list.append(SN_chunk)\n",
    "#             return cleaned_SN_list\n",
    "            \n",
    "        \n",
    "#         # Run [A, B, C, D] Steps...\n",
    "#         prediction_chunks = remove_duplicates(prediction_chunks) # A\n",
    "#         #print(\"PREDICTION :: \", prediction_chunks)\n",
    "        \n",
    "#         SN_list, CN_list = split_chunklist(prediction_chunks) # B\n",
    "#         SN_list = [x for x in SN_list if x[2] == 'SN']\n",
    "#         CN_list = [x for x in CN_list if x[2] == 'CN']\n",
    "#         #print(\"SPLIT :: SN LIST == \", SN_list, \"; CN LIST == \", CN_list)\n",
    "        \n",
    "#         SN_list = list(map(lambda x: [extract_cleanchunk(x[0]), x[1]], SN_list)) # C\n",
    "#         CN_list = list(map(lambda x: [extract_cleanchunk(x[0]), x[1]], CN_list))\n",
    "#         #print(\"Semi-Final :: SN LIST == \", SN_list, \"; CN LIST == \", CN_list)\n",
    "        \n",
    "#         SN_list = final_cleaning(SN_list) # D\n",
    "#         #print(\" ** FINAL ** :: SN LIST == \", SN_list, \"; CN LIST == \", CN_list)\n",
    "        \n",
    "#         return SN_list, CN_list\n",
    "    \n",
    "\n",
    "#     ##########################################\n",
    "#     # Chunk Identification starts...\n",
    "#     ##########################################\n",
    "#     #print(\"\\n OLD\")\n",
    "    \n",
    "#     # Find Supplier Tags & Customer Tags in file...\n",
    "#     TAG_SupplierName = check_SupplierTags(df)\n",
    "#     TAG_CustomerName = check_CustomerTags(df)\n",
    "#     #print(\"\\nTAG :: TAG_SN = \", TAG_SupplierName, \"; TAG_CN = \", TAG_CustomerName)\n",
    "    \n",
    "#     final_SN, final_CN = [], []\n",
    "#     ### Y_PRED == 1 ###\n",
    "#     if df[df.Y_PRED == 1].shape[0] > 0:\n",
    "#         # sorted by prob of 1 (0.50 to 1.0)\n",
    "#         sorted_df = df[df.Y_PRED == 1].sort_values(by=['P1'], ascending=False)\n",
    "#         sorted_df = check_NoiseLines(sorted_df)\n",
    "#         predictions = [[line, score, qualify] for line, score, qualify in zip(sorted_df.LINES.tolist(), sorted_df.P1.tolist(), sorted_df.QUALIFY.tolist())]\n",
    "#         final_SN, final_CN = cleaning(predictions, TAG_SupplierName, TAG_CustomerName)\n",
    "\n",
    "#     ### Y_PRED != 1 ###\n",
    "#     else:\n",
    "#         # sorted by prob of 1 (0 to 0.50)\n",
    "#         sorted_df = df.sort_values(by=['P1'], ascending=False)\n",
    "#         sorted_df = check_NoiseLines(sorted_df)\n",
    "#         predictions = [[line, score, qualify] for line, score, qualify in zip(sorted_df.LINES.tolist(), sorted_df.P1.tolist(), sorted_df.QUALIFY.tolist())]\n",
    "#         final_SN, final_CN = cleaning(predictions, TAG_SupplierName, TAG_CustomerName)\n",
    "        \n",
    "#     return final_SN, final_CN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.275783,
     "end_time": "2021-02-04T08:33:32.675134",
     "exception": false,
     "start_time": "2021-02-04T08:33:32.399351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.323617,
     "end_time": "2021-02-04T08:33:33.280216",
     "exception": false,
     "start_time": "2021-02-04T08:33:32.956599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T08:33:33.839851Z",
     "iopub.status.busy": "2021-02-04T08:33:33.838893Z",
     "iopub.status.idle": "2021-02-04T08:33:33.848050Z",
     "shell.execute_reply": "2021-02-04T08:33:33.847239Z"
    },
    "papermill": {
     "duration": 0.29196,
     "end_time": "2021-02-04T08:33:33.848224",
     "exception": false,
     "start_time": "2021-02-04T08:33:33.556264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OLD :: Version 1.0\n",
    "#     def chunk_identification(df):   \n",
    "#         SUPPLIER_TAGS = [\"Credit\\s+to[^\\:]*\\:+\", \"Remit\\s+to[^\\:]*\\:+\", \"Remittance\\s+to[^\\:]*\\:+\", \"f/b/o\\s*\", \n",
    "#                          \"Direct\\s+\\w+Inquiries\\s+to[^\\:]*\\:+\", \"Payable\\s+to[^\\:]*\\:+\", \"Checks\\s+to[^\\:]*\\:+\", \n",
    "#                          \"Check\\s+to[^\\:]*\\:+\", \"Account\\s+Name[^\\:]*\\:+\", ]\n",
    "\n",
    "#         CUSTOMER_TAGS = [\"Billed\\s+to[^\\:]*\\:+\", \"Bill\\s+to[^\\:]*\\:+\", \"Billing\\s+to[^\\:]*\\:+\"]\n",
    "\n",
    "#         SN_LINE_regex = \"[^A-z\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/]\"\n",
    "\n",
    "#         MAX_LENGTH_LINE = 200\n",
    "\n",
    "#         def final_list_chunks(Top_lines):\n",
    "#             def check_similarity(s1, s2):\n",
    "#                 if word_tokenize(s1)[0].strip().lower() == word_tokenize(s2)[0].strip().lower() and fuzz.ratio(s1.lower(), s2.lower()) > 30:\n",
    "#                     return True\n",
    "#                 else:\n",
    "#                     return False\n",
    "#             def elimnate_duplicates(a):\n",
    "#                 for index, name in enumerate(a):\n",
    "#                     for i, match_with in enumerate(a):\n",
    "#                         if index != i and check_similarity(name, match_with):\n",
    "#                             del a[i:i+1]                \n",
    "#                 return a\n",
    "\n",
    "#             a = Top_lines\n",
    "#             a = list(OrderedDict.fromkeys(a))\n",
    "#             if len(a) > 2:\n",
    "#                 a = elimnate_duplicates(a[:5])\n",
    "#                 if len(a) == 1:\n",
    "#                     a += Top_lines[5:6]\n",
    "#                 return a\n",
    "#             else:\n",
    "#                 return Top_lines\n",
    "        \n",
    "#         def check_SupplierTags(df):\n",
    "#             def find_header_tag(line):\n",
    "#                 header_tag = [t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line, re.IGNORECASE)]\n",
    "#                 if len(header_tag) > 0:\n",
    "#                     header_value = \" \".join(re.findall(r\"{}([\\w\\&\\.\\,\\(\\)\\-\\@\\<\\>\\\\\\/\\s]+)\".format(header_tag[0]), line, re.IGNORECASE)).strip()\n",
    "#                     if len(header_value) > 0:\n",
    "#                         return header_value\n",
    "#                 return None\n",
    "            \n",
    "#             HEADER_FOUND = None\n",
    "#             for line_index in df[df.Y_PRED==1].index.values:\n",
    "#                 line_present = df.iloc[line_index:line_index+1].LINES.values[0]\n",
    "#                 header_value = find_header_tag(line_present)\n",
    "#                 if header_value != None:\n",
    "#                     header_line = header_value\n",
    "#                     HEADER_FOUND = \"line_present\"\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     if line_index != 0:\n",
    "#                         line_above = df.iloc[line_index-1:line_index].LINES.values[0]\n",
    "#                     else:\n",
    "#                         line_above = \"\"\n",
    "#                     if len([t for t in SUPPLIER_TAGS if re.findall(r\"{}\".format(t), line_above, re.IGNORECASE)]) > 0:\n",
    "#                         header_line = line_present\n",
    "#                         HEADER_FOUND = \"line_above\"\n",
    "#                     else:\n",
    "#                         continue \n",
    "            \n",
    "#             if HEADER_FOUND != None:                \n",
    "#                 Pred_SN, Pred_SN_P0, Pred_SN_P1 = header_line, 0.01, 0.99\n",
    "#                 return Pred_SN, Pred_SN_P0, Pred_SN_P1\n",
    "#             else:\n",
    "#                 return None\n",
    "        \n",
    "#         def check_CustomerTags(df):\n",
    "#             return None\n",
    "                       \n",
    "#         def check_NoiseLines(df):\n",
    "#             def noise_line_found(x):  \n",
    "#                 find_keyword = re.sub(r\"\\s+\", \" \", re.sub(SN_LINE_regex, \" \", x.LINES.strip(), re.IGNORECASE))\n",
    "#                 find_len = len(re.sub(r\"\\s+\", \" \", re.sub(SN_LINE_regex, \" \", x.LINES.strip(), re.IGNORECASE).strip()))\n",
    "#                 find_abbrv = len(re.findall(r\"(?=(\\b\" + '\\\\b|\\\\b'.join(list_abbrv_regex) + r\"\\b))\", x.LINES.lower().strip()))\n",
    "#                 find_digits = x.F1_CONTAINSALLDIGIT\n",
    "#                 find_emails = len(re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\", x.LINES.strip(), re.IGNORECASE))\n",
    "#                 find_header_tag = len(re.findall(r\"^[^\\:]*\\:\\s*$\", x.LINES.strip(), re.IGNORECASE))\n",
    "#                 find_noise = len(re.findall(\"To\\s*\\:+|From\\s*\\:+|Subject\\s*\\:+|CC\\s*\\:+|BCC\\s*\\:+|Customer\\s*\\-*\\:*|\\\n",
    "#                                             Terms\\s+and\\s+conditions|Terms\\s+&\\s+conditions|Terms&Conditions\\s+|\\\n",
    "#                                             TermsandConditions\\s+|Confidential\\s+|T&C\\s+|TC\\s+|If you have any questions|\\\n",
    "#                                             If you have questions|Write\\s*to\\:+|Bill\\s*to\\:+|Billed\\s*to.*\\:+|\\\n",
    "#                                             Contact\\s*to\\:+|Email\\s*to\\:+|Write\\s*at\\:+|Email\\s*at\\:+|Email\\s*\\@\\:+|\\\n",
    "#                                             Confidential|Client\\s*|Manager\\s*|Customs\\s*|Trade\\s*\", \n",
    "#                                             re.sub(SN_LINE_regex, \" \", x.LINES, re.IGNORECASE).strip(), re.IGNORECASE))\n",
    "#                 find_stopwords = len([w for w in word_tokenize(re.sub(SN_LINE_regex, \" \", x.LINES, re.IGNORECASE).strip()) \\\n",
    "#                                       if w in stop_words])*100.0/len(word_tokenize(re.sub(SN_LINE_regex, \" \", x.LINES, re.IGNORECASE).strip()))\n",
    "#                 # Checks\n",
    "#                 if find_abbrv > 0 and find_len <= 30:\n",
    "#                     return 1\n",
    "#                 if find_len > MAX_LENGTH_LINE or find_digits > 0 or find_header_tag > 0 or find_noise > 0 :\n",
    "#                     return 0\n",
    "                 \n",
    "#                 return 1\n",
    "#             df['QUALIFY'] = df.apply(noise_line_found, axis=1)\n",
    "#             df = df[df.QUALIFY != 0].drop(columns=['QUALIFY'])\n",
    "#             return df\n",
    "        \n",
    "#         # Chunk Identification starts...\n",
    "#         if df[df.Y_PRED == 1].shape[0] > 0:\n",
    "            \n",
    "            \n",
    "#             Pred_SN, Pred_SN_P0, Pred_SN_P1 = \"\", 0, 0\n",
    "            \n",
    "#             # Check for header tags\n",
    "#             if check_SupplierTags(df) != None:\n",
    "#                 Pred_SN, Pred_SN_P0, Pred_SN_P1 = check_SupplierTags(df)\n",
    "#                 return Pred_SN, Pred_SN_P0, Pred_SN_P1, [Pred_SN]\n",
    "            \n",
    "#             # Prediction using a classifier\n",
    "#             fdf = df[df.Y_PRED == 1].copy()\n",
    "            \n",
    "# #             # remove noise lines\n",
    "# #             if fdf.shape[0] > 2:\n",
    "# #                 fdf = check_NoiseLines(fdf)\n",
    "\n",
    "# #             # Normalize it\n",
    "# #             fdf[normalize_cols] = Normalize.transform(fdf[normalize_cols])\n",
    "\n",
    "# #             if 'P0' in fdf.columns:\n",
    "# #                 fdf['P0'], fdf['P1'] = 0,0\n",
    "\n",
    "# #             # MODELS...\n",
    "# # #             #NN\n",
    "# # #             max_features = 5\n",
    "# # #             sequence_length = 6\n",
    "# # #             embedding_dim = 6\n",
    "# # #             X_unseen_text = tokenizer.texts_to_sequences(fdf.LINE_NER)\n",
    "# # #             X_unseen_text = pad_sequences(X_unseen_text, padding='post', maxlen=sequence_length)\n",
    "# # #             X_unseen_num = fdf[Features_NUM]\n",
    "# # #             X_unseen_num[normalize_cols] = Normalize.transform(X_unseen_num[normalize_cols])\n",
    "# # #             fdf['Final_P0'], fdf['Final_P1'] = zip(*model.predict([X_unseen_text, X_unseen_num]))\n",
    "# #             ## RF\n",
    "# #             fdf['Final_P0'], fdf['Final_P1'] = zip(*model.predict_proba(fdf[Features_L2]))\n",
    "\n",
    "#             fdf['Final_P0'], fdf['Final_P1'] = df.P0, df.P1\n",
    "\n",
    "#             # Max Prob in \"Final_P1\"\n",
    "#             prediction = fdf[fdf['Final_P1']==fdf['Final_P1'].max()]\n",
    "#             Pred_SN, Pred_SN_P0, Pred_SN_P1 = fdf.LINES.values[0], prediction['Final_P0'].values[0], prediction['Final_P1'].values[0]\n",
    "            \n",
    "#             # Lines sorted on predicted score\n",
    "#             Top_Lines = fdf.sort_values(by=['Final_P1'], ascending=False).LINES.tolist()\n",
    "            \n",
    "#             Top_Lines = final_list_chunks(Top_Lines)[:5]\n",
    "            \n",
    "#             return Pred_SN, Pred_SN_P0, Pred_SN_P1, Top_Lines\n",
    "#         else:\n",
    "#             return \"\", 0, 0, []\n",
    "\n",
    "#     # Execute for each filename...\n",
    "#     final_df = []\n",
    "#     for f in df.FILENAME.unique():\n",
    "        \n",
    "#         # every df\n",
    "#         tempdf = df[df.FILENAME == f].copy().reset_index(drop=True)\n",
    "        \n",
    "#         #########################\n",
    "#         # 1. Line Classification\n",
    "#         # TRUE\n",
    "#         actual = tempdf[tempdf.Y_SN == 1]\n",
    "#         actual_SN = str(actual.SUPPLIER_NAME.tolist()[0])\n",
    "#         # PRED\n",
    "#         predicted = tempdf[tempdf.Y_PRED == 1]\n",
    "#         predicted_SN = predicted.LINES.tolist()\n",
    "#         # Accuracy\n",
    "#         correct_df = tempdf[(tempdf.Y_SN == 1) & (tempdf.Y_PRED == 1)]\n",
    "#         correct_LINES, correct_QUAD = correct_df['LINES'].tolist(), correct_df['F6_lineQuadrant'].tolist()\n",
    "#         if len(correct_LINES) > 0:\n",
    "#             correct_LINE_found = 1\n",
    "#         else:\n",
    "#             correct_LINE_found = 0\n",
    "#         #########################\n",
    "        \n",
    "#         #########################\n",
    "#         # 2. Chunk Identification\n",
    "#         Final_Pred_SN, Final_Pred_SN_P0, Final_Pred_SN_P1, Top_Lines = chunk_identification(tempdf)\n",
    "#         Final_Pred_SN = generic_cleaning(Final_Pred_SN)\n",
    "        \n",
    "#         #########################\n",
    "    \n",
    "#         # STORE\n",
    "#         final_df.append({\"FILE\": f, \"SN\": actual_SN, \n",
    "#                          #\"Count_True_Lines\": actual.shape[0], \n",
    "#                          #\"Count_Pred_Lines\": predicted.shape[0], \n",
    "#                          \"Pred_Lines\": predicted_SN, \"is_CorrectLineFound\": correct_LINE_found, \n",
    "#                          \"Correct_Pred_Lines\": correct_LINES, \"Correct_Quad\": correct_QUAD,\n",
    "#                          #\"PRED_SN_P0\": Final_Pred_SN_P0, \"PRED_SN_P1\": Final_Pred_SN_P1, \n",
    "#                          \"PRED_SN_Byorder\":Final_Pred_SN, 'PRED_SN_Byprob':Top_Lines})\n",
    "    \n",
    "#     pred_df = pd.DataFrame.from_dict(final_df)\n",
    "#     print(\"Total Files = {}\\n**LINE CLASSIFICATION**\\nCorrect Line Found = {}\\nLines Missed = {}\"\n",
    "#           .format(pred_df.shape[0], pred_df.is_CorrectLineFound.sum(), pred_df.shape[0] - pred_df.is_CorrectLineFound.sum()))\n",
    "    \n",
    "#     # DISPLAY\n",
    "#     pred_df = pred_df.drop(columns=['is_CorrectLineFound', 'Correct_Pred_Lines', 'Correct_Quad', 'Pred_Lines', 'PRED_SN_Byorder'])\n",
    "    \n",
    "#     return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.277235,
     "end_time": "2021-02-04T08:33:34.410219",
     "exception": false,
     "start_time": "2021-02-04T08:33:34.132984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--- X -- X ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 211.413621,
   "end_time": "2021-02-04T08:33:35.499617",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-04T08:30:04.085996",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
